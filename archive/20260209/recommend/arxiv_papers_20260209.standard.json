{
  "mode": "standard",
  "generated_at": "2026-02-09T04:41:22.207919+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 0,
    "deep_cap": 6,
    "deep_selected": 0,
    "quick_candidates": 1,
    "quick_skim_target": 11,
    "quick_selected": 1
  },
  "deep_dive": [],
  "quick_skim": [
    {
      "id": "2602.06923v1",
      "title": "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers",
      "abstract": "Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on \"world models\" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous \"AI Physicist\" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively \"bake in\" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.",
      "authors": [
        "Ziming Liu",
        "Sophia Sanborn",
        "Surya Ganguli",
        "Andreas Tolias"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "physics.class-ph"
      ],
      "published": "2026-02-06 18:17:37+00:00",
      "link": "https://arxiv.org/pdf/2602.06923v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Focuses on discovering physical laws and world models from data, a core goal of symbolic regression.",
      "llm_evidence_cn": "关注从数据中发现物理定律和世界模型，这是符号回归的核心目标。",
      "llm_evidence": "关注从数据中发现物理定律和世界模型，这是符号回归的核心目标。",
      "llm_tldr_en": "Explores using Transformers with inductive biases to discover physical laws, bridging prediction and understanding.",
      "llm_tldr_cn": "探索利用带有归纳偏置的Transformer发现物理定律，弥合了预测与理解之间的差距。",
      "llm_tldr": "探索利用带有归纳偏置的Transformer发现物理定律，弥合了预测与理解之间的差距。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "7"
    }
  ]
}