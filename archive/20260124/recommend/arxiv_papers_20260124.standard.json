{
  "mode": "standard",
  "generated_at": "2026-01-24T14:01:49.448097+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 3,
    "deep_divecandidates": 3,
    "deep_cap": 8,
    "deep_selected": 3,
    "quick_candidates": 12,
    "quick_skim_target": 13,
    "quick_selected": 12
  },
  "deep_dive": [
    {
      "id": "2601.15717v1",
      "title": "Investigation of the Generalisation Ability of Genetic Programming-evolved Scheduling Rules in Dynamic Flexible Job Shop Scheduling",
      "abstract": "Dynamic Flexible Job Shop Scheduling (DFJSS) is a complex combinatorial optimisation problem that requires simultaneous machine assignment and operation sequencing decisions in dynamic production environments. Genetic Programming (GP) has been widely applied to automatically evolve scheduling rules for DFJSS. However, existing studies typically train and test GP-evolved rules on DFJSS instances of the same type, which differ only by random seeds rather than by structural characteristics, leaving their cross-type generalisation ability largely unexplored. To address this gap, this paper systematically investigates the generalisation ability of GP-evolved scheduling rules under diverse DFJSS conditions. A series of experiments are conducted across multiple dimensions, including problem scale (i.e., the number of machines and jobs), key job shop parameters (e.g., utilisation level), and data distributions, to analyse how these factors influence GP performance on unseen instance types. The results show that good generalisation occurs when the training instances contain more jobs than the test instances while keeping the number of machines fixed, and when both training and test instances have similar scales or job shop parameters. Further analysis reveals that the number and distribution of decision points in DFJSS instances play a crucial role in explaining these performance differences. Similar decision point distributions lead to better generalisation, whereas significant discrepancies result in a marked degradation of performance. Overall, this study provides new insights into the generalisation ability of GP in DFJSS and highlights the necessity of evolving more generalisable GP rules capable of handling heterogeneous DFJSS instances effectively.",
      "authors": [
        "Luyao Zhu",
        "Fangfang Zhang",
        "Yi Mei",
        "Mengjie Zhang"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 07:38:27+00:00",
      "link": "https://arxiv.org/pdf/2601.15717v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "automatically evolve scheduling rules using genetic programming",
      "llm_evidence_cn": "使用遗传编程自动进化调度规则",
      "llm_evidence": "使用遗传编程自动进化调度规则",
      "llm_tldr_en": "Investigates the generalization of GP-evolved scheduling rules for complex combinatorial optimization in DFJSS.",
      "llm_tldr_cn": "研究遗传编程进化的调度规则在动态柔性作业车间调度这一复杂组合优化问题中的泛化能力。",
      "llm_tldr": "研究遗传编程进化的调度规则在动态柔性作业车间调度这一复杂组合优化问题中的泛化能力。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.15738v1",
      "title": "LLM-Assisted Automatic Dispatching Rule Design for Dynamic Flexible Assembly Flow Shop Scheduling",
      "abstract": "Dynamic multi-product delivery environments demand rapid coordination of part completion and product-level kitting within hybrid processing and assembly systems to satisfy strict hierarchical supply constraints. The flexible assembly flow shop scheduling problem formally defines dependencies for multi-stage kitting, yet dynamic variants make designing integrated scheduling rules under multi-level time coupling highly challenging. Existing automated heuristic design methods, particularly genetic programming constrained to fixed terminal symbol sets, struggle to capture and leverage dynamic uncertainties and hierarchical dependency information under transient decision states. This study develops an LLM-assisted Dynamic Rule Design framework (LLM4DRD) that automatically evolves integrated online scheduling rules adapted to scheduling features. Firstly, multi-stage processing and assembly supply decisions are transformed into feasible directed edge orderings based on heterogeneous graph. Then, an elite knowledge guided initialization embeds advanced design expertise into initial rules to enhance initial quality. Additionally, a dual-expert mechanism is introduced in which LLM-A evolutionary code to generate candidate rules and LLM-S conducts scheduling evaluation, while dynamic feature-fitting rule evolution combined with hybrid evaluation enables continuous improvement and extracts adaptive rules with strong generalization capability. A series of experiments are conducted to validate the effectiveness of the method. The average tardiness of LLM4DRD is 3.17-12.39% higher than state-of-the-art methods in 20 practical instances used for training and testing, respectively. In 24 scenarios with different resource configurations, order loads, and disturbance levels totaling 480 instances, it achieves 11.10% higher performance than the second best competitor, exhibiting excellent robustness.",
      "authors": [
        "Junhao Qiu",
        "Haoyang Zhuang",
        "Fei Liu",
        "Jianjun Liu",
        "Qingfu Zhang"
      ],
      "primary_category": "cs.NE",
      "categories": [
        "cs.NE"
      ],
      "published": "2026-01-22 08:06:40+00:00",
      "link": "https://arxiv.org/pdf/2601.15738v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-assisted automated heuristic design for scheduling",
      "llm_evidence_cn": "大模型辅助的调度启发式算法自动设计",
      "llm_evidence": "大模型辅助的调度启发式算法自动设计",
      "llm_tldr_en": "Develops an LLM-assisted method for automatically designing dispatching rules in dynamic scheduling environments.",
      "llm_tldr_cn": "开发了一种大模型辅助方法，用于在动态调度环境中自动设计派发规则。",
      "llm_tldr": "开发了一种大模型辅助方法，用于在动态调度环境中自动设计派发规则。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    },
    {
      "id": "2601.16175v1",
      "title": "Learning to Discover at Test Time",
      "abstract": "How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.",
      "authors": [
        "Mert Yuksekgonul",
        "Daniel Koceja",
        "Xinhao Li",
        "Federico Bianchi",
        "Jed McCaleb",
        "Xiaolong Wang",
        "Jan Kautz",
        "Yejin Choi",
        "James Zou",
        "Carlos Guestrin",
        "Yu Sun"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-22 18:24:00+00:00",
      "link": "https://arxiv.org/pdf/2601.16175v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 8.0,
      "llm_evidence_en": "AI to discover new state of the art for scientific problems",
      "llm_evidence_cn": "利用AI发现科学问题的新SOTA",
      "llm_evidence": "利用AI发现科学问题的新SOTA",
      "llm_tldr_en": "Introduces TTT-Discover, using test-time reinforcement learning to discover novel solutions for scientific problems.",
      "llm_tldr_cn": "提出TTT-Discover方法，通过测试时强化学习为科学问题自动发现更优的解决方案。",
      "llm_tldr": "提出TTT-Discover方法，通过测试时强化学习为科学问题自动发现更优的解决方案。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2601.15127v1",
      "title": "DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search",
      "abstract": "Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS",
      "authors": [
        "Bostan Khan",
        "Masoud Daneshtalab"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV",
        "cs.DC"
      ],
      "published": "2026-01-21 16:03:25+00:00",
      "link": "https://arxiv.org/pdf/2601.15127v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Automated model design and architectural heuristics",
      "llm_evidence_cn": "自动化模型设计与架构启发式方法",
      "llm_evidence": "自动化模型设计与架构启发式方法",
      "llm_tldr_en": "Introduces DeepFedNAS, a framework using multi-objective fitness and heuristics for automated neural architecture search.",
      "llm_tldr_cn": "引入DeepFedNAS框架，利用多目标适应度和启发式方法实现自动神经架构搜索。",
      "llm_tldr": "引入DeepFedNAS框架，利用多目标适应度和启发式方法实现自动神经架构搜索。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15038v1",
      "title": "A Curriculum-Based Deep Reinforcement Learning Framework for the Electric Vehicle Routing Problem",
      "abstract": "The electric vehicle routing problem with time windows (EVRPTW) is a complex optimization problem in sustainable logistics, where routing decisions must minimize total travel distance, fleet size, and battery usage while satisfying strict customer time constraints. Although deep reinforcement learning (DRL) has shown great potential as an alternative to classical heuristics and exact solvers, existing DRL models often struggle to maintain training stability-failing to converge or generalize when constraints are dense. In this study, we propose a curriculum-based deep reinforcement learning (CB-DRL) framework designed to resolve this instability. The framework utilizes a structured three-phase curriculum that gradually increases problem complexity: the agent first learns distance and fleet optimization (Phase A), then battery management (Phase B), and finally the full EVRPTW (Phase C). To ensure stable learning across phases, the framework employs a modified proximal policy optimization algorithm with phase-specific hyperparameters, value and advantage clipping, and adaptive learning-rate scheduling. The policy network is built upon a heterogeneous graph attention encoder enhanced by global-local attention and feature-wise linear modulation. This specialized architecture explicitly captures the distinct properties of depots, customers, and charging stations. Trained exclusively on small instances with N=10 customers, the model demonstrates robust generalization to unseen instances ranging from N=5 to N=100, significantly outperforming standard baselines on medium-scale problems. Experimental results confirm that this curriculum-guided approach achieves high feasibility rates and competitive solution quality on out-of-distribution instances where standard DRL baselines fail, effectively bridging the gap between neural speed and operational reliability.",
      "authors": [
        "Mertcan Daysalilar",
        "Fuat Uyguroglu",
        "Gabriel Nicolosi",
        "Adam Meyers"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 14:42:33+00:00",
      "link": "https://arxiv.org/pdf/2601.15038v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Heuristics for vehicle routing problem",
      "llm_evidence_cn": "车辆路径问题的启发式方法",
      "llm_evidence": "车辆路径问题的启发式方法",
      "llm_tldr_en": "A curriculum-based DRL framework for the electric vehicle routing problem as an alternative to classical heuristics.",
      "llm_tldr_cn": "一种基于课程学习的深度强化学习框架，用于解决电动汽车路径规划问题。",
      "llm_tldr": "一种基于课程学习的深度强化学习框架，用于解决电动汽车路径规划问题。",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EOH"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15876v1",
      "title": "EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience",
      "abstract": "The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries -- reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.",
      "authors": [
        "Taofeng Xue",
        "Chong Peng",
        "Mianqiu Huang",
        "Linsen Guo",
        "Tiancheng Han",
        "Haozhe Wang",
        "Jianing Wang",
        "Xiaocheng Zhang",
        "Xin Yang",
        "Dengchang Zhao",
        "Jinrui Ding",
        "Xiandi Ma",
        "Yuchen Xie",
        "Peng Pei",
        "Xunliang Cai",
        "Xipeng Qiu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 11:36:43+00:00",
      "link": "https://arxiv.org/pdf/2601.15876v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Self-sustaining evolutionary cycle for agent policy optimization",
      "llm_evidence_cn": "智能体策略优化的自维持进化循环",
      "llm_evidence": "智能体策略优化的自维持进化循环",
      "llm_tldr_en": "EvoCUA uses an evolutionary cycle of data generation and policy optimization to improve computer use agents.",
      "llm_tldr_cn": "EvoCUA 通过数据生成和策略优化的进化循环来提升计算机使用智能体的性能。",
      "llm_tldr": "EvoCUA 通过数据生成和策略优化的进化循环来提升计算机使用智能体的性能。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15131v1",
      "title": "Vehicle Routing with Finite Time Horizon using Deep Reinforcement Learning with Improved Network Embedding",
      "abstract": "In this paper, we study the vehicle routing problem with a finite time horizon. In this routing problem, the objective is to maximize the number of customer requests served within a finite time horizon. We present a novel routing network embedding module which creates local node embedding vectors and a context-aware global graph representation. The proposed Markov decision process for the vehicle routing problem incorporates the node features, the network adjacency matrix and the edge features as components of the state space. We incorporate the remaining finite time horizon into the network embedding module to provide a proper routing context to the embedding module. We integrate our embedding module with a policy gradient-based deep Reinforcement Learning framework to solve the vehicle routing problem with finite time horizon. We trained and validated our proposed routing method on real-world routing networks, as well as synthetically generated Euclidean networks. Our experimental results show that our method achieves a higher customer service rate than the existing routing methods. Additionally, the solution time of our method is significantly lower than that of the existing methods.",
      "authors": [
        "Ayan Maity",
        "Sudeshna Sarkar"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-21 16:05:04+00:00",
      "link": "https://arxiv.org/pdf/2601.15131v1",
      "tags": [
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "deep reinforcement learning for vehicle routing optimization",
      "llm_evidence_cn": "深度强化学习用于车辆路径优化",
      "llm_evidence": "深度强化学习用于车辆路径优化",
      "llm_tldr_en": "Uses DRL and network embedding to solve vehicle routing problems with finite time horizons.",
      "llm_tldr_cn": "利用深度强化学习和网络嵌入解决有限时间范围内的车辆路径问题。",
      "llm_tldr": "利用深度强化学习和网络嵌入解决有限时间范围内的车辆路径问题。",
      "llm_tags": [
        "keyword:LNS"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16056v1",
      "title": "Designing faster mixed integer linear programming algorithm via learning the optimal path",
      "abstract": "Designing faster algorithms for solving Mixed-Integer Linear Programming (MILP) problems is highly desired across numerous practical domains, as a vast array of complex real-world challenges can be effectively modeled as MILP formulations. Solving these problems typically employs the branch-and-bound algorithm, the core of which can be conceived as searching for a path of nodes (or sub-problems) that contains the optimal solution to the original MILP problem. Traditional approaches to finding this path rely heavily on hand-crafted, intuition-based heuristic strategies, which often suffer from unstable and unpredictable performance across different MILP problem instances. To address this limitation, we introduce DeepBound, a deep learning-based node selection algorithm that automates the learning of such human intuition from data. The core of DeepBound lies in learning to prioritize nodes containing the optimal solution, thereby improving solving efficiency. DeepBound introduces a multi-level feature fusion network to capture the node representations. To tackle the inherent node imbalance in branch-and-bound trees, DeepBound employs a pairwise training paradigm that enhances the model's ability to discriminate between nodes. Extensive experiments on three NP-hard MILP benchmarks demonstrate that DeepBound achieves superior solving efficiency over conventional heuristic rules and existing learning-based approaches, obtaining optimal feasible solutions with significantly reduced computation time. Moreover, DeepBound demonstrates strong generalization capability on large and complex instances. The analysis of its learned features reveals that the method can automatically discover more flexible and robust feature selection, which may effectively improve and potentially replace human-designed heuristic rules.",
      "authors": [
        "Ruizhi Liu",
        "Liming Xu",
        "Xulin Huang",
        "Jingyan Sui",
        "Shizhe Ding",
        "Boyang Xia",
        "Chungong Yu",
        "Dongbo Bu"
      ],
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-22 15:41:22+00:00",
      "link": "https://arxiv.org/pdf/2601.16056v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 7.0,
      "llm_evidence_en": "Automating heuristic design for MILP search paths",
      "llm_evidence_cn": "MILP搜索路径的启发式设计自动化",
      "llm_evidence": "MILP搜索路径的启发式设计自动化",
      "llm_tldr_en": "DeepBound uses deep learning to replace hand-crafted heuristics in MILP branch-and-bound algorithms.",
      "llm_tldr_cn": "DeepBound利用深度学习取代MILP分支定界算法中的手工启发式策略。",
      "llm_tldr": "DeepBound利用深度学习取代MILP分支定界算法中的手工启发式策略。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "7"
    },
    {
      "id": "2601.15212v1",
      "title": "ZENITH: Automated Gradient Norm Informed Stochastic Optimization",
      "abstract": "Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.",
      "authors": [
        "Dhrubo Saha"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CV"
      ],
      "published": "2026-01-21 17:36:12+00:00",
      "link": "https://arxiv.org/pdf/2601.15212v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "automated evolution of learning rates",
      "llm_evidence_cn": "学习率的自动演化优化",
      "llm_evidence": "学习率的自动演化优化",
      "llm_tldr_en": "Introduces ZENITH, an optimizer that automatically adapts learning rates using gradient norm evolution history.",
      "llm_tldr_cn": "引入了ZENITH优化器，利用梯度范数的演化历史自动调整学习率。",
      "llm_tldr": "引入了ZENITH优化器，利用梯度范数的演化历史自动调整学习率。",
      "llm_tags": [
        "keyword:EOH",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15482v1",
      "title": "Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding",
      "abstract": "Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.",
      "authors": [
        "Huayu Li",
        "ZhengXiao He",
        "Siyuan Tian",
        "Jinghao Wen",
        "Ao Li"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-21 21:34:29+00:00",
      "link": "https://arxiv.org/pdf/2601.15482v1",
      "tags": [
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Theoretically-grounded search and pruning algorithm",
      "llm_evidence_cn": "理论基础扎实的搜索与剪枝算法",
      "llm_evidence": "理论基础扎实的搜索与剪枝算法",
      "llm_tldr_en": "Introduces Martingale Foresight Sampling to improve LLM decoding through principled search space pruning.",
      "llm_tldr_cn": "引入鞅预见采样，通过理论化的搜索空间剪枝改进大模型解码。",
      "llm_tldr": "引入鞅预见采样，通过理论化的搜索空间剪枝改进大模型解码。",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15561v1",
      "title": "Enhanced Convergence in p-bit Based Simulated Annealing with Partial Deactivation for Large-Scale Combinatorial Optimization Problems",
      "abstract": "This article critically investigates the limitations of the simulated annealing algorithm using probabilistic bits (pSA) in solving large-scale combinatorial optimization problems. The study begins with an in-depth analysis of the pSA process, focusing on the issues resulting from unexpected oscillations among p-bits. These oscillations hinder the energy reduction of the Ising model and thus obstruct the successful execution of pSA in complex tasks. Through detailed simulations, we unravel the root cause of this energy stagnation, identifying the feedback mechanism inherent to the pSA operation as the primary contributor to these disruptive oscillations. To address this challenge, we propose two novel algorithms, time average pSA (TApSA) and stalled pSA (SpSA). These algorithms are designed based on partial deactivation of p-bits and are thoroughly tested using Python simulations on maximum cut benchmarks that are typical combinatorial optimization problems. On the 16 benchmarks from 800 to 5,000 nodes, the proposed methods improve the normalized cut value from 0.8% to 98.4% on average in comparison with the conventional pSA.",
      "authors": [
        "Naoya Onizawa",
        "Takahiro Hanyu"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.LG"
      ],
      "published": "2026-01-22 01:01:35+00:00",
      "link": "https://arxiv.org/pdf/2601.15561v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "large-scale combinatorial optimization and simulated annealing",
      "llm_evidence_cn": "大规模组合优化与模拟退火",
      "llm_evidence": "大规模组合优化与模拟退火",
      "llm_tldr_en": "Enhances simulated annealing for large-scale combinatorial optimization by addressing oscillation issues.",
      "llm_tldr_cn": "通过解决振荡问题，增强了用于大规模组合优化的模拟退火算法。",
      "llm_tldr": "通过解决振荡问题，增强了用于大规模组合优化的模拟退火算法。",
      "llm_tags": [
        "keyword:LNS",
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15640v1",
      "title": "An Empirical Study on Ensemble-Based Transfer Learning Bayesian Optimisation with Mixed Variable Types",
      "abstract": "Bayesian optimisation is a sample efficient method for finding a global optimum of expensive black-box objective functions. Historic datasets from related problems can be exploited to help improve performance of Bayesian optimisation by adapting transfer learning methods to various components of the Bayesian optimisation pipeline. In this study we perform an empirical analysis of various ensemble-based transfer learning Bayesian optimisation methods and pipeline components. We expand on previous work in the literature by contributing some specific pipeline components, and three new real-time transfer learning Bayesian optimisation benchmarks. In particular we propose to use a weighting strategy for ensemble surrogate model predictions based on regularised regression with weights constrained to be positive, and a related component for handling the case when transfer learning is not improving Bayesian optimisation performance. We find that in general, two components that help improve transfer learning Bayesian optimisation performance are warm start initialisation and constraining weights used with ensemble surrogate model to be positive.",
      "authors": [
        "Natasha Trinkle",
        "Huong Ha",
        "Jeffrey Chan"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "stat.ML"
      ],
      "published": "2026-01-22 04:41:26+00:00",
      "link": "https://arxiv.org/pdf/2601.15640v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "efficient automatic algorithm for black-box optimization",
      "llm_evidence_cn": "黑盒优化的高效自动算法",
      "llm_evidence": "黑盒优化的高效自动算法",
      "llm_tldr_en": "Empirical study on ensemble-based transfer learning for efficient Bayesian optimization with mixed variables.",
      "llm_tldr_cn": "对混合变量类型下基于集成迁移学习的高效贝叶斯优化进行实证研究。",
      "llm_tldr": "对混合变量类型下基于集成迁移学习的高效贝叶斯优化进行实证研究。",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.15727v1",
      "title": "Towards Automated Kernel Generation in the Era of LLMs",
      "abstract": "The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation.",
      "authors": [
        "Yang Yu",
        "Peiyu Zang",
        "Chi Hsu Tsai",
        "Haiming Wu",
        "Yixin Shen",
        "Jialing Zhang",
        "Haoyu Wang",
        "Zhiyou Xiao",
        "Jingze Shi",
        "Yuyu Luo",
        "Wentao Zhang",
        "Chunlei Men",
        "Guang Liu",
        "Yonghua Lin"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CL"
      ],
      "published": "2026-01-22 07:53:52+00:00",
      "link": "https://arxiv.org/pdf/2601.15727v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Automated kernel generation and iterative feedback-driven optimization",
      "llm_evidence_cn": "自动内核生成与迭代反馈驱动优化",
      "llm_evidence": "自动内核生成与迭代反馈驱动优化",
      "llm_tldr_en": "Explores using LLM-based agents for automated, scalable kernel generation and hardware-level optimization.",
      "llm_tldr_cn": "探索利用基于大模型的智能体进行自动化、可扩展的内核生成和硬件级优化。",
      "llm_tldr": "探索利用基于大模型的智能体进行自动化、可扩展的内核生成和硬件级优化。",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16025v1",
      "title": "EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery",
      "abstract": "Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.",
      "authors": [
        "Yajuan Xu",
        "Xixian Han",
        "Xiaolong Wan"
      ],
      "primary_category": "cs.DB",
      "categories": [
        "cs.DB"
      ],
      "published": "2026-01-22 14:52:36+00:00",
      "link": "https://arxiv.org/pdf/2601.16025v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "efficient automatic algorithm for discovery",
      "llm_evidence_cn": "高效自动发现算法",
      "llm_evidence": "高效自动发现算法",
      "llm_tldr_en": "EAIFD is a fast, scalable algorithm for incremental functional dependency discovery in databases.",
      "llm_tldr_cn": "EAIFD 是一种用于数据库增量函数依赖发现的高效、可扩展算法。",
      "llm_tldr": "EAIFD 是一种用于数据库增量函数依赖发现的高效、可扩展算法。",
      "llm_tags": [
        "keyword:EAA"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2601.16156v1",
      "title": "All ascents exponential from valued constraint graphs of pathwidth three",
      "abstract": "Many combinatorial optimization problems can be formulated as finding as assignment that maximized some pseudo-Boolean function (that we call the fitness function). Strict local search starts with some assignment and follows some update rule to proceed to an adjacent assignment of strictly higher fitness. This means that strict local search algorithms follow ascents in the fitness landscape of the pseudo-Boolean function. The complexity of the pseudo-Boolean function (and the fitness landscapes that it represents) can be parameterized by properties of the valued constraint satisfaction problem (VCSP) that encodes the pseudo-Boolean function. We focus on properties of the constraint graphs of the VCSP, with the intuition that spare graphs are less complex than dense ones. Specifically, we argue that pathwidth is the natural sparsity parameter for understanding limits on the power of strict local search. We show that prior constructions of sparse VCSPs where all ascents are exponentially long had pathwidth greater than or equal to four. We improve this this with our controlled doubling construction: a valued constraint satisfaction problem of pathwidth three where all ascents are exponentially long from a designated initial assignment. From this, we conclude that all strict local search algorithms can be forced to take an exponential number of steps even on simple valued constraint graphs of pathwidth three.",
      "authors": [
        "Artem Kaznatcheev",
        "Willemijn Volgering"
      ],
      "primary_category": "cs.DM",
      "categories": [
        "cs.DM",
        "cs.DS"
      ],
      "published": "2026-01-22 17:57:54+00:00",
      "link": "https://arxiv.org/pdf/2601.16156v1",
      "tags": [
        "keyword:EAA",
        "keyword:EOH",
        "keyword:LNS"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "local search algorithms and combinatorial optimization",
      "llm_evidence_cn": "局部搜索算法与组合优化",
      "llm_evidence": "局部搜索算法与组合优化",
      "llm_tldr_en": "Analyzes the complexity of strict local search ascents in fitness landscapes of pseudo-Boolean functions.",
      "llm_tldr_cn": "分析了伪布尔函数适应度景观中严格局部搜索上升的复杂性。",
      "llm_tldr": "分析了伪布尔函数适应度景观中严格局部搜索上升的复杂性。",
      "llm_tags": [
        "keyword:LNS"
      ],
      "quick_tier": "6"
    }
  ]
}