{
  "mode": "standard",
  "generated_at": "2026-02-10T04:47:40.182123+00:00",
  "stats": {
    "mode": "standard",
    "tag_count": 1,
    "deep_divecandidates": 5,
    "deep_cap": 6,
    "deep_selected": 5,
    "quick_candidates": 4,
    "quick_skim_target": 11,
    "quick_selected": 4
  },
  "deep_dive": [
    {
      "id": "2602.07834v1",
      "title": "Interpretable Analytic Calabi-Yau Metrics via Symbolic Distillation",
      "abstract": "Calabi--Yau manifolds are essential for string theory but require computing intractable metrics. Here we show that symbolic regression can distill neural approximations into simple, interpretable formulas. Our five-term expression matches neural accuracy ($R^2 = 0.9994$) with 3,000-fold fewer parameters. Multi-seed validation confirms that geometric constraints select essential features, specifically power sums and symmetric polynomials, while permitting structural diversity. The functional form can be maintained across the studied moduli range ($ψ\\in [0, 0.8]$) with coefficients varying smoothly; we interpret these trends as empirical hypotheses within the accuracy regime of the locally-trained teachers ($σ\\approx 8-9\\%$ at $ψ\\neq 0$). The formula reproduces physical observables -- volume integrals and Yukawa couplings -- validating that symbolic distillation recovers compact, interpretable models for quantities previously accessible only to black-box networks.",
      "authors": [
        "D Yang Eng"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "math.DG"
      ],
      "published": "2026-02-08 05:51:35+00:00",
      "link": "https://arxiv.org/pdf/2602.07834v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "symbolic regression for distilling neural approximations into interpretable formulas",
      "llm_evidence_cn": "利用符号回归将神经网络近似蒸馏为可解释公式",
      "llm_evidence": "利用符号回归将神经网络近似蒸馏为可解释公式",
      "llm_tldr_en": "Uses symbolic regression to find interpretable analytic metrics for Calabi-Yau manifolds in string theory.",
      "llm_tldr_cn": "利用符号回归为弦理论中的卡拉比-丘流形寻找可解释的解析度规。",
      "llm_tldr": "利用符号回归为弦理论中的卡拉比-丘流形寻找可解释的解析度规。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08270v1",
      "title": "A few-shot and physically restorable symbolic regression turbulence model based on normalized general effective-viscosity hypothesis",
      "abstract": "Turbulence is a complex, irregular flow phenomenon ubiquitous in natural processes and engineering applications. The Reynolds-averaged Navier-Stokes (RANS) method, owing to its low computational cost, has become the primary approach for rapid simulation of engineering turbulence problems. However, the inaccuracy of classical turbulence models constitutes the main drawback of the RANS framework. With the rapid development of data-driven approaches, many data-driven turbulence models have been proposed, yet they still suffer from issues of generalizability and accuracy. In this work, we propose a few-shot, physically restorable, symbolic regression turbulence model based on the normalized general effective-viscosity hypothesis. Few-shot indicates that our model is trained on limited flow configurations spanning only a narrow subset of turbulent flow physics, yet can still outperform the baseline model in substantially different turbulent flows. Physically restorable means our model can nearly revert to the baseline model in regimes satisfying specific physical conditions, using only the symbolic regression training results. The normalized general effective-viscosity hypothesis was proposed in our previous study. Specifically, we first formalize the concept of few-shot data-driven turbulence models. Second, we train our symbolic regression turbulence models using only direct numerical simulation (DNS) data for three-dimensional periodic hill flow slices. Third, we evaluate our models on periodic hill flows, zero pressure gradient flat plate flow, NACA0012 airfoil flows, and NASA Rotor 37 transonic axial compressor flows. One of our symbolic regression turbulence models consistently outperforms the baseline model, and we further demonstrate that this model can nearly revert to baseline behavior in certain flow regimes.",
      "authors": [
        "Ziqi Ji",
        "Penghao Duan",
        "Gang Du"
      ],
      "primary_category": "physics.flu-dyn",
      "categories": [
        "physics.flu-dyn"
      ],
      "published": "2026-02-09 05:08:04+00:00",
      "link": "https://arxiv.org/pdf/2602.08270v1",
      "tags": [
        "keyword:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "novel symbolic regression model for turbulence modeling in physics",
      "llm_evidence_cn": "用于物理湍流建模的新型符号回归模型",
      "llm_evidence": "用于物理湍流建模的新型符号回归模型",
      "llm_tldr_en": "A few-shot symbolic regression model for turbulence, demonstrating interdisciplinary application in fluid dynamics.",
      "llm_tldr_cn": "一种用于湍流的少样本符号回归模型，展示了在流体动力学中的跨学科应用。",
      "llm_tldr": "一种用于湍流的少样本符号回归模型，展示了在流体动力学中的跨学科应用。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.08885v1",
      "title": "Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression",
      "abstract": "Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.",
      "authors": [
        "Paul Saegert",
        "Ullrich Köthe"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.SC"
      ],
      "published": "2026-02-09 16:47:00+00:00",
      "link": "https://arxiv.org/pdf/2602.08885v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 10.0,
      "llm_evidence_en": "Algorithmic advancement in amortized symbolic regression",
      "llm_evidence_cn": "摊销符号回归的算法改进",
      "llm_evidence": "摊销符号回归的算法改进",
      "llm_tldr_en": "Introduces SimpliPy to accelerate expression simplification in amortized symbolic regression by 100x.",
      "llm_tldr_cn": "引入 SimpliPy，将摊销符号回归中的表达式简化速度提高 100 倍。",
      "llm_tldr": "引入 SimpliPy，将摊销符号回归中的表达式简化速度提高 100 倍。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.07360v1",
      "title": "In-Context System Identification for Nonlinear Dynamics Using Large Language Models",
      "abstract": "Sparse Identification of Nonlinear Dynamics (SINDy) is a powerful method for discovering parsimonious governing equations from data, but it often requires expert tuning of candidate libraries. We propose an LLM-aided SINDy pipeline that iteratively refines candidate equations using a large language model (LLM) in the loop through in-context learning. The pipeline begins with a baseline SINDy model fit using an adaptive library and then enters a LLM-guided refinement cycle. At each iteration, the current best equations, error metrics, and domain-specific constraints are summarized in a prompt to the LLM, which suggests new equation structures. These candidate equations are parsed against a defined symbolic form and evaluated on training and test data. The pipeline uses simulation-based error as a primary metric, but also assesses structural similarity to ground truth, including matching functional forms, key terms, couplings, qualitative behavior. An iterative stopping criterion ends refinement early if test error falls below a threshold (NRMSE < 0.1) or if a maximum of 10 iterations is reached. Finally, the best model is selected, and we evaluate this LLM-aided SINDy on 63 dynamical system datasets (ODEBench) and march leuba model for boiling nuclear reactor. The results are compared against classical SINDy and show the LLM-loop consistently improves symbolic recovery with higher equation similarity to ground truth and lower test RMSE than baseline SINDy for cases with complex dynamics. This work demonstrates that an LLM can effectively guide SINDy's search through equation space, integrating data-driven error feedback with domain-inspired symbolic reasoning to discover governing equations that are not only accurate but also structurally interpretable.",
      "authors": [
        "Linyu Lin"
      ],
      "primary_category": "eess.SY",
      "categories": [
        "eess.SY"
      ],
      "published": "2026-02-07 04:52:33+00:00",
      "link": "https://arxiv.org/pdf/2602.07360v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "LLM-aided discovery of parsimonious governing equations",
      "llm_evidence_cn": "LLM辅助发现简约控制方程",
      "llm_evidence": "LLM辅助发现简约控制方程",
      "llm_tldr_en": "Proposes an LLM-aided SINDy pipeline to iteratively refine symbolic equations for nonlinear dynamics identification.",
      "llm_tldr_cn": "提出一种LLM辅助的SINDy流水线，通过迭代优化符号方程来识别非线性动力学系统。",
      "llm_tldr": "提出一种LLM辅助的SINDy流水线，通过迭代优化符号方程来识别非线性动力学系统。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    },
    {
      "id": "2602.07651v1",
      "title": "Cosmology with one galaxy: An analytic formula relating $Ω_{\\rm m}$ with galaxy properties",
      "abstract": "Standard cosmological analyses typically treat galaxy formation and cosmological parameter inference as decoupled problems, relying on population-level statistics such as clustering, lensing, or halo abundances. However, classical studies of baryon fractions in massive galaxy clusters have long suggested that gravitationally bound systems may retain cosmological information through their baryonic content. Building on this insight, we present the first analytic and physically interpretable cosmological tracer that links the matter density parameter, $Ω_m$, directly to intrinsic galaxy-scale observables, demonstrating that cosmological information can be extracted from individual galaxies. Using symbolic regression applied to state-of-the-art hydrodynamical simulations from the CAMELS project, we identify a compact functional form that robustly recovers $Ω_m$ across multiple simulation suites (IllustrisTNG, ASTRID, SIMBA, and Swift-EAGLE), requiring only modest recalibration of a small number of coefficients. The resulting expression admits a transparent physical interpretation in terms of baryonic retention and enrichment efficiency regulated by gravitational potential depth, providing a clear explanation for why $Ω_m$ is locally encoded in galaxy properties. Our work establishes a direct, interpretable bridge between small-scale galaxy physics and large-scale cosmology, opening a complementary pathway to cosmological inference that bypasses traditional clustering-based statistics and enables new synergies between galaxy formation theory and precision cosmology.",
      "authors": [
        "Kito Liao",
        "Francisco Villaescusa-Navarro",
        "Romain Teysser",
        "Natalí S. M. de Santi"
      ],
      "primary_category": "astro-ph.CO",
      "categories": [
        "astro-ph.CO",
        "astro-ph.GA"
      ],
      "published": "2026-02-07 18:23:07+00:00",
      "link": "https://arxiv.org/pdf/2602.07651v1",
      "tags": [
        "keyword:SR"
      ],
      "llm_score": 9.0,
      "llm_evidence_en": "Application of symbolic regression in cosmology",
      "llm_evidence_cn": "符号回归在宇宙学中的应用",
      "llm_evidence": "符号回归在宇宙学中的应用",
      "llm_tldr_en": "Uses symbolic regression to derive an analytic formula for matter density from individual galaxy properties.",
      "llm_tldr_cn": "利用符号回归从单个星系属性中推导出物质密度的解析公式。",
      "llm_tldr": "利用符号回归从单个星系属性中推导出物质密度的解析公式。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ]
    }
  ],
  "quick_skim": [
    {
      "id": "2602.07518v1",
      "title": "Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units",
      "abstract": "Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities efficiently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfigurable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input-output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a reconfigurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transformations. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of $\\sim$250 pJ and an end-to-end inference latency of $\\sim$600 ns for a representative workload, corresponding to a $\\sim$10$^{2}$-10$^{3}\\times$ reduction in energy accompanied by a $\\sim$10$\\times$ reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify analog KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference.",
      "authors": [
        "Manuel Escudero",
        "Mohamadreza Zolfagharinejad",
        "Sjoerd van den Belt",
        "Nikolaos Alachiotis",
        "Wilfred G. van der Wiel"
      ],
      "primary_category": "cs.ET",
      "categories": [
        "cs.ET",
        "cs.AR",
        "cs.LG",
        "nlin.AO"
      ],
      "published": "2026-02-07 12:33:11+00:00",
      "link": "https://arxiv.org/pdf/2602.07518v1",
      "tags": [
        "keyword:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Kolmogorov-Arnold Networks as alternative to symbolic regression",
      "llm_evidence_cn": "KAN网络作为符号回归的替代方案",
      "llm_evidence": "KAN网络作为符号回归的替代方案",
      "llm_tldr_en": "Implements analog Kolmogorov-Arnold Networks (KANs) using reconfigurable nonlinear-processing units.",
      "llm_tldr_cn": "利用可重构非线性处理单元实现了模拟Kolmogorov-Arnold网络（KAN），是符号建模的新硬件方案。",
      "llm_tldr": "利用可重构非线性处理单元实现了模拟Kolmogorov-Arnold网络（KAN），是符号建模的新硬件方案。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.07659v1",
      "title": "Continuous Program Search",
      "abstract": "Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.   We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.   Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.",
      "authors": [
        "Matthew Siper",
        "Muhammad Umair Nasir",
        "Ahmed Khalifa",
        "Lisa Soros",
        "Jay Azhang",
        "Julian Togelius"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "q-fin.ST"
      ],
      "published": "2026-02-07 18:41:14+00:00",
      "link": "https://arxiv.org/pdf/2602.07659v1",
      "tags": [
        "keyword:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Continuous program search for genetic programming",
      "llm_evidence_cn": "遗传编程的连续程序搜索",
      "llm_evidence": "遗传编程的连续程序搜索",
      "llm_tldr_en": "Learns a continuous program space to improve locality and efficiency in genetic programming search.",
      "llm_tldr_cn": "学习连续程序空间以提高遗传编程搜索的局部性和效率。",
      "llm_tldr": "学习连续程序空间以提高遗传编程搜索的局部性和效率。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.07970v1",
      "title": "Learning-guided Kansa collocation for forward and inverse PDEs beyond linearity",
      "abstract": "Partial Differential Equations are precise in modelling the physical, biological and graphical phenomena. However, the numerical methods suffer from the curse of dimensionality, high computation costs and domain-specific discretization. We aim to explore pros and cons of different PDE solvers, and apply them to specific scientific simulation problems, including forwarding solution, inverse problems and equations discovery. In particular, we extend the recent CNF (NeurIPS 2023) framework solver to multi-dependent-variable and non-linear settings, together with down-stream applications. The outcomes include implementation of selected methods, self-tuning techniques, evaluation on benchmark problems and a comprehensive survey of neural PDE solvers and scientific simulation applications.",
      "authors": [
        "Zheyuan Hu",
        "Weitao Chen",
        "Cengiz Öztireli",
        "Chenliang Zhou",
        "Fangcheng Zhong"
      ],
      "primary_category": "cs.CE",
      "categories": [
        "cs.CE",
        "cs.AI",
        "cs.LG",
        "math.NA"
      ],
      "published": "2026-02-08 13:44:36+00:00",
      "link": "https://arxiv.org/pdf/2602.07970v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "equation discovery and inverse problems in PDEs",
      "llm_evidence_cn": "偏微分方程中的方程发现与逆问题",
      "llm_evidence": "偏微分方程中的方程发现与逆问题",
      "llm_tldr_en": "Explores neural PDE solvers for equation discovery and inverse problems across scientific domains.",
      "llm_tldr_cn": "探索用于科学领域方程发现和逆问题的神经偏微分方程求解器。",
      "llm_tldr": "探索用于科学领域方程发现和逆问题的神经偏微分方程求解器。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    },
    {
      "id": "2602.08733v1",
      "title": "Foundation Inference Models for Ordinary Differential Equations",
      "abstract": "Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.",
      "authors": [
        "Maximilian Mauel",
        "Johannes R. Hübers",
        "David Berghaus",
        "Patrick Seifner",
        "Ramses J. Sanchez"
      ],
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "published": "2026-02-09 14:39:11+00:00",
      "link": "https://arxiv.org/pdf/2602.08733v1",
      "tags": [
        "keyword:SR",
        "query:SR"
      ],
      "llm_score": 6.0,
      "llm_evidence_en": "Comparison with symbolic regression for ODE inference",
      "llm_evidence_cn": "在ODE推断中与符号回归进行对比",
      "llm_evidence": "在ODE推断中与符号回归进行对比",
      "llm_tldr_en": "Proposes a foundation model for ODE inference as an alternative to traditional symbolic regression methods.",
      "llm_tldr_cn": "提出一种用于 ODE 推断的基础模型，作为传统符号回归方法的替代方案。",
      "llm_tldr": "提出一种用于 ODE 推断的基础模型，作为传统符号回归方法的替代方案。",
      "llm_tags": [
        "keyword:SR",
        "query:SR"
      ],
      "quick_tier": "6"
    }
  ]
}