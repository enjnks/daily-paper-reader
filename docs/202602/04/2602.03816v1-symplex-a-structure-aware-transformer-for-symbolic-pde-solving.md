---
title: "SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving"
title_zh: SymPlex：一种用于符号偏微分方程求解的结构感知 Transformer
authors: "Yesom Park, Annie C. Lu, Shao-Ching Huang, Qiyang Hu, Y. Sungtaek Ju, Stanley Osher"
date: 2026-02-03
pdf: "https://arxiv.org/pdf/2602.03816v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 使用Transformer和强化学习的符号求解算法创新
tldr: SymPlex 是一个利用强化学习发现偏微分方程（PDE）解析符号解的框架，无需真实表达式作为监督。其核心是 SymFormer，一种结构感知 Transformer，通过树状相对自注意力和语法约束解码来建模符号依赖并确保语法正确。该方法直接在符号表达式空间操作，能生成可解释、可处理非平滑行为及参数依赖的解，实现了对复杂 PDE 精确符号解的自动发现。
motivation: 传统的数值和神经求解器缺乏可解释性，且难以处理非平滑行为或显式参数依赖，因此需要一种能直接生成解析符号解的方法。
method: 提出 SymPlex 框架，利用强化学习优化由 SymFormer 生成的树状决策，并通过语法约束的自回归解码确保符号表达式的合法性。
result: 实验证明该方法仅凭 PDE 方程和边界条件即可精确还原非平滑及参数化 PDE 的符号解。
conclusion: SymPlex 证明了深度学习符号化方法在求解 PDE 方面的潜力，为获取高可解释性和精确性的数学解提供了新途径。
---

## 摘要
我们提出了 SymPlex，这是一个强化学习框架，用于在无法获取真值表达式的情况下发现偏微分方程（PDE）的解析符号解。SymPlex 将符号 PDE 求解建模为树状结构决策，并仅利用 PDE 及其边界条件来优化候选解。其核心是 SymFormer，这是一种结构感知 Transformer，它通过树相对自注意力（tree-relative self-attention）对层级符号依赖进行建模，并通过语法约束的自回归解码来强制执行语法有效性，从而克服了基于序列的生成器表达能力有限的问题。与在离散或隐式函数空间中逼近解的数值和神经方法不同，SymPlex 直接在符号表达式空间中运行，从而能够生成可解释且人类可读的解，这些解能够自然地表示非光滑行为和显式参数依赖。实验结果表明，使用基于深度学习的符号方法可以精确恢复非光滑和参数化 PDE 的解。

## Abstract
We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.

---

## 论文详细总结（自动生成）

这是一份关于论文《SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving》的结构化深入总结：

### 1. 核心问题与研究动机
*   **核心问题**：如何在没有真值表达式（Ground-truth）监督的情况下，自动发现偏微分方程（PDE）的精确解析符号解。
*   **研究动机**：
    *   **传统数值法（如FDM、FEM）**：依赖网格离散化，在处理尖锐梯度或不连续性时会产生数值扩散，且缺乏解析可解释性。
    *   **神经网络法（如PINNs）**：虽然是连续的，但属于黑盒模型，难以进行解析外推，且存在逼近偏差。
    *   **符号解的优势**：具有完全的可解释性、精确性，能显式表达参数依赖关系，且存储极小。
    *   **挑战**：符号空间搜索是离散组合优化问题，且在求解未知 PDE 时缺乏目标标签，只能通过方程残差提供稀疏奖励。

### 2. 方法论
SymPlex 框架将符号发现建模为树状结构的强化学习（RL）决策过程，核心组件包括：
*   **SymFormer（结构感知 Transformer）**：
    *   **树相对自注意力（Tree-Relative Self-Attention）**：不同于标准 Transformer 的线性注意力，它根据抽象语法树（AST）中的层级关系（父子、兄弟、祖先等）分配学习到的嵌入，增强了对数学逻辑结构的建模能力。
    *   **语法约束解码**：在自回归生成过程中，根据算子的元数（Arity）动态限制可选 Token，确保生成的表达式在数学语法上始终合法。
    *   **遍历感知位置编码**：结合前缀遍历顺序，区分不同子树中具有相似结构角色的节点。
*   **强化学习训练流程**：
    *   **奖励函数**：基于 PDE 残差、边界条件（BC）和初始条件（IC）的 $L_2$ 范数构建。
    *   **常数优化**：在评估奖励前，利用梯度下降对符号结构中的连续常数进行微调，解耦了结构搜索与参数拟合。
    *   **多样性感知 Top-k 记忆库**：存储高奖励且结构/语义各异的表达式，通过模仿学习（Imitation Learning）稳定训练。
    *   **课程学习（Curriculum Learning）**：分阶段训练（空间变量 $\rightarrow$ 时空变量 $\rightarrow$ 全参数化解），降低搜索空间的复杂度。

### 3. 实验设计
*   **测试场景**：
    *   **平滑解问题**：Poisson、平流（Advection）、热传导（Heat）方程。
    *   **非平滑解问题**：Eikonal 方程、Burgers 方程（含不连续性/扭结）。
    *   **参数化解问题**：含物理系数 $\kappa$ 的平流和热传导方程。
*   **基准方法（Benchmarks）**：
    *   **SSDE**：基于 RNN 的符号 PDE 求解器。
    *   **FEX**：确定性树搜索方法。
    *   **PINN+DSR**：先训练 PINN 再用符号回归拟合其输出。
    *   **KAN**：基于 Kolmogorov-Arnold 网络的符号构建。
    *   **WENO**：经典的数值格式（用于性能对比）。

### 4. 资源与算力
*   **硬件**：所有实验均在单台 **NVIDIA GV100 (TITAN V)** GPU 上完成。
*   **训练细节**：每个课程阶段最多 500 个 Epoch，每步采样 64 个序列。虽然训练时间（约 642 秒）长于数值法，但推理速度极快（$10^{-5}$ 秒量级），且存储需求极低（0.07 MB）。

### 5. 实验数量与充分性
*   **实验规模**：涵盖了 5 种主要 PDE 类型，每种类型包含多个变体（平滑、非平滑、参数化）。
*   **充分性**：
    *   对比了 4 种最先进的符号/神经基准方法。
    *   针对基准方法运行了 20 个不同的随机种子以确保公平性。
    *   包含了消融实验（如课程学习的有效性验证）。
    *   **客观性**：不仅测量了均方误差（MSE），还引入了符号恢复率（SRR），直接验证是否找到了正确的数学形式。

### 6. 主要结论与发现
*   **精确恢复**：SymPlex 在所有测试的 PDE（包括非线性、不连续和参数化案例）中均实现了 **100% 的符号恢复率（SRR）**，数值误差接近于零。
*   **处理非平滑性**：在处理 Burgers 等含扭结的方程时，符号解完全避免了数值方法常见的震荡或扩散。
*   **参数化泛化**：通过将物理系数视为符号变量，SymPlex 能直接发现解对参数的显式依赖关系，这是传统数值法无法做到的。

### 7. 优点与亮点
*   **结构归纳偏置**：SymFormer 通过树相对注意力显式建模数学层级，比线性序列模型更符合数学本质。
*   **无监督发现**：完全不依赖真值解，仅凭物理方程驱动，具有很强的自主探索能力。
*   **课程学习策略**：有效地解决了符号搜索空间随表达式深度呈指数级增长的“组合爆炸”问题。

### 8. 不足与局限
*   **高维扩展性**：尽管有课程学习，但在处理极高维度的 PDE 时，搜索空间依然巨大，可能面临收敛困难。
*   **词汇表依赖**：符号发现的效果高度依赖于预定义的算子库（词汇表）。如果解包含库之外的特殊函数，模型将无法找到精确解。
*   **计算开销**：相比简单的数值模拟，训练一个 Transformer 策略网络并进行强化学习的计算成本较高。
*   **收敛保证**：作为一种基于 RL 的启发式搜索，目前还缺乏对复杂非线性 PDE 搜索过程的严格数学收敛性证明。

（完）
