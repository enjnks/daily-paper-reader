Title: From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers

URL Source: https://arxiv.org/pdf/2602.06923v1

Published Time: Mon, 09 Feb 2026 02:16:36 GMT

Number of Pages: 16

Markdown Content:
# From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers 

Ziming Liu 1 Sophia Sanborn 1 Surya Ganguli 1 Andreas Tolias 1

# Abstract 

Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on ‚Äúworld models‚Äù‚Äîcausal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previ-ous ‚ÄúAI Physicist‚Äù approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively ‚Äúbake in‚Äù the physics. Conversely, Vafa et al. (2025) re-cently showed that generic Transformers fail to acquire these world models, achieving high pre-dictive accuracy without capturing the underlying physical laws. We bridge this gap by systemati-cally introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: tem-poral locality . By restricting the attention win-dow to the immediate past‚Äîimposing the sim-ple assumption that future states depend only on the local state rather than a complex history‚Äîwe force the model to abandon curve-fitting and dis-cover Newtonian force representations. Our re-sults demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward auto-mated scientific discovery. 

# 1. Introduction 

Given the broad skills and knowledge demonstrated by foun-dation models (Brown et al., 2020; Chowdhery et al., 2023; 

> 1

Stanford University. Correspondence to: Ziming Liu <zm-liu1@stanford.edu >, Andreas Tolias <tolias@stanford.edu >.

Touvron et al., 2023; Radford et al., 2021; Alayrac et al., 2022; Liu et al., 2023; Zitkovich et al., 2023; Kim et al., 2024; Reed et al., 2022), it is natural to expect that they possess robust internal ‚Äúworld models‚Äù‚Äîcausal abstrac-tions that do not merely predict what happens next (e.g., Kepler‚Äôs geometric fits), but capture the simple physical mechanisms determining why it happens (e.g., Newton‚Äôs dynamical laws). This expectation raises a central question: do world models truly emerge within foundation models? Answeing this question is a challenge. These models are highly com-plex, and the notion of a ‚Äúworld model‚Äù is often context-dependent or vaguely defined. Thus, it is useful to study world-model emergence in simple, controlled settings where the ground truth is well understood ‚Äì for instance, Newto-nian physics, in which the governing ‚Äúworld model‚Äù reduces to a set of simple differential equations. In this vein, Vafa et al. (2025) used planetary motion as a testbed and found that although a transformer can make highly accurate pre-dictions, gravitational forces fail to emerge in its internal representations, even when a GPT-2-scale transformer is trained on datasets as large as 20B tokens. However, the rea-son behind the failure remains unclear. The central research question of this paper is thus: Research Question: Why do transformers fail to learn the Newtonian world model for planetary motion, and how can we fix this problem? Answering this is a critical litmus test for the vision of de-veloping ‚ÄòAI Scientists‚Äô: if general-purpose architectures cannot recover the simple, known laws of classical mechan-ics, they are unlikely to be trusted to discover the unknown laws of novel phenomena. We can gain some insights from the success of ‚ÄúAI physi-cist‚Äù models (Wu & Tegmark, 2019; Brunton et al., 2016; Cranmer et al., 2020; Lemos et al., 2023; Liu & Tegmark, 2021; Liu et al., 2022; 2024; Udrescu & Tegmark, 2020), which not only make accurate predictions but also discover symbolic laws underlying the data ‚Äì i.e., they successfully recover ‚Äúworld models‚Äù ‚Äì often in settings more complex 1

> arXiv:2602.06923v1 [cs.LG] 6 Feb 2026

Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? [ ]1 [2]  [3]  [4]  [5]  [6]  [7]  [8]  [9]  [10] 

> [11]
> [12]
> [13]
> [14]
> [15]
> [16]
> [17]
> [18]
> [19]
> [20]

[ 3 ] 

[16] 

[ 5 ] 

[18] 

[ 6 ] 

[19] 

[ 7 ] 

[20] 

[ 9 ] 

[20] 

x

y

> Spatial
> Smoothness
> (Sec 2)
> Spatial
> Stability
> (Sec 3)
> Temporal
> Locality
> (Sec 4)

Inductive 

Biases  Failure  Modes  Fixed 

> Better spatial map
> More robust prediction
> From Kepler to Newton

Problem Setup of Vafa et al. 

Inductive Biases Fix Failures 

Two Tales: Keplerian vs Newtonian World Model     

> Use all past states to estimate the ellipse,
> then evolve by continuing the elliptical curve
> current state
> next state

# üåû 

a

b

> past states

Kepler‚Äôs first law 

> Every planet orbits the sun in an ellipse,
> with the Sun located at one of the two foci
> (focus points) of that ellipse.

Keplerian world model 

Key quantities       

> Semi-major axis length a
> Semi-minor axis length b
> Laplace-Runge-Lenz vector  ‚Éó A=(Ax,Ay)

# üåû 

Fx

Fy

F    

> use two recent states to estimate grav force,
> then evolve by simulating the equation

Newton‚Äôs second law 

Newtonian world model 

Key quantities      

> Gravitational force magnitude |‚ÉóF|=GM/r2

F = ma = m d2‚Éó r

dt2        

> Gravitational force x-component Fx=‚àíGMx/r3
> Gravitational force y-component Fy=‚àíGMy/r3

Kepler  Newton    

> LRL vector ‚ÉóA

‚ÄúContext length‚Äù = all  ‚ÄúContext length‚Äù = 2 

> dependence
> Next-token(s) prediction

# üåû 

Figure 1. Visual abstract. Top left: The problem setup of Vafa et al. (2025): planetary motion prediction is formulated as next token(s) prediction. Bottom left: Inductive biases are key to learning Newtonian world models. Three inductive biases are identified and used to fix respective failure modes. Right: The context length controls the world model learned by transformers. Long context lengths lead to the Keplerian model (global, geometry-based), while small context lengths lead to the Newtonian model (local, force-based). 

than planetary motion. The key for these AI physicist mod-els to succeed is that they typically incorporate stronger inductive biases than transformers. We are thus motivated to study what inductive biases are lacking in transformers and how we can fix them. We find that simple and general inductive biases, like spatial smoothness, temporal conti-nuity and temporal locality, are powerful enough to induce correct world models. The inductive biases do not need to know that much about the underlying law to be learned, but without them, it is impossible to learn. We identify three key inductive biases required by a world model: 

Inductive bias 1: spatial smoothness . Default tokeniza-tion discretizes continuous spatial coordinates ‚Éór = ( x, y )

into bins (tokens), each represented by a randomly initial-ized, learnable embedding vector. This discretization breaks spatial smoothness, because two points that are close in physical space but fall into different bins are treated by the transformer as completely unrelated (at least prior to train-ing). One might hope that the model could learn a good spatial map given enough compute and data, but the spa-tial map does not fully emerge in the setup of Vafa et al. (2025), even though their model size, data size and train-ing compute are comparable to GPT-2‚Äìscale models. This spatial smoothness problem may be relevant for any train-ing paradigm involving tokenization, which motivates us to study how the emergence of a spatial map depends on key hyperparameters, i.e., vocabulary size V , training data size 

D, and embedding dimension N , which exhibit intriguing scaling behaviors detailed in Section 2. If one insists on using tokenization, one must carefully choose V, D, N to maximize spatial map emergence. An-other solution, which is arguably simpler and more natural, is to use continuous coordinates without discretizing them. This, however, would lead to a stability problem stated be-low. 2Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion?    

> Figure 2. Analyzing the embeddings of the transformer model used in Vafa et al. (2025). (a) Illustration of training dynamics of token embeddings: embeddings are randomly initialized (left), gradually gain spatial structure during training (middle), requiring substantial compute and data to reach true spatial map (right). (b) The learned embeddings exhibit poor locality: circular structures in the true coordinate space (left) fragment into four point clouds, losing fine-grained structure within each quadrant (right). (c) Learned embeddings show poor linear decodability to the true spatial map (left for x, right for y).

Inductive bias 2: spatial stability. It is known that auto-regressive models suffer severely from error accumulation when dealing with continuous variables (Ren et al., 2025). In addition, Vafa et al. (2025) reported that discretized coor-dinates trained with cross-entropy loss (classification) per-formed better than continuous coordinates trained with MSE loss (regression). However, as continuous coordinates nat-urally guarantee spatial smoothness, we believe they merit further investigation. In fact, inference robustness can be significantly improved by injecting noise into the training contexts‚Äîa strategy known as noisy context learning (Ren et al., 2025). With this mitigation in place, we find that regression consistently outperforms classification across all data scales we evaluate. We elaborate on this regression-related failure mode and its remedy in Section 3. 

Inductive bias 3: temporal locality. Newtonian mechan-ics has temporal locality since it is a second-order differen-tial equation, i.e., when the time interval ‚àÜt is small enough, the next state ‚Éór(t + ‚àÜ t) is solely dependent on the current state ‚Éór(t) and the previous state ‚Éór(t ‚àí ‚àÜt) , but no states before that. This is different from a default transformer, which has a long context length (1k or longer). This inspires us to vary the context length to control temporal locality. Surprisingly, we find that: temporal locality induces the transformer to be a Newtonian world model, while lack of this knowledge induces a Keplerian world model‚Äîfitting elliptical equations based on all previous points and making predictions by continuing the curve. By contrast, a Newto-nian world model would compute gravitational forces based on temporally local states and then make predictions by simulating the differential equation (see Figure 1 for an il-lustration). We elaborate on the two stories about Kepler versus Newton in Section 4. The main findings and contributions in Section 2, 3 and 4 are summarized in Figure 1. Conclusions and discus-sions are in Section 5. Codes are available at https:// github.com/KindXiaoming/newton-kepler .

# 2. Inductive Bias 1: Spatial Smoothness 

2.1. Problem setup 

Vafa et al. (2025) trained a GPT-2-scale transformer model to predict planetary motion. They reduced the problem to 2D, placing the sun at (0 , 0) and representing the planet‚Äôs position (e.g., Earth‚Äôs) in the plane as ‚Éór = ( x, y ). The posi-tion is recorded every time interval ‚àÜt: at the ith snapshot (time t = i‚àÜt), the planet‚Äôs position is (xi, y i). The trans-former fŒ∏ predicts the next position in an auto-regressive manner, 

(xi+1 , y i+1 ) = fŒ∏ (xi, y i, x i‚àí1, y i‚àí1, . . . , x 0, y 0).

Tokenization scheme. A key design choice lies in their tokenization strategy. Rather than treating (x, y ) as con-tinuous variables, x and y are independently discretized into bins (tokens). The procedure for x is as follows (and similarly for y): (1) partition the interval [‚àíL, L ] (with 

L = 50 AU ) into V = 7000 uniform bins; (2) assign each 

x ‚àà [‚àíL, L ] to the kth bin via k = ‚åä(x/L + 1) V / 2‚åã; (3) associate the kth bin ( k = 0 , 1, . . . , V ‚àí 1) with a token em-bedding ‚Éó Ex,k ‚àà Rnmodel , where nmodel = 768 . Likewise, 

y coordinates use embeddings ‚Éó Ey,k ‚àà Rnmodel . These to-ken embeddings are randomly initialized and learned during training. After tokenization, the continuous regression task becomes a next-token prediction (NTP) problem (Figure 1, top left). The only difference from standard language mod-eling is that the model outputs two tokens at each step (one for x, one for y). With this formulation, training follows the NanoGPT framework (Karpathy, 2024) using a GPT-2-scale transformer with an NTP cross-entropy loss. 

Tokenization disrupts spatial smoothness because ‚Éó Ex,k 1

and ‚Éó Ex,k 2 are randomly initialized and therefore uncorre-lated for k1Ã∏ = k2, regardless of how close k1 and k2 are in the physical space. Although one might hope that the trans-former could learn a meaningful spatial map given enough data and training time, we show that the learned embedding 3Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

space does not contain a good spatial map (see Figure 2). This holds despite the fact that Vafa et al. (Vafa et al., 2025) trained a GPT-2-scale model for days on 8 √óH100 GPUs using a massive dataset containing 20B training tokens. 

2.2. The emergent spatial map is poor Linear probing Recent work in mechanistic interpretability shows that many concepts correspond to linear directions in a model‚Äôs embedding space. For example, a world map can emerge when token embeddings are projected onto a suitable 2D plane (Gurnee & Tegmark, 2024). In the same spirit, we search for linear directions in the token embedding space that correlate most strongly with the true spatial coordinates. We treat x and y independently. Take x as an example: we aim to find a direction ‚Éó t ‚àà Rdmodel such that ‚Éó tx =arg min ‚Éót

P 

> i

‚à•‚ÉóEx,i ¬∑‚Éó t ‚àí xi‚à•2 and we use the coefficient of determination R2 to measure the goodness of fit. If 

R2 ‚âà 1, then a clean linear direction corresponding to the 

x coordinate exists. Before training, R2 ‚âà 0 because token embeddings are randomly initialized. 1

Low R2 indicates a poor spatial map Using the pretrained checkpoint released by Vafa et al. (2025), we obtain R2 ‚âà

0.86 for both x and y. Although this is much better than random initialization ‚Äì indicating that the spatial map has partially emerged ‚Äì it is still far from satisfactory. The model captures coarse spatial continuity but fails to learn fine-grained locality. To illustrate this, Figure 2(c) compares the true coordinates with the predicted coordinates along the best linear directions. While a global linear trend is present, substantial deviation from the ideal R2 ‚âà 1 remains. As a consequence, panel (b) shows that a circular orbit in real space is ‚Äúperceived‚Äù by the model as a fuzzy point cloud: the global circular structure is weakly preserved (as indicated by color coding across quadrants), but local structure within each quadrant is highly distorted and noisy. 

Poor spatial map is a deal breaker for world models Vafa et al. (2025) reported that the transformer‚Äôs internal represen-tations fail to encode the gravitational law F ‚àù 1/r 2, where 

r is the distance between two bodies. Computing r requires an accurate spatial map; without one, the model cannot re-cover distances reliably, let alone the gravitational force. Although the spatial map might, in principle, be stored non-linearly , we show that a high-quality linear spatial map is achievable with appropriate hyperparameter choices.         

> 1The embedding dimension nmodel = 768 is much smaller than the vocabulary size V= 7000 . If nmodel > V , one might trivially overfit even random embeddings.

2.3. Conditions and scaling laws for spatial map emergence 

Since the spatial map is only weakly emergent in the setup of Vafa et al. (2025), it is important to understand the con-ditions under which a spatial map does emerge. We con-jecture that both data coverage and model capacity play crucial roles: (1) Data coverage: the training data must adequately cover all tokens in the vocabulary, motivating us to vary both the training size D and the vocabulary size V ;(2) Model complexity: we vary the embedding dimension 

N while keeping other hyperparameters fixed. 2

To simplify the setting while retaining the essential features of tokenization, we adopt a 1D sine-wave dataset, which qualitatively resembles the oscillatory behavior of planetary motion but reduces the problem to one dimension. 

1D sine-wave dataset A 1D harmonic oscillator has sine-wave solutions x(t) = A sin( œât + œÜ). We choose ‚àÜt = 0 .2

and T = 20 , yielding T / ‚àÜt = 100 points per trajectory. We sample A ‚àà U [0 .5, 1] , œâ ‚àà [0 .5, 2] , and œÜ ‚àà [0 , 2œÄ)

to generate Dtraj trajectories (equivalently D ‚â° 100 Dtraj 

training tokens). Since x ‚àà [‚àí1, 1] , we partition this range uniformly into V bins/tokens, converting each trajectory into a sequence of token IDs, e.g., [6 , 12 , 17 , 20 , 21 , 19 , . . . ].Transformer models are trained using next-token predic-tion with cross-entropy loss. We use the Adam opti-mizer (Kingma, 2014) for 10 4 steps at learning rate 10 ‚àí3,followed by 10 4 steps at learning rate 10 ‚àí4. As in the previ-ous subsection, we apply linear probing to the transformer‚Äôs embedding matrix to measure whether a linear direction correlates with the true spatial coordinate, producing an R2

score. We study how R2 depends on three key parameters: training size D, vocabulary size V , and embedding dimen-sion N . The R2 we report here is the highest R2 (or lowest 

1 ‚àí R2) in training. Detailed training dynamics of R2 are included in Appendix B.1. 

Larger data size D improves spatial map emergence As shown in Figure 3(b), increasing D consistently improves 

R2. This is expected: more data imposes stronger con-straints on the hypothesis space, reducing overfitting and encouraging the model to learn the true spatial structure. 

Smaller vocabulary size V improves emergence Fig-ure 3(a) shows the training dynamics of embedding pro-jections onto the best linear direction. For a fixed amount of data ( D = 10 4), smaller vocabulary sizes produce signifi-cantly better spatial maps. Intuitively, larger vocabularies require proportionally more data to maintain adequate cov-erage before a spatial map can emerge. We fix N = 32 and sweep V in {64 , 128 , 256 , 512 , 1024 }

and Dtraj in {64 , 128 , 256 , 512 , 1024 }. The scaling law in     

> 2We choose nlayer = 2 ,nhead = 1 .

4Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Figure 3(b, left) fits well to 

1‚àíR2 ‚âà AD ‚àíŒ±D V Œ±V (A = 0 .52 , Œ± D = 1 .15 , Œ± V = 1 .33) 

(1) with an excellent fit ( R2 ‚âà 0.995 ). Since Œ±V ‚â≥ Œ±D , the training size D must increase at least as fast as the vocabu-lary size V to maintain comparable spatial-map quality. 

Embedding dimension N exhibits a critical value Nc

One might expect that increasing the embedding dimen-sion N would help spatial map emergence, since higher-dimensional embeddings provide more ‚Äúlottery tickets‚Äù for discovering a good linear direction. However, Figure 3(b, middle) shows that 1 ‚àí R2 decreases with N only up to a critical value Nc ‚âà 8, beyond which performance rapidly plateaus. Increasing N further offers little benefit. 

Scaling up The above analyses use small-scale hyperparam-eters to allow sweeping many configurations efficiently. To connect more directly to the large-scale setup of Vafa et al. (2025), which uses V = 7000 and N = 768 , we repeat our experiments with these values while sweeping D up to 10 8

tokens. Figure 3(b, right) shows that scaling slows markedly at large D, suggesting diminishing returns once the data suf-ficiently cover the space. Moreover, comparing N = 768 

and N = 32 (with V = 7000 fixed) reveals that increasing 

N offers little improvement ‚Äì and may even hinder ‚Äì spatial map emergence. In contrast, reducing V to 128 produces a dramatic improvement. 

Take-home message Among all levers, reducing the vocab-ulary size V is the most effective way to improve spatial map emergence. However, choosing V too small leads to overly coarse binning, reducing the accuracy of predictions. An optimal intermediate choice of V should therefore bal-ance spatial-map quality and predictive resolution ‚Äì an issue we investigate in the next section. Another natural solution would be using continuous coordinate without tokenization, but it would then face a spatial stability problem discussed in the next section. Inductive Bias 1: Spatial smoothness 

Failure mode: Vafa et al. (2025)‚Äôs transformer fails to learn a spatial map in the embedding space. 

Solution: (1) choose a smaller vocabulary size for tokenization, or (2) use continuous coordinates with-out tokenization. With (2), the spatial map is by default contained in the input. 

# 3. Inductive Bias 2: Spatial Stability 

Following the setup of Vafa et al. (2025), we have so far treated trajectory prediction as a classification problem by discretizing continuous spatial coordinates into discrete to-kens. A natural question is whether we can instead use                          

> Figure 3. Spatial map emergence strongly depends on tokenization, and weakly on embedding dimensions. (a) Evolution of learned embeddings, i.e., token embeddings projected onto the best lin-early decodable direction at the last step (200). From left to right: vocabulary size V= 100 ,1000 ,10000 . Each inset shows the true coordinate and the learned coordinate. The spatial map emerges easily for a small vocabulary size V, but becomes poorly emergent for large V. (b) Spatial map quality, measured by R2between the true coordinate and the learned coordinate. Left: 1‚àíR2obeys a scaling law with respect to vocabulary size Vand training tokens
> D. Middle: R2saturates when the embedding dimension Nis beyond a critical value at 8. Right: Scaling up Vor Ndoes not improve, but actually harms spatial map emergence.

continuous spatial coordinates directly as inputs, thereby eliminating the need to learn a spatial map altogether. Al-though Vafa et al. (2025) reported that discretized coordi-nates with cross-entropy loss (classification) outperform continuous coordinates with MSE loss (regression), we ar-gue that the continuous formulation merits further investiga-tion. To examine this question systematically, we introduce the Kepler dataset‚Äîa simplified benchmark consisting of idealized planetary orbits. On this controlled testbed, we directly compare two formulations: next-state prediction (regression) and next-token prediction (classification). 

Kepler dataset The Kepler dataset consists of 2D elliptical orbits of a planet around a central body (the sun) fixed at the origin. Each trajectory is generated by numerically integrating the gravitational equation of motion, 

d2‚Éórdt 2 = ‚àíGM‚Éó r

‚à•‚Éór‚à• 3 , (2) where ‚Éór = ( x, y ) is the position vector and GM = 1 .0 is the gravitational parameter. For each trajectory, we initialize the system at perihelion (closest approach) with orbital param-eters sampled uniformly: eccentricity e ‚àà [0 .0, 0.8] , semi-major axis a ‚àà [0 .5, 2.0] , and initial orientation Œ∏ ‚àà [0 , 2œÄ].The initial position and velocity are computed from these parameters and then rotated by Œ∏.5Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

We integrate the system using solve ivp in scipy with relative and absolute tolerances of 10 ‚àí8, sampling 100 equally spaced time points at a step size ‚àÜt = 0 .2. The resulting dataset contains Dtraj trajectories (equivalently 

D = 100 Dtraj tokens), each providing a sequence of (x, y )

positions of shape (100 , 2) that captures the full elliptical motion of the planet around the sun. 

3.1. Regression: next state prediction 

As discussed above, using continuous spatial coordinates as inputs naturally removes the need to learn a spatial map. However, this comes at a cost: the regression formulation is considerably more prone to error accumulation than its classification counterpart. It is well known that autoregres-sive models suffer from the spatial stability issue, i.e., error accumulates quickly at inference time; continuous variables exacerbate this issue because their outputs can become un-bounded. In contrast, discrete tokenization imposes a finite set of possible outputs, providing a form of ‚Äúerror correction‚Äù by projecting predictions back onto a discrete vocabulary. Perhaps for this reason, Vafa et al. (2025) reported that ‚Äúwe experimented between using (a) continuous coordinates (and MSE loss) and (b) discretized coordinates (with cross-entropy loss), finding the latter worked better.‚Äù Below, we (1) identify the failure mode associated with regression and (2) introduce a mitigation strategy. 

A failure mode: error accumulation As shown in Figure 4 (leftmost, œÉ = 0 .0), we condition on the first 50 points of a trajectory and autoregressively generate the next 50 points, comparing them with the true trajectory. Although the initial prediction error is small, it accumulates rapidly and causes catastrophic divergence, often sending the planet into the sun or off to infinity. 

Fix: noisy context learning To make the generation process more robust, the model must be trained to handle deviations from perfect past contexts. To simulate such deviations, we add Gaussian noise to the historical inputs during training, leading to the modified objective 

L =

> T

X

> i=1

‚à•‚Éór i+1 ‚àí fŒ∏ (‚Éór i + œÉ‚Éó œµ i, ¬∑ ¬∑ ¬∑ ,‚Éór 0 + œÉ‚Éó œµ 0)‚à•2 , (3) where ‚Éóœµ i ‚àº N (0 , I2) is drawn from a normal distribution. This is a technique known as noisy context learning (Ren et al., 2025). Figure 4 shows that an intermediate noise level (œÉ = 0 .1) yields the most accurate predictions: too little noise fails to counteract error accumulation, while too much noise overwhelms the learning signal. 

3.2. Fair comparison: regression wins over classification 

To fairly compare regression and classification models, two subtleties must be addressed. (1) Hyperparameter choices. 

We have shown that vocabulary size V is a crucial hyperpa-rameter for classification, while the noise scale œÉ strongly affects regression performance. Thus, for each method, we sweep over the relevant hyperparameters and report the per-formance of the best model. (2) Evaluation metric. Because classification models are trained with cross-entropy loss and regression models with MSE loss, their training losses are not directly comparable. Instead, we convert predicted to-kens into continuous coordinates (x, y ) by mapping each token to the center of its corresponding bin. Using the first 50 points as context, we autoregressively generate the next 50 points. The evaluation metric is the mean distance er-ror , defined as the Euclidean distance between generated and ground-truth positions, averaged over the 50 gener-ated points. Detailed training dynamics are included in Appendix B.2 (classification) and B.3 (regression). Figure 5 reports the mean distance error for both regression and classification models. In the left panel, we confirm that an intermediate noise scale œÉ yields the best regression performance. In the middle panel, we observe a sweet spot for vocabulary size V , with the optimal V increasing with the training size D. In the right panel, regression models (solid green) consistently outperform classification models (solid blue) across all training sizes when hyperparameters are optimized (choosing V for classification and œÉ for re-gression). However, if œÉ is naively fixed to zero, regression models (dashed green) may underperform the best classifi-cation models (solid blue) at large training sizes ‚Äì precisely the regime considered by Vafa et al. (2025). Inductive Bias 2: Spatial Stability 

Failure mode: Transformers (auto-regressive mod-els in general) accumulate errors fast, especially for unbounded continuous variables. 

Solution: forcing the model to correct errors in in-ference by adding input perturbations in training. As a result, regression models with continuous inputs outperform classification models with tokenization. 

# 4. Inductive Bias 3: Temporal Locality 

Having established in the previous sections that regression models have two key advantages over classification mod-els ‚Äì (1) they naturally preserve spatial continuity without needing to learn a spatial map, and (2) they can achieve lower mean prediction error‚Äîwe now return to the central question: do regression transformers learn a Newtonian world model ? There is one more inductive bias needed ‚Äì temporal locality. We note that Newtonian mechanics is a second-order differential equation, meaning that the next state depends only on the current state and the previous state, but not on other states before that. This suggests using a 6Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion?                 

> Figure 4. Error accumulation and fixing it by adding context noise in training. Each subplot shows the ground truth trajectory (blue solid circle), conditioning 50 points (green), and the generated 50 points (red). From left to right: training with different levels of context noise
> œÉ. Naively training a regression-based transformer leads to severe error accumulation (left, œÉ= 0 ), whereas adding a reasonable amount of noise œÉ(e.g., œÉ= 0 .1) to contexts during training substantially improves robustness.
> Figure 5. Comparing regression and classification transformers ( D:training tokens), using mean distance error as the metric to evaluate predictive performance. Left: regression models exhibit a sweet spot in the context noise scale œÉ. Middle: regression models also exhibit a sweet spot in the vocabulary size V. Right: comparing regression and classification across different training data sizes D.Regression models consistently outperform classification models when their best hyperparameters ( œÉor V) are selected. However, naively trained regression models ( œÉ= 0 ) underperform the best classification models when the training data is large.

context length of 2. This motivates us to vary context length to control temporal locality. 

Newtonian world model We apply linear probing to search for linear directions in the model‚Äôs latent representations that correlate with force variables: force magnitude F ‚â°‚à•‚Éó F ‚à•, the x-component Fx, and the y-component Fy . We probe a wide range of internal activations, including the inputs and outputs of attention and MLP blocks (both before and after residual merging), as well as hidden states inside MLP modules. For each force-related target, we report the highest R2 across all layers (Results for R2 of each layer are included in Appendix C). Figure 6(a) (green bars) reveals that while context length 100 partially captures these force variables ( R2 ‚âà 0.9), only context length 2 yields a precise representation R2 ‚âà 0.999 .

Keplerian world model This raises the question: what world model does a transformer learn when the context length is large? It is obvious that the model must have learned something meaningful in order to make accurate predictions. If we adopt Kepler‚Äôs geometric perspective ‚Äì orbits are ellipses (Kepler‚Äôs first law), we can first fit an elliptical equation based on all previous points and then predict the next state by continuing the curve. We refer to this global geometric approach as Keplerian world model ,in contrast to Newtonian world model based on force com-putations (see Figure 1 right for illustrations). To test this hypothesis, we probe the model for key geometric param-eters of ellipses: semi-major axis a, semi-minor axis b,Laplace‚ÄìRunge‚ÄìLenz vector ‚Éó A = ( Ax, A y ). Figure 6(b) shows that these geometric quantities are linearly encoded almost perfectly ( R2 ‚âà 0.998 ) when the context length is 100, whereas they are relatively poor ( R2 ‚âà 0.9) when the context length is 2. There is a phase transition between Ke-plerian and Newtonian models by varying context lengths, as shown in Figure 6(c) ‚Äì while transformers with small 

context lengths preferentially learn Newtonian world mod-els , transformers with large context lengths preferentially learn Keplerian world models . Furthermore, Figure 6(d) shows that larger context lengths achieve lower prediction error, because the Keplerian model (global, geometry-based) is more robust to noise than the Newtonian model (local, force-based), hence making more accurate long-horizon predictions. Inductive Bias 3: Temporal Locality 

Failure mode: A regression transformer fails to learn the Newtonian model when the context length is too large. 

Solution: Reducing the context length to 2 induces a Newtonian model. Short context lengths induce the Newtonian model, while long context lengths induce the Keplerian model. 

# 5. Conclusions and discussion 

The goal of this paper is to identify failure modes that pre-vent current foundation models from learning ‚Äúworld mod-els,‚Äù using a controlled setup based on synthetic planetary motion data. We find that Vafa et al. (2025) fails to learn a spatial map due to its tokenization scheme. After address-7Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion?    

> Figure 6. The tale of two world models ‚Äì the context length controls the transformer to learn a Newtonian model or a Keplerian model. (a) Small context lengths (e.g., 2) lead to Newtonian models, with transformers internally computing gravitational forces. (b) Long context lengths (e.g., 100) lead to Keplerian models, with transformers internally computing orbit parameters (semi-major/minor-axis length a/b ,Laplace-Runge-Lenz vector ‚ÉóA). (c) Varying context lengths controls the phase transition between Newtonian vs Keplerian world models. (d) Mean distance error as a function of future steps and context lengths. Larger context lengths lead to improved predictive performance (lower errors).

ing this issue‚Äîeither by reducing the vocabulary size or by using regression models trained with MSE loss‚ÄîNewton‚Äôs world model still does not emerge. Instead, a geometric approach that fits ellipses ( Kepler‚Äôs world model ) emerges. Newton‚Äôs world model appears only when the block size is reduced to 2, consistent with the fact that Newtonian mechanics is a second-order dynamical system. Our find-ings underscore the challenges and subtleties involved in inducing even very simple world models (such as planetary motion) in transformer architectures. Finally, our results offer a critical perspective on the defini-tion of ‚Äúworld models‚Äù in artificial intelligence. Please refer to Appendix A for a summary of previous definitions. While 

prevailing views often characterize a world model purely by its predictive utility‚Äîthe ability to simulate future states given an action‚Äîwe argue that prediction is necessary but not sufficient . As our ‚ÄúKeplerian‚Äù models demonstrate, a system can achieve high predictive accuracy by merely fitting complex curves to historical data, effectively mem-orizing the training distribution. However, true scientific understanding requires discovering the governing mech-anisms ‚Äîthe simple, invariant laws (like F = ma ) that generate the data. We speculate that this mechanistic understanding is the prerequisite for radical out-of-distribution (OOD) gen-eralization . A curve-fitter (Kepler) is bound to the ‚Äúgrey elephants‚Äù (familiar trajectories) it has seen; it cannot reli-ably predict the behavior of a ‚Äúpink elephant‚Äù‚Äîa scenario strictly outside its training data. In contrast, a mechanistic model (Newton) decouples the rule from the history. Be-cause it understands the causal invariant ( F = ma ), it can correctly predict the future of a pink elephant just as accu-rately as a grey one. Our work suggests that for AI to evolve from a predictor to a scientist, we must architecturally con-strain it to look for these simple, local mechanisms rather than complex global histories. 

Limitations (1) For simplicity, we have simplified the setup of Vafa et al. (Vafa et al., 2025) by using one time-scale (they used two time-scales). (2) We used linear probes to verify the existence of world models, but this method yields only implicit knowledge: the network ‚Äúknows‚Äù Force, but does not explicitly output Newton‚Äôs equations. Furthermore, probing requires us to know what to look for a priori. A fully autonomous ‚ÄúAI Physicist‚Äù would require an additional mechanism‚Äîsuch as a secondary ‚Äúinterpreter‚Äù network or symbolic regression head‚Äîthat actively searches the latent space for simple, linear relationships to automatically ex-tract and output symbolic laws like F = ma without human supervision. Related works about ‚ÄúAI Physicists‚Äù are re-viewed in Appendix A ‚Äì although they have proven useful in highly-controlled setups where carefully designed specialist models are employed, end-to-end extraction of clean and simple laws (if they do exist) from black-box foundation laws remains a challenging but urgent direction. 

# Acknowledgement 

We would like to thank Liam Storan for the helpful discus-sion. S.G thanks the Simons Collaboration on the Physics of Learning and Neural Computation and a Schmidt Sciences Polymath award for funding. Z.L, S.S and A.T thank The James Fickel Enigma Project Fund. 8Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

# References 

Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. Advances in neural information processing systems , 35:23716‚Äì23736, 2022. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. 

Advances in neural information processing systems , 33: 1877‚Äì1901, 2020. Brunton, S. L., Proctor, J. L., and Kutz, J. N. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113(15):3932‚Äì3937, 2016. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research ,24(240):1‚Äì113, 2023. Cranmer, M., Sanchez Gonzalez, A., Battaglia, P., Xu, R., Cranmer, K., Spergel, D., and Ho, S. Discovering sym-bolic models from deep learning with inductive biases. 

Advances in neural information processing systems , 33: 17429‚Äì17442, 2020. Friston, K. The free-energy principle: a unified brain theory? 

Nature Reviews Neuroscience , 11:127‚Äì138, 2010. Goh, G. e. a. Multimodal neurons in artificial neural net-works, 2021. OpenAI. Gurnee, W. and Tegmark, M. Language models represent space and time. In The Twelfth International Conference on Learning Representations , 2024. URL https:// openreview.net/forum?id=jE8xbmvFin .Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122 , 2(3), 2018. Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019a. Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and Davidson, J. Learning latent dynamics for planning from pixels. In International conference on machine learning , pp. 2555‚Äì2565. PMLR, 2019b. Karpathy, A. Nanogpt. https://github.com/ karpathy/nanoGPT , 2024. Accessed: 2025-02-XX. Kim, M. J., Pertsch, K., Karamcheti, S., Xiao, T., Balakr-ishna, A., Nair, S., Rafailov, R., Foster, E., Lam, G., San-keti, P., et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246 , 2024. Kingma, D. P. Adam: A method for stochastic optimization. 

arXiv preprint arXiv:1412.6980 , 2014. Korchinski, D. J., Karkada, D., Bahri, Y., and Wyart, M. On the emergence of linear analogies in word embeddings. 

arXiv preprint arXiv:2505.18651 , 2025. Lemos, P., Jeffrey, N., Cranmer, M., Ho, S., and Battaglia, P. Rediscovering orbital mechanics with machine learning. 

Machine Learning: Science and Technology , 4(4):045002, 2023. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tun-ing. Advances in neural information processing systems ,36:34892‚Äì34916, 2023. Liu, Z. and Tegmark, M. Machine learning conservation laws from trajectories. Physical Review Letters , 126(18): 180604, 2021. Liu, Z., Madhavan, V., and Tegmark, M. Machine learning conservation laws from differential equations. Physical Review E , 106(4):045307, 2022. Liu, Z., Sturm, P. O., Bharadwaj, S., Silva, S. J., and Tegmark, M. Interpretable conservation laws as sparse invariants. Phys. Rev. E , 109:L023301, Feb 2024. doi: 10.1103/PhysRevE.109.L023301. URL https://link.aps.org/doi/10.1103/ PhysRevE.109.L023301 .Lotter, W., Kreiman, G., and Cox, D. Deep predictive coding networks for video prediction and unsupervised learning. In ICLR , 2017. Park, K., Choe, Y. J., Jiang, Y., and Veitch, V. The geometry of categorical and hierarchical concepts in large language models. arXiv preprint arXiv:2406.01506 , 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning , pp. 8748‚Äì8763. PmLR, 2021. Rao, R. P. and Ballard, D. H. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nature Neuroscience , 2:79‚Äì87, 1999. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. 

arXiv preprint arXiv:2205.06175 , 2022. 9Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Ren, S., Yu, Q., He, J., Shen, X., Yuille, A., and Chen, L.-C. Beyond next-token: Next-x prediction for autoregres-sive visual generation. arXiv preprint arXiv:2502.20388 ,2025. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi `ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation lan-guage models. arXiv preprint arXiv:2302.13971 , 2023. Udrescu, S.-M. and Tegmark, M. Ai feynman: A physics-inspired method for symbolic regression. Science ad-vances , 6(16):eaay2631, 2020. Vafa, K., Chang, P. G., Rambachan, A., and Mullainathan, S. What has a foundation model found? using induc-tive bias to probe for world models. arXiv preprint arXiv:2507.06952 , 2025. Wu, T. and Tegmark, M. Toward an artificial intelli-gence physicist for unsupervised learning. Phys. Rev. E, 100:033311, Sep 2019. doi: 10.1103/PhysRevE.100. 033311. URL https://link.aps.org/doi/10. 1103/PhysRevE.100.033311 .Zitkovich, B., Yu, T., Xu, S., Xu, P., Xiao, T., Xia, F., Wu, J., Wohlhart, P., Welker, S., Wahid, A., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning , pp. 2165‚Äì2183. PMLR, 2023. 10 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

# A. Related works 

World models The notion of ‚Äúworld models‚Äù has appeared across several threads in machine learning, generally referring to internal representations/computations that capture the dynamics or structure of an environment. Early work in model-based reinforcement learning developed predictive latent-state models for planning and control, such as the World Models framework of Ha and Schmidhuber (Ha & Schmidhuber, 2018), PlaNet (Hafner et al., 2019b), and Dreamer (Hafner et al., 2019a), which learn compact dynamic models from pixel observations. A parallel line of work in neuroscience-inspired ML explores generative predictive coding frameworks and latent dynamics models as computational analogues of internal world representations (Rao & Ballard, 1999; Friston, 2010; Lotter et al., 2017). More recently, the emergence of world models in large foundation models has attracted growing attention: language models have been shown to encode linearizable geometric or semantic structures (Gurnee & Tegmark, 2024; Park et al., 2024; Korchinski et al., 2025), vision-language models capture rich conceptual relationships (Radford et al., 2021; Goh, 2021; Alayrac et al., 2022), and multimodal agents acquire implicit affordances for acting in embodied environments (Zitkovich et al., 2023; Reed et al., 2022; Kim et al., 2024). 

AI physicists Within physics and scientific modeling, AI physicist approaches have demonstrated that symbolic laws and interpretable dynamical equations can emerge from neural network training (Wu & Tegmark, 2019; Brunton et al., 2016; Cranmer et al., 2020; Lemos et al., 2023; Liu & Tegmark, 2021; Liu et al., 2022; 2024; Udrescu & Tegmark, 2020), suggesting that data-driven models can recover ground-truth world structures under the right inductive biases. Crucially, these methods typically impose strong structural priors (e.g., sparsity, symmetry, or graph structure). This leaves open the question of whether general-purpose architectures like Transformers can recover such ‚Äùworld models‚Äù without these domain-specific constraints. Recent work by Vafa et al. (Vafa et al., 2025) suggests they cannot, reporting negative results even on simple planetary-motion datasets. Our work builds on these lines of inquiry but demonstrates that recovering true physical laws does not require strong symbolic priors, but rather a minimal assumption of locality‚Äîthe intuitive idea that nature is governed by simple local rules rather than a complex history of the past. By constraining the model to seek such simple explanations, we show that general-purpose architectures can recover the ground-truth world structure without domain-specific constraints. 

# B. Training dynamics 

B.1. 1D sine wave dataset 

In the main paper, for the 1D sine-wave dataset, we report the best (lowest) value of 1 ‚àí R2 attained during training. This choice is necessary because the optimal mapping is not always achieved at the final training step: the training dynamics can exhibit non-monotonic behavior due to overfitting. Figure 7 illustrates the evolution of the training loss, test loss, and map quality R2 for different vocabulary sizes and numbers of training tokens. For small training budgets (e.g., 10 4 tokens), severe overfitting is observed, as evidenced by a large gap between the training and test losses. In this regime, both the test loss and the map quality evolve non-monotonically: they initially improve but subsequently degrade once overfitting sets in. Notably, with limited data, a smaller vocabulary size ( V = 128 ) yields better maps than a larger one ( V = 7000 ), highlighting the advantage of smaller vocabularies in the low-data regime. Larger vocabulary sizes require more data to avoid overfitting. For example, with 10 6 training tokens (middle column), the train‚Äìtest gap is negligible for V = 128 but remains noticeable for V = 7000 . However, larger vocabularies benefit from longer scaling as the training data increases: for V = 7000 , performance continues to improve when scaling from 10 6 to 10 8 tokens, whereas such gains largely saturate for V = 128 .

B.2. Kepler as classification 

When we formulate the Kepler problem as a classification task (next-token prediction) trained with cross-entropy loss, two key hyperparameters are the vocabulary size and the number of training tokens. As shown in Figure 8, overfitting can occur when the training data are limited, especially for large vocabulary sizes. Although the model is trained using cross-entropy loss, we additionally compute an effective MSE loss defined by the squared distance between the center of the predicted token and the true next position. This allows for a fair comparison with the regression results shown in Figure 9. 

B.3. Kepler as regression 

When we formulate the Kepler problem as a regression task (next-state prediction) trained using MSE loss, two key hyperparameters are the in-context noise level and the number of training tokens. As shown in Figure 9, overfitting can 11 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Figure 7. Training dynamics of the training loss, test loss, and R2 for the 1D sine-wave dataset, with vocabulary sizes 128 , 7000 and training token counts 10 4, 10 6, 10 8. When the training set is small, the learned map initially improves (lower 1 ‚àí R2) but subsequently degrades as overfitting sets in. Larger vocabulary sizes require more data to mitigate overfitting, as reflected by a larger train‚Äìtest loss gap. 

Figure 8. When formulating the Kepler problem as a classification task (next-token prediction), performance depends on both the vocabulary size and the number of training tokens. For each configuration, we plot the training dynamics of the cross-entropy loss, the effective MSE loss (defined as the squared distance between the true point and the center of the predicted token), and the generation distance error averaged over the next 50 steps. When the training data are limited, smaller vocabulary sizes yield better performance. 

12 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

occur when the training data are limited; however, the amount of data required to avoid overfitting is smaller than in the classification setting shown in Figure 8. For example, with 10 5 training tokens, a noticeable train‚Äìtest gap remains for classification, whereas it is negligible for regression. In addition, introducing a moderate level of in-context noise during training ( œÉ) helps reduce generation error, as demonstrated in Figure 4.  

> Figure 9. When formulating the Kepler problem as a regression task (next-state prediction), performance depends on both the noise scale and the number of training tokens. For each configuration, we plot the training dynamics of the MSE loss and the generation distance error averaged over the next 50 steps. Introducing a moderate level of in-context noise during training offers advantages over training without noise.

# C. More probing results 

C.1. Interpolating between Kepler and Newton by varying context lengths 

In the main paper, for the Kepler model, we showed that a transformer with a short context length (2) learns a Newtonian model, whereas a transformer with a long context length (100) learns a Keplerian model. What happens when we interpolate between these two extremes? Figure 10 shows that the transition is monotonic: increasing the context length yields models that are progressively more Keplerian and less Newtonian. For simplicity, the metrics in the rightmost plot are averaged across components (i.e., the Keplerian score averages over a, b, A x, A y , while the Newtonian score averages over F, F x, F y ). 

C.2. Probing results across layers 

In the main paper, we report the best R2 over all hidden representations. But where are the best representations, and how do they emerge in forward computations? For context length 2, probing results are shown in Figure 11. For context length 100, probing results are shown in Figure 12. Besides the probing targets reported in the main paper, we compute other related quantities as well. 

Keplerian model : semi-major axis length a (and 1/a , 1/a 2), semi-minor axis length b (and 1/b , 1/b 2), half focal length 

c = ‚àöa2 ‚àí b2, ellipticity e = c/a , average radius ¬Ør = ‚àöab , Laplace-Runge-Lenz vector ‚Éó A = ( Ax, A y ) and magnitude |‚ÉóA|,radial unit vector ÀÜr = ( nx, n y ).

Newtonian model Gravitational force ‚Éó F = ( Fx, F y ) and magnitude |‚ÉóF |, force direction ( ÀÜFx, ÀÜFy ) = ÀÜ r = ( nx, n y ), position ‚Éór = ( x, y ), radius r = px2 + y2 (and r2, 1/r , 1/r 2, 1/r 3). 13 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Figure 10. Effect of context length on the learned world model. Left: Larger context lengths favor the emergence of a Newtonian world model. Middle: Smaller context lengths favor the emergence of a Keplerian world model. Right: A summary plot obtained by averaging the R2 scores across different probing targets. 

14 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Figure 11. Probing results for context length 2. Top: Probes related to the Keplerian model. Bottom: Probes related to the Newtonian model. 

15 Why do transformers fail to learn the ‚Äúworld model‚Äù for planetary motion? 

Figure 12. Probing results for context length 100. Top: Probes related to the Keplerian model. Bottom: Probes related to the Newtonian model. 

16