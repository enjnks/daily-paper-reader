---
title: "From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers"
title_zh: 从开普勒到牛顿：归纳偏置引导 Transformer 中学习到的世界模型
authors: "Ziming Liu, Sophia Sanborn, Surya Ganguli, Andreas Tolias"
date: 2026-02-06
pdf: "https://arxiv.org/pdf/2602.06923v1"
tags: ["keyword:SR", "query:SR"]
score: 7.0
evidence: 关注从数据中发现物理定律和世界模型，这是符号回归的核心目标。
tldr: 本研究探讨了通用Transformer能否超越单纯的预测，进而发现物理规律。针对通用模型往往只能实现高精度预测而无法捕捉底层物理规律的问题，本文引入了空间平滑性、稳定性和时间局部性三种极简归纳偏置。实验证明，这些偏置能引导模型从简单的开普勒式曲线拟合进化到发现牛顿力学表征。这一发现揭示了架构选择在AI从“曲线拟合器”向“物理学家”转变中的关键作用，为自动化科学发现迈出了重要一步。
motivation: 旨在解决通用Transformer在物理建模中仅能实现高精度预测，却无法理解和提取底层物理规律（世界模型）的问题。
method: 系统地引入了空间平滑性（连续回归）、稳定性（噪声上下文训练）和时间局部性（限制注意力窗口）三种极简归纳偏置。
result: 空间平滑与稳定性使模型习得开普勒式轨道拟合，而时间局部性则迫使模型放弃曲线拟合，成功发现牛顿力学中的力表征。
conclusion: 简单的架构偏置决定了AI是成为“曲线拟合器”还是“物理学家”，这对于实现自动化科学发现具有重要意义。
---

## 摘要
通用人工智能架构能否超越预测，去发现统治宇宙的物理定律？真正的智能依赖于“世界模型”——即因果抽象，它使智能体不仅能预测未来状态，还能理解底层的动力学机制。虽然之前的“AI 物理学家”方法已成功恢复了此类定律，但它们通常依赖于强大的、特定领域的先验知识，这些知识实际上已经预先“植入”了物理学。相反，Vafa 等人最近表明，通用的 Transformer 无法获得这些世界模型，虽然实现了很高的预测准确率，但未能捕捉到底层的物理定律。我们通过系统地引入三种最小归纳偏置来弥补这一差距。我们表明，通过确保空间平滑性（将预测表述为连续回归）和稳定性（通过使用带噪声的上下文进行训练以减轻误差累积），通用的 Transformer 能够克服之前的失败，学习到连贯的开普勒世界模型，并成功地将椭圆拟合到行星轨道上。然而，真正的物理洞察力需要第三种偏置：时间局部性。通过将注意力窗口限制在紧邻的过去——即施加一个简单的假设，即未来状态仅取决于局部状态而非复杂的历史——我们迫使模型放弃曲线拟合，转而发现牛顿力学表示。我们的结果表明，简单的架构选择决定了 AI 是成为曲线拟合器还是物理学家，这标志着迈向自动化科学发现的关键一步。

## Abstract
Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on "world models" -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous "AI Physicist" approaches have successfully recovered such laws, they typically rely on strong, domain-specific priors that effectively "bake in" the physics. Conversely, Vafa et al. recently showed that generic Transformers fail to acquire these world models, achieving high predictive accuracy without capturing the underlying physical laws. We bridge this gap by systematically introducing three minimal inductive biases. We show that ensuring spatial smoothness (by formulating prediction as continuous regression) and stability (by training with noisy contexts to mitigate error accumulation) enables generic Transformers to surpass prior failures and learn a coherent Keplerian world model, successfully fitting ellipses to planetary trajectories. However, true physical insight requires a third bias: temporal locality. By restricting the attention window to the immediate past -- imposing the simple assumption that future states depend only on the local state rather than a complex history -- we force the model to abandon curve-fitting and discover Newtonian force representations. Our results demonstrate that simple architectural choices determine whether an AI becomes a curve-fitter or a physicist, marking a critical step toward automated scientific discovery.