---
title: LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization
title_zh: 基于物理信息告知的 Token 正则化策略优化的 LLM 科学方程发现
authors: "Boxiao Wang, Kai Li, Tianyi Liu, Chen Li, Junzhe Wang, Yifan Zhang, Jian Cheng"
date: 2026-02-11
pdf: "https://arxiv.org/pdf/2602.10576v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 使用大语言模型的符号回归算法创新
tldr: 本研究针对符号回归中大语言模型（LLM）作为静态生成器时存在的物理不一致和冗余问题，提出了PiT-PO框架。该框架通过强化学习将LLM转化为自适应生成器，引入层级物理信息约束和Token级正则化，确保生成的方程既符合物理规律又结构精简。实验证明，PiT-PO在标准基准测试中达到SOTA水平，并成功发现新型湍流模型，甚至使小规模模型性能超越了闭源大模型。
motivation: 现有的基于LLM的符号回归方法主要依赖提示词引导，缺乏基于搜索反馈的模型内部更新，导致生成的方程常出现物理失真或数学冗余。
method: 提出PiT-PO框架，利用强化学习结合层级物理有效性约束和细粒度Token惩罚，动态优化LLM的方程生成策略。
result: 该方法在多个基准测试中取得领先，并成功应用于复杂流体力学问题，证明了小规模模型在科学发现任务中也能超越闭源巨头。
conclusion: PiT-PO通过物理对齐和结构优化，显著提升了LLM在科学方程发现中的准确性与简洁性，推动了高性能科学发现的普及。
---

## 摘要
符号回归旨在从观测数据中提取数学方程。最近的方法已成功利用大语言模型（LLM）生成方程假设，利用了其庞大的预训练科学先验。然而，现有框架主要将 LLM 视为静态生成器，依赖提示词级别的引导来驱动探索。这种范式无法根据搜索反馈更新模型的内部表示，通常会产生物理上不一致或数学上冗余的表达式。在这项工作中，我们提出了 PiT-PO（物理信息告知的 Token 正则化策略优化），这是一个通过强化学习将 LLM 演化为自适应生成器的统一框架。PiT-PO 的核心是一种双重约束机制，它在严格执行分层物理有效性的同时，应用细粒度的 Token 级惩罚以抑制冗余结构。因此，PiT-PO 使 LLM 能够生成既符合科学一致性又具有结构简洁性的方程。实验表明，PiT-PO 在标准基准测试中达到了最先进的性能，并成功为具有挑战性的流体动力学问题发现了新型湍流模型。我们还证明了 PiT-PO 能够使小规模模型超越闭源巨头模型，从而使高性能科学发现的获取更加民主化。

## Abstract
Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, existing frameworks predominantly treat the LLM as a static generator, relying on prompt-level guidance to steer exploration. This paradigm fails to update the model's internal representations based on search feedback, often yielding physically inconsistent or mathematically redundant expressions. In this work, we propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully discovers novel turbulence models for challenging fluid dynamics problems. We also demonstrate that PiT-PO empowers small-scale models to outperform closed-source giants, democratizing access to high-performance scientific discovery.

---

## 论文详细总结（自动生成）

### 论文总结：基于物理信息告知的 Token 正则化策略优化的 LLM 科学方程发现 (PiT-PO)

#### 1. 核心问题与研究动机
*   **核心问题**：符号回归（SR）旨在从观测数据中发现简洁、可解释的数学方程。虽然现有方法开始利用大语言模型（LLM）的科学先验，但大多将 LLM 视为**静态生成器**（仅通过 Prompt 引导），导致生成的方程往往符合数值拟合但违背物理定律，且存在严重的数学结构冗余。
*   **研究动机**：将 LLM 从“静态建议者”转变为“自适应生成器”。通过在搜索过程中动态更新模型参数，使模型能够内化物理约束并自动剔除冗余项，从而发现真正符合科学逻辑的控制方程。

#### 2. 方法论
PiT-PO 框架结合了进化搜索与强化学习，核心技术包括：
*   **搜索中 LLM 演化（In-Search LLM Evolution）**：采用 **GRPO（群组相对策略优化）** 算法，在搜索迭代中通过 LoRA 细微调整 LLM 参数，使模型分布向高奖励（更准确、更物理）的方程区域对齐。
*   **双重约束学习信号**：
    *   **层级物理约束**：分为通用层（量纲一致性、平滑性）和领域特定层（如流体力学中的实现性、边界条件）。引入**门控机制**，仅在方程达到一定拟合精度后才激活物理惩罚，避免搜索早期过度受限。
    *   **定理引导的数学约束**：提出 **支持排除定理（Support Exclusion Theorem）**。该定理通过分析基函数的经验格拉姆矩阵（Gram Matrix）和拟合系数，从理论上识别哪些项是“伪发现”（冗余项）。
*   **Token 级正则化**：不同于标准 GRPO 的序列级奖励，PiT-PO 将冗余项识别转化为 **Token 级别的惩罚**。通过降低冗余 Token 的生成概率，强制模型生成结构精简的方程。

#### 3. 实验设计
*   **数据集与场景**：
    *   **LLM-SR Suite**：包含非线性振荡器、大肠杆菌生长模型、应力-应变实验数据。
    *   **LLM-SRBench**：包含 239 个任务，分为 LSR-Transform（公式变形）和 LSR-Synth（合成新公式），涵盖化学、生物、物理和材料科学。
    *   **实际应用案例**：周期性山丘流（Periodic Hills）的湍流模型建模，旨在发现非线性雷诺应力各向异性张量的符号修正项。
*   **对比方法**：包括传统方法（GPlearn, PySR）、神经符号方法（uDSR, RAG-SR）以及最先进的 LLM 方法（LLM-SR, LaSR, SGA）。
*   **评估指标**：平均准确率（Acc avg）、符号准确率（SA）、归一化均方误差（NMSE）以及 OOD（分布外）泛化性能。

#### 4. 资源与算力
*   **硬件环境**：实验在单张 **NVIDIA RTX 3090 GPU** 上完成。
*   **模型配置**：使用了 4-bit 量化的开源模型，包括 Llama-3.1-8B、Llama-3.2-3B 和 Llama-3.2-1B。
*   **训练细节**：每个任务运行 2500 次搜索迭代。通过 LoRA（秩 r=16）进行高效微调，学习率为 1e-6。论文强调了该方法在受限算力下的可行性。

#### 5. 实验数量与充分性
*   **实验规模**：论文进行了大规模测试，涵盖了超过 240 个合成与真实科学任务。
*   **消融实验**：针对“物理约束”和“Token 正则化”分别进行了消融，证明了两者对降低 NMSE 和提升 OOD 泛化能力的必要性。
*   **客观性与公平性**：在相同的时间预算（Wall-clock time）和相同的 LLM 底座下对比了 LLM-SR，确保了效率对比的公正性。此外，通过 CFD（计算流体力学）软件 OpenFOAM 进行了后验验证，证明了发现的方程在实际物理仿真中的有效性。

#### 6. 主要结论与发现
*   **性能领先**：PiT-PO 在所有基准测试中均达到 SOTA，尤其在符号准确率（SA）上显著优于现有方法。
*   **小模型逆袭**：通过策略优化，**1B/3B 的小规模开源模型在 PiT-PO 框架下能够超越 GPT-4o-mini 或 Mixtral 等闭源巨头**。
*   **突破停滞**：相比于 LLM-SR 容易陷入局部最优（拟合好但结构错），PiT-PO 的双重约束能引导模型在搜索后期实现“阶跃式”的误差下降。
*   **物理保真度**：在湍流建模中，PiT-PO 发现的方程在预测分离泡大小和皮肤摩擦系数方面比传统 RANS 模型更接近 DNS（直接数值模拟）真值。

#### 7. 优点
*   **自适应性**：通过 RL 实时更新模型，解决了 LLM 预训练知识与特定科学任务之间的鸿沟。
*   **理论支撑**：引入支持排除定理，为方程的“稀疏性”和“精简性”提供了数学依据，而非仅靠启发式惩罚。
*   **计算友好**：支持量化和 LoRA，使得在消费级显卡上进行前沿科学发现成为可能。

#### 8. 不足与局限
*   **计算开销**：虽然单次迭代效率高，但相比于纯 Prompt 方法，增加了微调（Fine-tuning）的额外时间开销。
*   **先验依赖**：对于完全未知的领域，如果无法定义有效的层级物理约束（如领域特定先验），该方法的优势可能会减弱。
*   **超参数敏感性**：强化学习中的奖励权重（如物理惩罚与拟合奖励的比例）可能需要针对不同任务进行微调。

（完）
