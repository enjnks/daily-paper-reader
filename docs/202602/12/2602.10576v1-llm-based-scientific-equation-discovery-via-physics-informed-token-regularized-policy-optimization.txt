Title: LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization

URL Source: https://arxiv.org/pdf/2602.10576v1

Published Time: Thu, 12 Feb 2026 01:41:07 GMT

Number of Pages: 17

Markdown Content:
# LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

## Boxiao Wang 

wangboxiao22@mails.ucas.ac.cn Institute of Automation, Chinese Academy of Sciences Beijing, China 

## Kai Li âˆ—

kai.li@ia.ac.cn Institute of Automation, Chinese Academy of Sciences Beijing, China 

## Tianyi Liu 

franktyliu@outlook.com State Key Laboratory of Aerodynamics Mianyang, Sichuan, China 

## Chen Li 

lichen@skla.cardc.cn State Key Laboratory of Aerodynamics Mianyang, Sichuan, China 

## Junzhe Wang 

wangjunzhe21@mails.ucas.ac.cn School of Mathematical Sciences, University of Chinese Academy of Sciences Beijing, China 

## Yifan Zhang 

yifan.zhang@ia.ac.cn Institute of Automation, Chinese Academy of Sciences Beijing, China 

## Jian Cheng 

jian.cheng@ia.ac.cn Institute of Automation, Chinese Academy of Sciences Beijing, China 

Abstract 

Symbolic regression aims to distill mathematical equations from observational data. Recent approaches have successfully leveraged Large Language Models (LLMs) to generate equation hypotheses, capitalizing on their vast pre-trained scientific priors. However, ex-isting frameworks predominantly treat the LLM as a static genera-tor, relying on prompt-level guidance to steer exploration. This par-adigm fails to update the modelâ€™s internal representations based on search feedback, often yielding physically inconsistent or mathe-matically redundant expressions. In this work, we propose PiT-PO 

(Physics-informed Token-regularized Policy Optimization), a uni-fied framework that evolves the LLM into an adaptive generator via reinforcement learning. Central to PiT-PO is a dual-constraint mechanism that rigorously enforces hierarchical physical validity while simultaneously applying fine-grained, token-level penalties to suppress redundant structures. Consequently, PiT-PO aligns LLM to produce equations that are both scientifically consistent and structurally parsimonious. Empirically, PiT-PO achieves state-of-the-art performance on standard benchmarks and successfully dis-covers novel turbulence models for challenging fluid dynamics prob-lems. We also demonstrate that PiT-PO empowers small-scale mod-els to outperform closed-source giants, democratizing access to high-performance scientific discovery. 

1 Introduction 

Symbolic Regression (SR) [22] stands as a cornerstone of data-driven scientific discovery, uniquely capable of distilling interpretable math-ematical equations from observational data. Unlike black-box mod-els that prioritize mere prediction, SR elucidates the fundamen-tal mechanisms governing system behavior, proving instrumen-tal in uncovering physical laws [21, 34], modeling chemical kinet-ics [4, 7], and analyzing complex biological dynamics [39, 48]. However, the search for exact governing equations represents a formidable challenge, formally classified as an NP-hard problem [47]. To navigate this vast search space, algorithmic strategies have evolved from Genetic Programming (GP) [5, 37] and Reinforcement Learn-ing (RL) [31] to Transformer-based architectures that map numer-ical data directly to symbolic equations [2, 14, 53]. Most recently, the advent of Large Language Models (LLMs) has introduced a new paradigm. Methods such as LLM-SR [40] and LaSR [10] leverage the pre-trained scientific priors and in-context learning capabili-ties of LLMs to generate equation hypotheses. These approaches typically employ an evolutionary search paradigm, where candi-dates are evaluated, and high-performing solutions are fed back via prompt-level conditioning to steer subsequent generation. Despite encouraging progress, existing LLM-based SR methods remain constrained by severe limitations. First, most approaches treat the LLM as a static generator, relying primarily on prompt-level, verbal guidance to steer the evolutionary search [10, 40]. This â€œfrozenâ€ paradigm inherently neglects the opportunity to adapt and enhance the generative capability of the LLM itself based on evaluation signals, preventing the model from internalizing feed-back and adjusting its generation strategies to the specific problem. Second, they typically operate in a physics-agnostic manner, prior-itizing syntactic correctness over physical validity [10, 40]. With-out rigorous constraints, LLMs often generate equations that fit the data numerically but violate fundamental physical principles, rendering them prone to overfitting and practically unusable. In this work, we propose to fundamentally shift the role of the LLM in SR from a static proposer to an adaptive generator. We es-tablish a dynamic feedback loop in which evolutionary exploration and parametric learning reinforce each other: evolutionary search uncovers diverse candidate equations and generates informative evaluation signals, while parametric adaptation enables the LLM 

> arXiv:2602.10576v1 [cs.LG] 11 Feb 2026

to consolidate effective symbolic patterns and guide subsequent ex-ploration more efficiently. By employing in-search fine-tuning, i.e., updating the LLM parameters during the evolutionary search pro-cess, we move beyond purely verbal, prompt-level guidance and in-troduce numerical guidance that allows feedback to be directly in-ternalized into the model parameters, progressively aligning LLM with the intrinsic properties of the target system. To realize this vision, we introduce PiT-PO (Physics-informed Token-regularized Policy Optimization), a unified framework that bridges LLM-driven evolutionary exploration with rigorous veri-fication. PiT-PO is built upon two technical components. 1) In-Search LLM Evolution. We implement the numerical guidance via reinforcement learning, which efficiently updates the LLMâ€™s pa-rameters during the search process. Instead of relying on static pre-trained knowledge, this in-search policy optimization enables LLM to dynamically align its generative distribution with the structural characteristics of the specific task, effectively transforming general scientific priors into domain-specific expertise on the fly. 2) Dual Constraints as Search Guidance. We enforce hierarchical physi-cal constraints to ensure scientific validity, and uniquely, we incor-porate a fine-grained regularization based on our proposed Support Exclusion Theorem . This theorem allows us to identify mathemati-cally redundant terms and translate them into token-level penal-ties, effectively pruning the search space and guiding LLM toward physically meaningful and structurally parsimonious equations. Comprehensive experimental results demonstrate that PiT-PO achieves state-of-the-art performance across standard SR bench-marks, including the LLM-SR Suite [40] and LLM-SRBench [41], and recovers the largest number of ground-truth equations among all evaluated methods. In addition to synthetic benchmarks, the ef-fectiveness of PiT-PO is validated on an application-driven turbu-lence modeling task involving flow over periodic hills. PiT-PO im-proves upon traditional Reynolds-Averaged Navier-Stokes (RANS) approaches by producing anisotropic Reynolds stresses closer to Direct Numerical Simulation (DNS) references. The learned method shows enhanced physical consistency, with reduced non-physical extremes and better flow field predictions. Finally, PiT-PO main-tains robust performance even when using resource-constrained small models, such as a quantized version of Llama-8B, and re-mains efficient under strict wall-clock time budgets, establishing a practical methodology for automated scientific discovery. 

2 Preliminaries 2.1 Problem Setup 

In SR, a dataset of inputâ€“output observations is given: 

ğ· = {(ğ‘¥ ğ‘– , ğ‘¦ ğ‘– )} ğ‘› ğ‘–=1 , ğ‘¥ ğ‘– âˆˆ â„ ğ‘‘ , ğ‘¦ ğ‘– âˆˆ â„. (1) The objective is to identify a compact and interpretable function 

ğ‘“ âˆˆ â„± such that ğ‘“ (ğ‘¥ ğ‘– ) â‰ˆ ğ‘¦ ğ‘– for the observed samples, while retain-ing the ability to generalize to unseen inputs. 

2.2 LLM-based SR Methods 

Contemporary LLM-based approaches reformulate SR as a itera-tive program synthesis task. In this paradigm, typified by frame-works such as LLM-SR [40], the discovery process is decoupled into Island-Based Exploration 

> General-Level Domain-Speci fi c
> Theoretical Constraints
> Support Exclusion
> Theorem
> Global Reward Token Penalty
> GRPO

PiT-PO  

> Physics-informed Token-Regularized Policy Optimization
> Physical Constraints
> Token-Aware Advantage Estimation
> Policy Update
> Prompt Update
> LLM Policy

Figure 1: The overall framework of PiT-PO. PiT-PO trans-forms the LLM from a static proposer into an adaptive gener-ator via a closed-loop evolutionary process. The framework integrates dual-constraint evaluationâ€”comprising physical constraints and theoretical constraintsâ€”to generate fine-grained token-level learning signals. These signals guide the LLM policy update via reinforcement learning, ensuring the discovery of parsimonious, physically consistent equations. 

two phases: structure proposal and parameter estimation. Specif-ically, the LLM functions as a symbolic generator, emitting func-tional skeletons with placeholders for learnable coefficients. A nu-merical optimizer (e.g., BFGS [9]) subsequently fits these constants to the observed data. To navigate the combinatorial search space, these methods employ an evolutionary-style feedback loop: high-fitness equations are maintained in a pool to serve as in-context ex-amples, prompting the LLM to refine subsequent generations. Our work leverages this architecture as a backbone, but fundamentally redefines the LLMâ€™s role from a static proposer to an adaptive gen-erator. 

2.3 Group Relative Policy Optimization 

Group Relative Policy Optimization (GRPO) [38] is a RL algorithm tailored for optimizing LLMs on reasoning tasks, characterized by its efficient baseline estimation without the need for a separate value network. In the context of LLM-based SR, the generation process is modeled as a Markov Decision Process (MDP), where LLM functions as a policy ğœ‹ ğœƒ that generates a sequence of tokens 

ğ‘œ = (ğ‘¡ 1, â€¦ , ğ‘¡ ğ¿ ) given a prompt ğ‘ . For each ğ‘ , GRPO samples a group of ğº outputs {ğ‘œ 1, â€¦ , ğ‘œ ğº } from the sampling policy ğœ‹ ğœƒ ğ‘œğ‘™ğ‘‘ . GRPO max-imizes the following surrogate loss function:       

> ğ’¥ ğºğ‘…ğ‘ƒğ‘‚ (ğœƒ) = ğ”¼ ğ‘âˆ¼ğ‘ƒ(ğ‘„),{ğ‘œ ğ‘– }âˆ¼ğœ‹ ğœƒ ğ‘œğ‘™ğ‘‘ [ 1ğº
> ğº
> âˆ‘
> ğ‘–=1
> ( 1ğ¿ ğ‘– ğ¿ ğ‘–
> âˆ‘
> ğ‘˜=1
> â„’ğ‘ğ‘™ğ‘–ğ‘ ğ‘–,ğ‘˜ (ğœƒ) âˆ’ ğ›½ğ”» ğ¾ ğ¿ (ğœ‹ ğœƒ ||ğœ‹ ğ‘Ÿğ‘’ğ‘“ ))] , (2)

where ğœ‹ ğ‘Ÿğ‘’ğ‘“ is the reference policy to prevent excessive deviation, and ğ›½ controls the KL-divergence penalty. The clipping term â„’ğ‘ğ‘™ğ‘–ğ‘ ğ‘–,ğ‘˜ (ğœƒ) 

ensures trust region updates:                   

> â„’ğ‘ğ‘™ğ‘–ğ‘ ğ‘–,ğ‘˜ (ğœƒ) = min ( ğœ‹ ğœƒ (ğ‘¡ ğ‘–,ğ‘˜ |ğ‘, ğ‘œ ğ‘–,<ğ‘˜ )ğœ‹ ğœƒ ğ‘œğ‘™ğ‘‘ (ğ‘¡ ğ‘–,ğ‘˜ |ğ‘, ğ‘œ ğ‘–,<ğ‘˜ )Ì‚ ğ´ ğ‘– ,clip ( ğœ‹ ğœƒ (ğ‘¡ ğ‘–,ğ‘˜ |ğ‘, ğ‘œ ğ‘–,<ğ‘˜ )ğœ‹ ğœƒ ğ‘œğ‘™ğ‘‘ (ğ‘¡ ğ‘–,ğ‘˜ |ğ‘, ğ‘œ ğ‘–,<ğ‘˜ ) , 1 âˆ’ ğœ–, 1 + ğœ–)Ì‚ ğ´ ğ‘– ) . (3)

Here, ğœ– is the clipping coefficient. A distinctive feature of GRPO lies in its advantage estimation, it computes the advantage Ì‚ ğ´ ğ‘– by LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

standardizing the reward ğ‘…(ğ‘œ ğ‘– ) relative to the group: Ì‚ğ´ ğ‘– = ğ‘…(ğ‘œ ğ‘– ) âˆ’ mean ({ğ‘…(ğ‘œ ğ‘— )}) 

std ({ğ‘…(ğ‘œ ğ‘— )}) . (4) Consequently, every token within the sequence ğ‘œ ğ‘– is assigned the exact same feedback signal. This coarse granularity treats valid and redundant terms indistinguishably, a limitation that our work ad-dresses by introducing fine-grained, token-level regularization. 

3 Method 

We propose PiT-PO (Physics-informed Token-regularized Policy Optimization), a framework that evolves LLM into an adaptive, physics-aware generator. PiT-PO establishes a closed-loop evolu-tionary process driven by two synergistic mechanisms: (1) a dual-constraint evaluation system that rigorously assesses candidates through hierarchical physical verification and theorem-guided re-dundancy pruning; and (2) a novel policy optimization strategy that updates LLM using fine-grained, token-level feedback derived from these constraints. This combination effectively aligns the LLM with the intrinsic structure of the problem, guiding the LLM to-ward solutions that are not only numerically accurate but also struc-turally parsimonious and scientifically consistent. 

3.1 Dual-Constraint Learning Signals 

Navigating the combinatorial space of symbolic equations requires rigorous guidance. We employ a reward system driven by dual con-straints: physical constraints delineate the scientifically valid re-gion, while theoretical constraints drive the search toward simpler equations by identifying and pruning redundant terms. 

3.1.1 Hierarchical Physical Constraints. To ensure scientific valid-ity, we construct a hierarchical filter that categorizes constraints into two levels: general properties and domain-specific priors. 

General-Level Constraints. We enforce fundamental physical properties applicable across scientific disciplines. To prune physi-cally impossible structures (e.g., adding terms with mismatched units), we assign penalty-based rewards for Dimensional Homo-geneity ( ğ‘ƒ ğ‘‘ğ‘–ğ‘š ) and Differentiability ( ğ‘ƒ ğ‘‘ğ‘–ğ‘“ ğ‘“ ). The former strictly pe-nalizes equations with unit inconsistencies, while the latter en-forces smoothness on the data-defined domain. 

Domain-Specific Constraints. To tackle specialized tasks, we inject expert knowledge as inductive biases. We define the domain-specific penalty ğ‘ƒ (ğ‘—) ğ‘‘ğ‘œğ‘šğ‘ğ‘–ğ‘› to penalize candidate equations that vi-olate the ğ‘— -th domain-specific constraint. Taking the turbulence modeling task (detailed in Appendix E.4) as a representative instan-tiation, we enforce four rigorous constraints: (1) Realizability [32], ensuring the Reynolds stress tensor has positive eigenvalues; (2) Boundary Condition Consistency [27], requiring stresses to decay to zero at the wall; (3) Asymptotic Scaling [44, 49], enforcing the cubic relationship between stress and wall distance in the viscous sublayer; and (4) Energy Consistency [26, 32], aligning predicted stress with turbulent kinetic energy production. This hierarchical design effectively embeds physical consistency as a hard constraint in the reward function, prioritizing scientific validity over mere empirical fitting. 

3.1.2 Theorem-Guided Mathematical Constraints. While physical constraints ensure validity, they do not prevent mathematical re-dundancy. To rigorously distinguish between essential terms and redundant artifacts, we introduce the Support Exclusion Theorem. Let ğ’® denote the full support set containing all candidate ba-sis functions {ğœ™ ğ‘— }. The ground truth equation is ğ‘“ âˆ— = âˆ‘ ğ‘—âˆˆ ğ’® â€² ğ‘ ğ‘— ğœ™ ğ‘— ,where ğ’® â€² âŠ† ğ’® is the true support set (i.e., the indices of basis func-tions that truly appear in the governing equation), and {ğ‘ ğ‘— }ğ‘—âˆˆ ğ’® â€² are the corresponding true coefficients. Consider a candidate equation 

ğ‘“ = âˆ‘ ğ‘—âˆˆ ğ’¦ ğ‘ ğ‘— ğœ™ ğ‘— , where ğ’¦ âŠ† ğ’® represents the current support set (i.e., the selected terms in the skeleton), b = {ğ‘ ğ‘— }ğ‘—âˆˆ ğ’¦ are the optimized coefficients derived from the data. We define the empir-ical Gram matrix of these basis functions as ğº âˆˆ â„ |ğ’® |Ã—| ğ’® | and the corresponding Projection Matrix as ğ‘‡ , where ğ‘‡ ğ‘–ğ‘— âˆ¶= ğº ğ‘—ğ‘– /ğº ğ‘–ğ‘– .TheoRem 3.1 (SuppoRt Exclusion TheoRem). Assume the ground-truth support is finite and satisfies |ğ’® â€²| â‰¤ ğ‘€ , and let the true function coefficients be bounded by ğ´ â‰¤ |ğ‘ ğ‘— | â‰¤ ğµ for all ğ‘— âˆˆ ğ’® â€². A term ğœ™ ğ‘– 

(ğ‘– âˆˆ ğ’¦ ) is theoretically guaranteed to be a false discovery (not in the true support ğ’® â€²) if its fitted coefficient magnitude satisfies: 

|ğ‘ ğ‘– | < ğ´ âˆ’ â›âœâœâœâœââˆ‘

> ğ‘—âˆˆ ğ’¦ ,ğ‘—â‰ ğ‘–

(ğµ + |ğ‘ ğ‘— |)|ğ‘‡ ğ‘–ğ‘— |âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> Internal Interference

+ ğµ 

> ğ‘š

âˆ‘

> ğ‘˜=1

ğ‘  (ğ‘˜) 

âŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> External Interference

ââŸâŸâŸâŸâ . (5) 

ğ‘ (ğ‘˜) denotes the ğ‘˜ -th largest value in {|ğ‘‡ ğ‘–â„“ | âˆ¶ â„“ âˆˆ ğ’® âˆ– ğ’¦ }, and ğ‘š âˆ¶= 

min (ğ‘€ âˆ’ 1, | ğ’® âˆ– ğ’¦ |) .

Detailed definitions of all notations and the rigorous proof of Theorem 3.1 are provided in Appendix B. This theorem formalizes the intuition that coefficients of redundant terms (absent from the true support ğ’® â€²) have significantly smaller magnitudes than those of valid components. Specifically, after fitting b, we compute the normalized coeffi-cient ratio ğœ ğ‘– = |ğ‘ ğ‘– |/(âˆ‘ ğ‘— |ğ‘ ğ‘— |+ğœ–) . We introduce a threshold ğœŒ âˆˆ (0, 1) 

to identify potentially redundant terms. Terms satisfying ğœ ğ‘– > ğœŒ 

incur no penalty, while components with ğœ ğ‘– â‰¤ ğœŒ are considered redundant. To suppress these redundancies, we define a token penalty for each token in redundant term ğ‘– :

ğ‘ƒ ğ‘¡ğ‘œğ‘˜ = ğ‘ â‹… max (0, âˆ’ log (|ğ‘ ğ‘– | + ğœ–)) , (6) where ğ‘ > 0 is a scaling coefficient. We use a logarithmic scale to impose stronger penalties on terms with smaller coefficients. By integrating this penalty into the policy optimization, we guide the LLM to reduce the probability of generating redundant terms, thereby steering the optimization toward parsimonious equations. 

3.2 Token-Aware Policy Update 

Our proposed PiT-PO effectively operationalize the hierarchical constraints and theoretical insights derived in Section 3.1. Unlike standard GRPO that assign a uniform scalar reward to the entire generated sequence, our method transitions the learning process from coarse-grained sequence scoring to fine-grained token-level credit assignment. This ensures that the policy not only learns to generate physically valid equations but also explicitly suppresses theoretically redundant terms. 3.2.1 Global Reward with Gated Constraints. The optimization is driven by a composite global reward, ğ‘… ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ , which balances fit-ting accuracy, structural parsimony, and physical consistency. For-mally, for a sampled equation ğ‘œ ğ‘– , the rewards are defined as follows: 

Fitting Accuracy (ğ‘… ğ‘“ ğ‘–ğ‘¡ ). We use the normalized log-MSE to en-courage precise data fitting: 

ğ‘… ğ‘“ ğ‘–ğ‘¡ = âˆ’ğ›¼ log (MSE + ğœ–), (7) where ğ‘€ğ‘†ğ¸ = 1ğ‘› âˆ‘ğ‘› ğ‘–=1 (ğ‘¦ ğ‘– âˆ’Ì‚ ğ‘¦ ğ‘– )2.

Complexity Penalty (ğ‘ƒ ğ‘ğ‘ğ‘™ğ‘¥ ). Adhering to Occamâ€™s Razor, we penalize structural complexity based on the Abstract Syntax Tree (AST) [30] node count: 

ğ‘ƒ ğ‘ğ‘ğ‘™ğ‘¥ = ğœ† ğ‘™ğ‘’ğ‘› â‹… Length (AST ). (8) In PiT-PO, each equation generated by the LLM is represented as a Python function and parsed into an AST, where each node cor-responds to a variable or operator. The total node count provides a meaningful estimate of structural complexity. 

Gated Physical Penalty (ğ‘ƒ ğ‘â„ğ‘¦ ). Imposing strict physical con-straints too early can hinder exploration, causing the model to dis-card potentially promising functional forms. We therefore activate physical penalties only after the candidate equation reaches a base-line fitting accuracy threshold ( ğ›¿ ğ‘”ğ‘ğ‘¡ğ‘’ ). Specifically, we define 

ğ‘ƒ ğ‘â„ğ‘¦ (ğ‘œ ğ‘– ) = 1(MSE (ğ‘œ ğ‘– ) < ğ›¿ ğ‘”ğ‘ğ‘¡ğ‘’ ) (ğ‘ƒ ğ‘‘ğ‘–ğ‘“ ğ‘“ (ğ‘œ ğ‘– ) + ğ‘ƒ ğ‘‘ğ‘–ğ‘š (ğ‘œ ğ‘– ) + âˆ‘ ğ½ ğ‘—=1 ğ‘ƒ (ğ‘—) 

> domain

(ğ‘œ ğ‘– )), 

(9) where ğŸ™(â‹…) is the indicator function. This mechanism effectively creates a soft curriculum: it allows â€œfreeâ€ exploration in the early stages and enforcing strict physical compliance only after the so-lution enters a plausible region. The total reward ğ‘… ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ is then formulated as: 

ğ‘… ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘œ ğ‘– ) = ğ‘… ğ‘“ ğ‘–ğ‘¡ (ğ‘œ ğ‘– ) âˆ’ ğ‘ƒ ğ‘ğ‘ğ‘™ğ‘¥ (ğ‘œ ğ‘– ) âˆ’ ğ‘ƒ ğ‘â„ğ‘¦ (ğ‘œ ğ‘– ). (10) 

3.2.2 Fine-Grained Advantage Estimation. Standard GRPO applies a uniform advantage across all tokens in a sequence. We refine this by synthesizing the global reward with the token-level penalty ğ‘ƒ ğ‘¡ğ‘œğ‘˜ 

(Equation 6). Specifically, we define the token-aware advantage Ì‚ğ´ ğ‘–,ğ‘˜ for the ğ‘˜ -th token in the ğ‘– -th sampled equation as: Ì‚ğ´ ğ‘–,ğ‘˜ = ğ‘… ğ‘”ğ‘™ğ‘œğ‘ğ‘ğ‘™ (ğ‘œ ğ‘– ) âˆ’ ğœ‡ ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ 

ğœ ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> Global Standardization

âˆ’ ğ‘ƒ ğ‘–,ğ‘˜ âŸ

> Local Pruning

. (11) Here, the first term standardizes the global reward against the group statistics ( ğœ‡ ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ , ğœ ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘ ), reinforcing equations that satisfy multi-objective criteria relative to their peers. The second term, ğ‘ƒ ğ‘–,ğ‘˜ ,applies a targeted penalty to suppress redundancy. Specifically, we set ğ‘ƒ ğ‘–,ğ‘˜ = 0 if token ğ‘˜ belongs to a non-redundant term, and 

ğ‘ƒ ğ‘–,ğ‘˜ = ğ‘ƒ ğ‘¡ğ‘œğ‘˜ otherwise. This ensures that penalties are applied exclu-sively to tokens contributing to mathematically redundant struc-tures, while valid terms remain unaffected. Substituting this token-aware advantage into the GRPO objec-tive, the policy gradient update of our PiT-PO becomes: 

âˆ‡ğ’¥ ğ‘ƒğ‘–ğ‘‡ âˆ’ğ‘ƒğ‘‚ âˆ âˆ‘ 

> ğ‘–,ğ‘˜

Ì‚ğ´ ğ‘–,ğ‘˜ âˆ‡ log ğœ‹ ğœƒ (ğ‘¡ ğ‘–,ğ‘˜ |ğ‘œ ğ‘–,<ğ‘˜ ). (12) 

Algorithm 1: PiT-PO Overall Training Pipeline 

Input: Dataset ğ· = {(ğ‘¥ ğ‘– , ğ‘¦ ğ‘– )} ğ‘› ğ‘–=1 ; LLM ğœ‹ ğœƒ ; number of islands 

ğ‘ ; group size ğº ; iterations ğ‘‡ .

Output: Best equation ğ‘œ âˆ—. 

> 1

Initialize ğ‘œ âˆ—, ğ‘  âˆ—, and buffers â„¬ğ‘— â† âˆ… for ğ‘— = 1, â€¦ , ğ‘  

> 2

for ğ‘¡ â† 1 to ğ‘‡ do 

// Stage 1: Island-Based Exploration  

> 3

for ğ‘— â† 1 to ğ‘ do  

> 4

ğ‘ ğ‘— â† BuildPRompt (ğ·, â„¬ğ‘— ) // in-context rule  

> 5

{ğ‘œ ğ‘– }ğº ğ‘–=1 âˆ¼ ğœ‹ ğœƒ (â‹… âˆ£ ğ‘ ğ‘— ) // sample a group  

> 6

for ğ‘– â† 1 to ğº do  

> 7

(ğ‘… ğ‘– , {ğ‘ƒ ğ‘–,ğ‘˜ }) â† DualConstRaintEval (ğ‘œ ğ‘– , ğ·) 

// ğ‘… ğ‘– = ğ‘… global (ğ‘œ ğ‘– ) 

> 8

â„¬ğ‘— â† â„¬ğ‘— âˆª {(ğ‘ ğ‘— , ğ‘œ ğ‘– , ğ‘… ğ‘– , {ğ‘ƒ ğ‘–,ğ‘˜ })}  

> 9

if ğ‘… ğ‘– > ğ‘  âˆ— then  

> 10

ğ‘œ âˆ— â† ğ‘œ ğ‘– ; ğ‘  âˆ— â† ğ‘… ğ‘– 

// Stage 2: In-Search LLM Evolution  

> 11

ğœƒ â† PiT-PO_Update (ğœƒ, { â„¬ğ‘— }ğ‘ ğ‘—=1 , ğœ‹ ğœƒ )

// Stage 3: Hierarchical Selection  

> 12

{â„¬ğ‘— }ğ‘ ğ‘—=1 â† SelectAndReset ({ â„¬ğ‘— }ğ‘ ğ‘—=1 ) 

> 13

return ğ‘œ âˆ—

This creates a dual-pressure optimization landscape: global rewards guide the policy toward physically consistent and accurate equa-tions, while local penalties surgically excise redundant terms. This ensures the final output aligns with the sparse, underlying physical laws rather than merely overfitting numerical data. 

3.3 Overall Training Pipeline 

We orchestrate the PiT-PO framework through a closed-loop evolu-tionary RL cycle. As illustrated in Algorithm 1, the training process iterates through three synergistic phases: Phase 1: Island-Based Exploration (Data Generation). To pre-vent premature convergence to local optima, a common pitfall in SR, we employ a standard multi-island topology ( ğ‘ islands) to structurally enforce search diversity [5, 35]. Each island ğ‘— main-tains an isolated experience buffer â„¬ğ‘— , evolving its own lineage of equations. This information isolation allows distinct islands to cul-tivate diverse functional forms independently. Phase 2: In-Search LLM Evolution (Policy Update). This phase transforms the collected data into parametric knowledge. We ag-gregate the trajectories from all ğ‘ islands into a global batch to perform policy optimization using PiT-PO. By minimizing the loss, the model explicitly lowers the probability of generating mathe-matically redundant tokens and physically inconsistent structures. To ensure computational efficiency during this iterative search, we implement the update using Low-Rank Adaptation (LoRA) [12]. Phase 3: Hierarchical Selection (Population Management). We apply standard survival-of-the-fittest mechanisms to maintain pop-ulation quality. Local buffers â„¬ğ‘— are updated by retaining only top-performing candidates, while underperforming islands are period-ically reset with high-fitness seeds to escape local optima. LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

This cycle establishes a reciprocal reinforcement mechanism: the island-based exploration maintains search diversity, while pol-icy update consolidates these findings into the model weights, pro-gressively transforming the LLM into a domain-specialized scien-tific discoverer. 

4 Experiments 4.1 Setup 

Benchmarks. To provide a comprehensive evaluation of PiT-PO, we adopt two widely used benchmarks to compare against state-of-the-art baselines: 

LLM-SR Suite [40]. This suite comprises four tasks spanning mul-tiple scientific domains: Oscillation 1 & 2 (Nonlinear Oscillatory Systems) feature dissipative couplings and non-polynomial nonlin-earities with explicit forcing, making recovery of the correct inter-action terms from trajectory data non-trivial; E. coli Growth [28, 36]models multivariate population dynamics with strongly cou-pled, multiplicative effects from nutrients, temperature, and acid-ity; and Stress-Strain [1] uses experimental measurements of Alu-minum 6061-T651 and exhibits temperature-dependent, piecewise non-linear deformation behavior. Detailed information about these tasks is provided in Appendix D.1. 

LLM-SRBench: [41] To evaluate generalization beyond canoni-cal forms, we adopt the comprehensive LLM-SRBench benchmark, which contains 239 tasks organized into two complementary sub-sets, LSR-Transform and LSR-Synth. LSR-Transform changes the prediction target to rewrite well-known physics equations into less common yet analytically equivalent forms, producing 111 trans-formed tasks. This design aims to reduce reliance on direct mem-orization of canonical templates and tests whether a method can recover the same physical law under non-trivial variable reparame-terizations. Complementarily, LSR-Synth composes equations from both known scientific terms and synthetic but plausible terms to further assess discovery beyond memorized templates: candidate terms are proposed by an LLM under domain context, assembled into full equations, and then filtered through multiple checks, in-cluding numerical solvability, contextual novelty, and expert plau-sibility, yielding 128 synthetic tasks. Further details are given in Appendix D.2. 

Baselines. 

We compare PiT-PO against representative baselines spanning both classical and LLM-based SR methods. For the four tasks in the LLM-SR Suite, we include GPlearn, a genetic programming-based SR approach; PySR [10], which couples evolutionary search with symbolic simplification; uDSR [17], which replaces the RNN policy in DSR with a pretrained Transformer and employs neural-guided decoding; RAG-SR [53], which incorporates structure retrieval to assist equation generation; and LLM-SR [40]. For the broader LLM-SRBench benchmark, we further compare against leading LLM-based SR methods, including SGA [20], which integrates LLM-driven hypothesis proposal with physics-informed parameter optimiza-tion in a bilevel search framework, and LaSR [10], which leverages abstract symbolic concepts distilled from prior equations to guide hybrid LLMâ€“evolutionary generation. 

Evaluation metrics. We evaluate methods using Accuracy to Tol-erance and Normalized Mean Squared Error (NMSE). For a toler-ance ğœ , we report Acc all (ğœ ) [2]and Acc avg (ğœ ) based on relative error:     

> Acc all (ğœ ) = 1(max 1â‰¤ğ‘–â‰¤ğ‘ test |Ì‚ğ‘¦ ğ‘– âˆ’ğ‘¦ ğ‘–
> ğ‘¦ ğ‘–
> | â‰¤ ğœ )

and Acc avg (ğœ ) = 1ğ‘ test   

> âˆ‘ğ‘ test
> ğ‘–=1 1(| Ì‚ğ‘¦ ğ‘– âˆ’ğ‘¦ ğ‘–
> ğ‘¦ ğ‘–
> | â‰¤ ğœ )

, where Ì‚ ğ‘¦ ğ‘– and ğ‘¦ ğ‘– denote the predicted and ground-truth values at the ğ‘– -th test point, respectively. We additionally report NMSE =

> 1ğ‘ test

âˆ‘ğ‘ test   

> ğ‘–=1 (Ì‚ğ‘¦ ğ‘– âˆ’ğ‘¦ ğ‘– )2
> Var (ğ‘¦)

to assess overall numerical accuracy. We addi-tionally adopt the Symbolic Accuracy (SA) metric [41], which di-rectly measures whether the discovered equation recovers the cor-rect symbolic form (i.e., whether it is mathematically equivalent to the ground-truth equation up to fitted constants). 

Hyperparameter Configurations. All experiments were run for 2,500 search iterations. To ensure a fair comparison, all hyperpa-rameters related to LLM generation and search were kept consis-tent with the default configuration of LLM-SR. For the in-search policy optimization specific to PiT-PO, we use a learning rate of 

1 Ã— 10 âˆ’6 , a group size of ğº = 4 , and a multi-island setting of ğ‘ = 4 ,resulting in an effective per-device batch size of ğº Ã—ğ‘ = 16 . The co-efficient of the KL regularization term was set to 0.01, and the LoRA rank was set to ğ‘Ÿ = 16 . In addition, experiments were conducted on a single NVIDIA RTX 3090 using 4-bit quantized Llama-3.2-1B-Instruct, Llama-3.2-3B-Instruct, and Llama-3.1-8B-Instruct [15] to evaluate the training stability and performance transferability of PiT-PO across different parameter scales under constrained com-pute and memory budgets. More details are in Appendix A. 

4.2 PiT-PO Demonstrates Superior Equation Discovery Capability 

As evidenced in Table 1, PiT-PO establishes a new state-of-the-art on LLM-SR Suite. It consistently dominates baseline methods across all metrics, achieving the highest accuracy while maintain-ing the lowest NMSE in nearly all test cases. Crucially, when con-trolling for the LLM backbone, PiT-PO yields a substantial perfor-mance margin over LLM-SR, validating the effectiveness of our in-search policy optimization framework. Notably, PiT-PO is the only approach to successfully identify the exact ground-truth equation for the Oscillator 1. This structural precision extends to the larger-scale LLM-SRBench (Table 2), where PiT-PO achieves the highest symbolic accuracy across all categories. These results collectively demonstrate that PiT-PO not only fits data numerically but excels in uncovering the true underlying equations. These quantitative gains are not accidental but stem from PiT-POâ€™s structural awareness. Analysis of the iterative trajectories of LLM-SR and PiT-PO in Appendix C.4 corroborates this conclusion: the iterations of LLM-SR remain persistently influenced by clearly incorrect terms, which violate physical meaning despite provid-ing strong numerical fits, as well as by additional nuisance terms. Consequently, the search of LLM-SR often stagnates in a low-MSE regime without reaching the correct structure. In contrast, once PiT-PO enters the same regime, the dual constraints rapidly elimi-nate terms that improve fitting performance but are structurally incorrect. This behavior highlights the central advantage of the proposed dual-constraint learning signals, in which the physical constraints and token-level penalties provide data-driven signals Models Oscillation 1 Oscillation 2 E. coli growth Stress-Strain 

Acc avg-0.001 (%) â†‘ NMSE â†“ Acc avg-0.001 (%) â†‘ NMSE â†“ Acc avg-0.1 (%) â†‘ NMSE â†“ Acc avg-0.1 (%) â†‘ NMSE â†“

GPlern 0.11 0.0972 0.05 0.2000 0.76 1.0023 28.43 0.3496 uDSR 1.78 0.0002 0.36 0.0856 1.12 0.5059 59.15 0.0639 PySR 3.80 0.0003 7.02 0.0002 2.80 0.4068 70.60 0.0347 RAG-SR 39.47 1.49e-6 0.43 0.0282 2.04 0.2754 76.28 0.0282 LLM-SR (Mixtral) 100.00 1.32e-11 99.98 1.18e-11 2.88 0.0596 71.44 0.0276 LLM-SR (4o-mini) 99.92 8.84e-12 99.97 8.70e-10 5.52 0.0453 85.33 0.0245 LLM-SR (Llama-3.1-8B) 59.58 1.17e-6 99.96 9.66e-10 4.86 0.0555 77.74 0.0246 LLM-SR (Llama-3.2-3B) 39.45 1.76e-6 66.34 6.99e-7 1.08 0.3671 74.78 0.0324 LLM-SR (Llama-3.2-1B) 3.28 4.47e-4 7.81 0.0002 1.70 0.5801 30.35 0.3801 PiT-PO (Llama-3.1-8B) 100.00 6.41e-31 99.99 2.11e-13 10.42 0.0090 84.45 0.0136 

PiT-PO (Llama-3.2-3B) 100.00 7.58e-31 99.97 9.77e-10 7.01 0.0248 84.54 0.0156 PiT-PO (Llama-3.2-1B) 99.95 1.34e-11 99.97 1.70e-8 4.76 0.0240 76.91 0.1767 

Table 1: Overall performance on LLM-SR Suite. 0 625 1250 1875 2500                           

> Iteration
> 10 25
> 10 19
> 10 13
> 10 7
> 10 1
> NMSE (log scale)
> Oscillation 1
> 0625 1250 1875 2500
> Iteration
> 10 11
> 10 8
> 10 5
> 10 2
> NMSE (log scale)
> Oscillation 2
> 0625 1250 1875 2500
> Iteration
> 10 2
> 10 1
> 10 0
> NMSE (log scale)
> E. coli Growth
> 0625 1250 1875 2500
> Iteration
> 10 2
> 10 1
> NMSE (log scale)
> Stress-Strain
> LLM-SR PiT-PO

Figure 2: NMSE trajectories (log scale) over search iterations for LLM-SR and PiT-PO (Llama-3.1-8B) on LLM-SR Suite. Lines denote the median over seeds, and shaded regions indi-cate the minâ€“max range.The remaining iteration curves for smaller backbones (3B and 1B) are deferred to Appendix C.1. 

for precise structural correction, guiding the LLM toward the true underlying equation rather than mere numerical overfitting. 

4.3 PiT-PO Empowers Lightweight Backbones to Rival Large Models 

As shown in Table 1, the performance of PiT-PO with the Llama-3.1-8B, Llama-3.2-3B, and Llama-3.2-1B backbones is competitive with, and often exceeds, the performance of LLM-SR that relies on substantially larger or proprietary models, including Mixtral 8 Ã—7B and 4o-mini. These results indicate that PiT-PO effectively bridges the capa-bility gap between lightweight open-source models and large-scale commercial systems. From a practical standpoint, this reduces the barrier to entry for scientific discovery: by delivering state-of-the-art performance on consumer-grade hardware (even maintaining competitiveness with a 1B backbone), PiT-PO eliminates the depen-dence on massive compute and closed-source APIs, thereby democ-ratizing access to powerful SR tools. w/o Phy w/o TokenReg PiT-PO         

> 10 29
> 10 26
> 10 23
> 10 20
> 10 17
> 10 14
> 10 11
> NMSE (log scale)
> 7.60e-21
> 2.06e-10
> 2.77e-19
> 9.97e-11
> 6.40e-31 1.63e-30
> ID
> OOD
> ID
> OOD

Figure 3: Ablation results of PiT-PO and its variants. 

4.4 PiT-PO Enhances Search Efficiency and Breaks Stagnation 

Figure 2 shows that PiT-PO achieves superior search efficiency in discovering accurate equations. In the early search stage, the red and blue curves are close across all four tasks: both methods pri-marily rely on the fitting signal (MSE) and therefore exhibit com-parable per-iteration progress. As NMSE enters a lower regime, the trajectories consistently separate: PiT-PO exhibits abrupt step-wise drops while LLM-SR tends to plateau, yielding a clear redâ€“ blue gap in every subplot. Concretely, once the search reaches these lower-error regions, PiT-PO repeatedly exits stagnation and transitions to the next accuracy phase with orders-of-magnitude NMSE reductions (most prominently in Oscillation 1 and Oscilla-tion 2, and also evident in E. coli Growth and Stress-Strain), whereas LLM-SR often remains trapped near its current error floor. This behavior confirms that the proposed dual-constraint mechanism effectively activates exactly when naive MSE feedback becomes insufficient. By penalizing physical inconsistencies and structural redundancy, PiT-PO forces the LLM to exit stagnation and transi-tion toward the correct functional form. While the in-search fine-tuning introduces a computational over-head, this cost is decisively outweighed by the substantial gains in performance. As detailed in Appendix C.2, PiT-PO maintains a sig-nificant performance edge even when evaluated under equivalent wall-clock time, demonstrating that the accelerated convergence speed effectively compensates for the additional training time. LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization                                                                                               

> Models LSR-Transform LSR-Synth
> Chemistry Biology Physics Material Science SA(%) â†‘Acc ğ‘ğ‘™ğ‘™âˆ’0.1 (%) â†‘NMSE â†“SA(%) â†‘Acc ğ‘ğ‘™ğ‘™âˆ’0.1 (%) â†‘NMSE â†“SA(%) â†‘Acc ğ‘ğ‘™ğ‘™âˆ’0.1 (%) â†‘NMSE â†“SA(%) â†‘Acc ğ‘ğ‘™ğ‘™âˆ’0.1 (%) â†‘NMSE â†“SA(%) â†‘Acc ğ‘ğ‘™ğ‘™âˆ’0.1 (%) â†‘NMSE â†“
> Direct Prompting 3.61 1.801 0.3697 0.00 0.00 0.0644 0.00 0.00 0.5481 0.00 0.00 0.0459 0.00 0.00 0.0826 SGA 2.70 0.909 0.3519 0.00 8.33 0.0458 0.00 0.00 0.2416 0.00 2.27 0.1549 0.00 12.12 0.0435 LaSR 5.41 45.94 0.0021 0.00 27.77 2.77e-4 4.16 16.66 2.73e-4 4.54 25.02 0.0018 8.21 64.22 7.44e-5 LLM-SR 30.63 38.55 0.0101 8.33 66.66 8.01e-6 25.30 58.33 1.04e-6 6.97 34.09 1.23e-4 4.10 88.12 1.15e-7 PiT-PO 34.23 46.84 0.0056 13.89 77.78 4.13e-7 29.17 70.83 9.37e-8 11.36 40.91 6.57e-5 12.00 92.00 1.18e-8

Table 2: Overall performance on LLM-SRBench (Llama-3.1-8B-Instruct). 0           

> 1
> 2
> 3
> 0123456789
> y/H
> x/H

Î± = 0.5  1.0 0.8    

> cyclic cyclic cyclic cyclic
> solid wall
> solid wall

flow direction 

H

Figure 4: Schematic of the geometries for periodic hills. 

4.5 Ablation Study 

To rigorously validate the contribution of each algorithmic compo-nent, we conduct an ablation study across three settings: w/o Phy ,which excludes the physics-consistency penalty ğ‘ƒ phy ; w/o Token-Reg , which removes the redundancy-aware token-level regulariza-tion; and the full PiT-PO framework. As shown in Figure 3, re-moving any single component leads to a substantial deterioration of NMSE and a larger generalization gap between In-Distribution (ID) and Out-Of-Distribution (OOD) data. These empirical results underscore the necessity of the complete framework, demonstrat-ing that the proposed dual constraints are indispensable for ensur-ing both search stability and robust generalization. 

4.6 Case Study: Turbulence Modeling 

To validate the practical utility of PiT-PO in high-fidelity scientific discovery, we select the Flow over Periodic Hills [52] (Figure 4) as a testbed. This problem is widely recognized in Computational Fluid Dynamics (CFD) [32] as a benchmark for Separated Turbulent Flows , presenting complex features such as strong adverse pressure gradients, massive flow detachment, and reattachment. 

Problem Definition and Physics: The geometry consists of a se-quence of polynomially shaped hills arranged periodically in the streamwise direction. The flow is driven by a constant body force at a bulk Reynolds number of ğ‘…ğ‘’ ğ‘ = 5600 (based on hill height ğ» 

and bulk velocity ğ‘ˆ ğ‘ ). The domain height is fixed at ğ¿ ğ‘¦ /ğ» = 3.036 ,while the streamwise length ğ¿ ğ‘¥ varies with the slope factor ğ›¼ ac-cording to ğ¿ ğ‘¥ /ğ» = 3.858ğ›¼ + 5.142 . Periodic boundary conditions are applied in the streamwise direction, with no-slip conditions on the walls. 

The Scientific Challenge: The challenge lies in the Separation Bubble [32], a region where turbulence exhibits strong anisotropy 

due to streamline curvature. Traditional Linear Eddy Viscosity Mod-els (LEVM) [32], such as the ğ‘˜ -ğœ” SST model [23, 24], rely on the 0.04 0.03 0.02 0.01 0.00 0.01 0.02 0.03                                            

> RANS LLM  -SR DNS
> --
> a11 /ub2
> --
> a22 /ub2
> ------
> a33 /ub2a12 /ub2
> ------
> PiT  -PO  DSRRANS
> --0.04 0.03 0.02 0.01 0.00 0.01 0.02 0.03 ----0.04 0.03 0.02 0.01 0.00 0.01 0.02 0.03 ----0.04 0.03 0.02 0.01 0.00 0.01 0.02 0.03 ----

Figure 5: Comparison of the four anisotropic Reynolds stress components for periodic hill training flow using RANS, DSRRANS, LLM-SR, PiT-PO and DNS, respectively. 

Boussinesq hypothesis which assumes isotropic turbulence. Conse-quently, they systematically fail to predict key flow features, such as the separation bubble size and reattachment location. 

Discovery Objective: Instead of fitting a simple curve, our goal is to discover a Non-linear Constitutive Relation for the Reynolds stress anisotropy tensor ğ‘ ğ‘–ğ‘— and the dimensionless Reynolds stress anisotropy tensor ğ‘ ğ‘–ğ‘— . By learning the Reynolds stress tensor ğœ ğ‘–ğ‘— 

from high-fidelity Direct Numerical Simulation (DNS) [32] data, PiT-PO aims to formulate a symbolic correction term that captures the anisotropic physics missed by linear models. 

Baselines: We follow standard turbulence modeling protocols and compare primarily against the standard ğ‘˜ -ğœ” SST model of RANS. We also include LLM-SR and DSRRANS [42], a strong SR-based turbulence modeling method specifically designed for turbulence tasks. We cast turbulence closure modeling as a SR problem [43] (see Appendix E for details). After obtaining the final symbolic equa-tion, we embed it into a RANS solver of OpenFOAM [50] and run CFD simulations on the periodic-hill configuration. We compare the resulting Reynolds-stress components, mean-velocity fields, and skin-friction profiles against DNS references. Figures 5â€“7 visualize these quantities, enabling a direct assessment of physical fidelity and flow-field prediction quality. Based on the comparative analysis of the anisotropic Reynolds stress contours (Figure 5), DSRRANS and PiT-PO show enhance-ment over the traditional RANS approach. Among them, PiT-PO performs the best: its contour matches the DNS reference most closely, with reduced error compared to DSRRANS and LLM-SR, demonstrating less severe non-physical extremes. RANS             

> PiT -PO DNS DSRRANS
> ux/ub
> 1.20 1.00 0. 80 0.60 0.40 0.20 0.00 0.20 -
> LLM -SR

Figure 6: Non-dimensional stream-wise velocity contours obtained by the learned model and the standard ğ‘˜ -ğœ” SST model of RANS, compared with DNS data. 

The stream-wise velocity contours illustrate the correction of the bubble size, a region of reversed flow that forms when fluid detaches from a surface. In Figure 6, PiT-PO most accurately rep-resents the extent and shape of the recirculation zone, where fluid circulates within the separated region, closely consistent with the DNS data throughout the domain, particularly within the separa-tion region and the recovery layer, where flow re-attaches to the surface. The skin friction coefficient (Figure 7), defined as the ratio of the wall stress to the dynamic pressure of the flow along the bottom wall, is a sensitive metric for predicting flow separation. The ğ‘˜ -ğœ” 

SST model of RANS underestimates the magnitude of the skin fric-tion and predicts a delayed reattachment location compared to the DNS. The learned model (PiT-PO) improves the prediction, align-ing more closely with the DNS profile. These results demonstrate that PiT-PO can generate symbolic equations tailored to turbulence modeling and that, under a poste-riori CFD evaluation, the resulting predictions more closely match DNS references, which increases the practical value of LLM-based SR in real scientific and engineering workflows. With the proposed dual constraints, PiT-PO provides targeted search and learning signals that en-able the internalization of turbulence priors during equation dis-covery, thereby steering the model toward physically consistent and domain-relevant structures. 

5 Related Work 

Traditional SR has been studied through several lines, including ge-netic programming , reinforcement learning [31], and transformer-based generation [3]. Genetic programming [16] casts equation dis-covery as an evolutionary search over tree-structured programs, where candidate expressions are iteratively refined via mutation and crossover. Reinforcement learning-based SR, introduced by Pe-tersen et al. [31], has developed into a family of policy-optimization frameworks [6, 8, 18, 29] that formulate SR as a sequential decision-making process. More recently, transformer-based models [14, 19, 45, 46, 53] have been adopted for SR, using large-scale pretrain-ing to map numerical data directly to equations. However, these methods typically fail to incorporate scientific prior knowledge. Recent progress in natural language processing has further en-abled LLM-based SR methods, including LLM-SR [40], LaSR [10], ICSR [25], CoEvo [11], and SR-Scientist [51]. LLM-SR exploits sci-entific priors that are implicitly captured by LLMs to propose plau-sible functional forms, followed by data-driven parameter estima-tion. LaSR augments SR with abstract concept generation to guide -0.020            

> -0.010
> 0.000
> 0.010
> 0.020
> 0.030
> 0.040
> -1 0123456789
> C f
> x/H

Figure 7: Skin friction distribution along the bottom ob-tained by the learned model and the standard ğ‘˜ -ğœ” SST model of RANS, compared with DNS data. 

hypothesis formation, while ICSR reformulates training examples as in-context prompts to elicit function generation. However, a uni-fying limitation across these methods is their reliance on the LLM as a frozen generator, which precludes incorporating search feed-back to update the generation strategy and consequently restricts their ability to adapt to complex problems. While some recent works, such as SOAR [33] and CALM [13], have begun to explore adaptive in-search tuning, they primarily focus on algorithm discovery or combinatorial optimization prob-lems, whereas our method is specifically tailored for SR. By inte-grating hierarchical physical constraints and theorem-guided to-ken regularization, PiT-PO establishes an adaptive framework ca-pable of discovering accurate and physically consistent equations. 

6 Conclusion 

In this work, we introduced PiT-PO, a unified framework that fun-damentally transforms LLMs from static equation proposers into adaptive, physics-aware generators for SR. By integrating in-search policy optimization with a novel dual-constraint evaluation mech-anism, PiT-PO rigorously enforces hierarchical physical validity while leveraging theorem-guided, token-level penalties to elimi-nate structural redundancy. This synergistic design aligns genera-tion with numerical fitness, scientific consistency, and parsimony, establishing new state-of-the-art performance on SR benchmarks. Beyond synthetic tasks, PiT-PO demonstrates significant practical utility in turbulence modeling, where the discovered symbolic cor-rections improve Reynolds stress and flow-field predictions. No-tably, PiT-PO achieves these results using small open-source back-bones, making it a practical and accessible tool for scientific com-munities with limited computational resources. Looking forward, we plan to extend PiT-PO to broader scientific and engineering domains by enriching the library of domain-specific constraints and validating it across more complex, real-world systems. More-over, we anticipate that integrating PiT-PO with larger-scale multi-modal foundation models could further unlock its potential in pro-cessing heterogeneous scientific data. LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

References 

[1] B. S. Aakash, JohnPatrick Connors, and Michael D Shields. 2019. Stress-strain data for aluminum 6061-T651 from 9 lots at 6 temperatures under uniaxial and plane strain tension. Data in Brief 25 (Aug 2019), 104085. doi:10.1016/j.dib.2019. 104085 [2] Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascandolo. 2021. Neural Symbolic Regression that Scales. arXiv:2106.06427 [cs.LG] https://arxiv.org/abs/2106.06427 [3] L. Biggio*, T. Bendinelli*, A. Neitz, A. Lucchi, and G. Parascandolo. 2021. Neural Symbolic Regression that Scales. In Proceedings of 38th International Conference on Machine Learning (ICML 2021) (Proceedings of Machine Learning Research, Vol. 139) . PMLR, 936â€“945. https://proceedings.mlr.press/v139/biggio21a.html *equal contribution. [4] Jindou Chen, Jidong Tian, Liang Wu, ChenXinWei, Xiaokang Yang, Yaohui Jin, and Yanyan Xu. 2025. KinFormer: Generalizable Dynamical Symbolic Re-gression for Catalytic Organic Reaction Kinetics. In International Conference on Representation Learning , Y. Yue, A. Garg, N. Peng, F. Sha, and R. Yu (Eds.), Vol. 2025. 67058â€“67080. https://proceedings.iclr.cc/paper_files/paper/2025/file/ a76b693f36916a5ed84d6e5b39a0dc03-Paper-Conference.pdf [5] Miles Cranmer. 2023. Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl. arXiv:2305.01582 [astro-ph.IM] https://arxiv.org/ abs/2305.01582 [6] Laure Crochepierre, Lydia Boudjeloud-Assala, and Vincent Barbesant. 2022. In-teractive Reinforcement Learning for Symbolic Regression from Multi-Format Human-Preference Feedbacks. In IJCAI 2022- 31st International Joint Conference on Artificial Intelligence . Vienne, Austria. https://hal.science/hal-03695471 [7] Song Deng, Junjie Wang, Li Tao, Su Zhang, and Hongwei Sun. 2023. EV charg-ing load forecasting model mining algorithm based on hybrid intelligence. Com-puters and Electrical Engineering 112 (2023), 109010. doi:10.1016/j.compeleceng. 2023.109010 [8] Mengge Du, Yuntian Chen, and Dongxiao Zhang. 2023. DISCOVER: Deep iden-tification of symbolically concise open-form PDEs via enhanced reinforcement-learning. arXiv:2210.02181 [cs.LG] https://arxiv.org/abs/2210.02181 [9] Roger Fletcher. 1987. Practical Methods of Optimization (2nd ed.). John Wiley & Sons, Chichester, New York. [10] Arya Grayeli, Atharva Sehgal, Omar Costilla-Reyes, Miles Cranmer, and Swarat Chaudhuri. 2024. Symbolic Regression with a Learned Concept Library. arXiv:2409.09359 [cs.LG] https://arxiv.org/abs/2409.09359 [11] Ping Guo, Qingfu Zhang, and Xi Lin. 2025. CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models. arXiv:2412.18890 [cs.AI] https://arxiv.org/abs/2412.18890 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685 [13] Ziyao Huang, Weiwei Wu, Kui Wu, Jianping Wang, and Wei-Bin Lee. 2025. CALM: Co-evolution of Algorithms and Language Model for Automatic Heuris-tic Design. arXiv:2505.12285 [cs.NE] https://arxiv.org/abs/2505.12285 [14] Pierre-Alexandre Kamienny, StÃ©phane dâ€™Ascoli, Guillaume Lample, and FranÃ§ois Charton. 2022. End-to-end symbolic regression with transformers. arXiv:2204.10532 [cs.LG] https://arxiv.org/abs/2204.10532 [15] Paul Kassianik, Baturay Saglam, Alexander Chen, Blaine Nelson, Anu Vel-lore, Massimo Aufiero, Fraser Burch, Dhruv Kedia, Avi Zohary, Sajana Weerawardhena, Aman Priyanshu, Adam Swanda, Amy Chang, Hyrum Anderson, Kojin Oshiba, Omar Santos, Yaron Singer, and Amin Kar-basi. 2025. Llama-3.1-FoundationAI-SecurityLLM-Base-8B Technical Report. arXiv:2504.21039 [cs.CR] https://arxiv.org/abs/2504.21039 [16] J.R. Koza. 1990. Genetically breeding populations of computer programs to solve problems in artificial intelligence. In [1990] Proceedings of the 2nd International IEEE Conference on Tools for Artificial Intelligence . 819â€“827. doi:10.1109/TAI.1990. 130444 [17] Mikel Landajuela, Chak Shing Lee, Jiachen Yang, Ruben Glatt, Claudio P San-tiago, Ignacio Aravena, Terrell Mundhenk, Garrett Mulcahy, and Brenden K Petersen. 2022. A Unified Framework for Deep Symbolic Regression. In 

Advances in Neural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 33985â€“33998. https://proceedings.neurips.cc/paper_files/paper/2022/file/ dbca58f35bddc6e4003b2dd80e42f838-Paper-Conference.pdf [18] Mikel Landajuela, Brenden K. Petersen, Soo K. Kim, Claudio P. Santiago, Ruben Glatt, T. Nathan Mundhenk, Jacob F. Pettit, and Daniel M. Faissol. 2021. Improv-ing exploration in policy gradient search: Application to symbolic optimization. arXiv:2107.09158 [cs.LG] https://arxiv.org/abs/2107.09158 [19] Wenqiang Li, Weijun Li, Linjun Sun, Min Wu, Lina Yu, Jingyi Liu, Yanjie Li, and Song Tian. 2023. Transformer-based model for symbolic regression via joint supervised learning. In International Conference on Learning Representa-tions . https://api.semanticscholar.org/CorpusID:259298765 [20] Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B. Tenen-baum, Daniela Rus, Chuang Gan, and Wojciech Matusik. 2024. LLM and Simula-tion as Bilevel Optimizers: A New Paradigm to Advance Physical Scientific Dis-covery. In Proceedings of the 41st International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 235) , Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (Eds.). PMLR, 33940â€“33962. https://proceedings.mlr.press/ v235/ma24m.html [21] Nour Makke and Sanjay Chawla. 2024. Data-driven discovery of Tsallis-like distribution using symbolic regression in high-energy physics. PNAS Nexus 

3, 11 (10 2024), pgae467. arXiv:https://academic.oup.com/pnasnexus/article-pdf/3/11/pgae467/60816181/pgae467.pdf doi:10.1093/pnasnexus/pgae467 [22] Nour Makke and Sanjay Chawla. 2024. Interpretable scientific discovery with symbolic regression: a review. Artificial Intelligence Review 57 (01 2024). doi:10. 1007/s10462-023-10622-0 [23] Florian R Menter. 1994. Two-equation eddy-viscosity turbulence models for engineering applications. AIAA journal 32, 8 (1994), 1598â€“1605. [24] Florian R Menter, Martin Kuntz, Robin Langtry, et al. 2003. Ten years of in-dustrial experience with the SST turbulence model. Turbulence, heat and mass transfer 4, 1 (2003), 625â€“632. [25] Matteo Merler, Katsiaryna Haitsiukevich, Nicola Dainese, and Pekka Marttinen. 2024. In-Context Symbolic Regression: Leveraging Large Language Models for Function Discovery. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop) . Associa-tion for Computational Linguistics, 589â€“606. doi:10.18653/v1/2024.acl-srw.49 [26] Shinsuke MOCHIZUKI and Hideo OSAKA. 2000. Management of a Stronger Wall Jet by a Pair of Streamwise Vortices. Reynolds Stress Tensor and Produc-tion Tenns. TRANSACTIONS OF THE JAPAN SOCIETY OF MECHANICAL ENGI-NEERS Series B 66, 646 (2000), 1309â€“1317. doi:10.1299/kikaib.66.646_1309 [27] Peter A. Monkewitz. 2021. Asymptotics of streamwise Reynolds stress in wall turbulence. Journal of Fluid Mechanics 931 (Nov. 2021). doi:10.1017/jfm.2021.924 [28] Jacques Monod. 1949. THE GROWTH OF BACTERIAL CULTURES. Annual Review of Microbiology 3, Volume 3, 1949 (1949), 371â€“394. doi:10.1146/annurev. mi.03.100149.002103 [29] T. Nathan Mundhenk, Mikel Landajuela, Ruben Glatt, Claudio P. San-tiago, Daniel M. Faissol, and Brenden K. Petersen. 2021. Symbolic Regression via Neural-Guided Genetic Programming Population Seeding. arXiv:2111.00053 [cs.NE] https://arxiv.org/abs/2111.00053 [30] Iulian Neamtiu, Jeffrey S. Foster, and Michael Hicks. 2005. Understanding source code evolution using abstract syntax tree matching. In Proceedings of the 2005 International Workshop on Mining Software Repositories (St. Louis, Mis-souri) (MSR â€™05) . Association for Computing Machinery, New York, NY, USA, 1â€“5. doi:10.1145/1083142.1083143 [31] Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Claudio P. San-tiago, Soo K. Kim, and Joanne T. Kim. 2021. Deep symbolic regression: Re-covering mathematical expressions from data via risk-seeking policy gradients. arXiv:1912.04871 [cs.LG] https://arxiv.org/abs/1912.04871 [32] Stephen B. Pope. 2000. Turbulent Flows . Cambridge University Press. doi:10. 1017/cbo9780511840531 [33] Julien Pourcel, CÃ©dric Colas, and Pierre-Yves Oudeyer. 2025. Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI. arXiv:2507.14172 [cs.LG] https://arxiv.org/abs/2507.14172 [34] Julia Reuter, Hani Elmestikawy, Fabien Evrard, Sanaz Mostaghim, and Berend van Wachem. 2023. Graph Networks as Inductive Bias for Genetic Program-ming: Symbolic Models for Particle-Laden Flows. In Genetic Programming ,Gisele Pappa, Mario Giacobini, and Zdenek Vasicek (Eds.). Springer Nature Switzerland, Cham, 36â€“51. [35] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan S El-lenberg, Pengming Wang, Omar Fawzi, et al. 2024. Mathematical discoveries from program search with large language models. Nature 625, 7995 (2024), 468â€“ 475. [36] L Rosso, J. R. Lobry, S Bajard, and J. P. Flandrois. 1995. Convenient Model To Describe the Combined Effects of Temperature and pH on Microbial Growth. 

Applied and Environmental Microbiology 61, 2 (Feb 1995), 610â€“6. doi:10.1128/ aem.61.2.610-616.1995 [37] Michael Schmidt and Hod Lipson. 2009. Distilling Free-Form Nat-ural Laws from Experimental Data. Science 324, 5923 (2009), 81â€“ 85. arXiv:https://www.science.org/doi/pdf/10.1126/science.1165893 doi:10.1126/ science.1165893 [38] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Lan-guage Models. arXiv:2402.03300 [cs.CL] https://arxiv.org/abs/2402.03300 [39] Hao Shi, Weili Song, Xinting Zhang, Jiahe Shi, Cuicui Luo, Xiang Ao, Hamid Arian, and Luis Seco. 2024. AlphaForge: A Framework to Mine and Dynamically Combine Formulaic Alpha Factors. arXiv:2406.18394 [q-fin.CP] https://arxiv. org/abs/2406.18394 [40] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2025. LLM-SR: Scientific Equation Discovery via Program-ming with Large Language Models. arXiv:2404.18400 [cs.LG] https://arxiv.org/ abs/2404.18400 [41] Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Fari-mani, Khoa D Doan, and Chandan K Reddy. 2025. LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models. arXiv:2504.10415 [cs.CL] https://arxiv.org/abs/2504.10415 [42] Hongwei Tang, Yan Wang, Tongguang Wang, and Linlin Tian. 2023. Discovering explicit Reynolds-averaged turbulence closures for turbulent separated flows through deep learning-based symbolic regression with non-linear corrections. 

Physics of Fluids 35, 2 (2023), 025118. arXiv:https://doi.org/10.1063/5.0135638 doi:10.1063/5.0135638 [43] Hongwei Tang, Yan Wang, Tongguang Wang, and Linlin Tian. 2023. Discovering explicit Reynolds-averaged turbulence closures for turbulent separated flows through deep learning-based symbolic regression with non-linear corrections. 

Physics of Fluids 35, 2 (Feb. 2023). doi:10.1063/5.0135638 [44] Henk Tennekes and John L. Lumley. 1972. A First Course in Turbulence . The MIT Press. doi:10.7551/mitpress/3014.001.0001 [45] Mojtaba Valipour, Bowen You, Maysum Panju, and Ali Ghodsi. 2021. SymbolicGPT: A Generative Transformer Model for Symbolic Regression. arXiv:2106.14131 [cs.LG] https://arxiv.org/abs/2106.14131 [46] Martin Vastl, JonÃ¡Å¡ KulhÃ¡nek, JiÅ™Ã­ KubalÃ­k, Erik Derner, and Robert BabuÅ¡ka. 2024. SymFormer: End-to-End Symbolic Regression Using Transformer-Based Architecture. IEEE Access 12 (2024), 37840â€“37849. doi:10.1109/ACCESS.2024. 3374649 [47] Marco Virgolin and Solon P. Pissis. 2022. Symbolic Regression is NP-hard. arXiv:2207.01018 [cs.NE] https://arxiv.org/abs/2207.01018 [48] Ylva Wahlquist, Jesper Sundell, and Kristian Soltesz. 2024. Learning pharmaco-metric covariate model structures with symbolic regression networks. Journal of Pharmacokinetics and Pharmacodynamics 51, 2 (2024), 155â€“167. doi:10.1007/ s10928-023-09887-3 [49] Wei WANG, Yang ZHANG, and Lili CHEN. 2019. An application of the shear stress transport low-Reynolds-number k-Îµ turbulence model on turbulent flows. 

ACTA AERODYNAMICA SINICA 37, 3 (2019), 419â€“425. doi:10.7638/kqdlxxb-2016.0158 [50] H.G. Weller, Gavin Tabor, Hrvoje Jasak, and Christer Fureby. 1998. A Tenso-rial Approach to Computational Continuum Mechanics Using Object Orientated Techniques. Computers in Physics 12 (11 1998), 620â€“631. doi:10.1063/1.168744 [51] Shijie Xia, Yuhan Sun, and Pengfei Liu. 2025. SR-Scientist: Scientific Equation Discovery With Agentic AI. arXiv:2510.11661 [cs.AI] https://arxiv.org/abs/2510. 11661 [52] Heng Xiao, Jin-Long Wu, Sylvain Laizet, and Lian Duan. 2020. Flows over periodic hills of parameterized geometries: A dataset for data-driven turbu-lence modeling from direct simulations. Computers & Fluids 200 (2020), 104431. doi:10.1016/j.compfluid.2020.104431 [53] Hengzhe Zhang, Qi Chen, Bing XUE, Wolfgang Banzhaf, and Mengjie Zhang. 2025. RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regres-sion. In The Thirteenth International Conference on Learning Representations .https://openreview.net/forum?id=NdHka08uWn 

A Additional Hyperparameters of PiT-PO 

Table 3 lists the additional hyperparameters of PiT-PO, including (i) general-level physical penalties (dimensional homogeneity and differentiability), (ii) domain-specific constraint penalties, (iii) the gated activation threshold for physical constraints, and (iv) theorem-guided redundancy detection and token-level penalization. 

B Support Exclusion Theorem: Theory and Practice B.1 Preliminaries: Empirical Function Space and Orthogonality 

Definition B.1 (Basis functions / dictionary). Let ğ’³ âŠ† â„ ğ‘‘ be the domain, and let {ğœ™ ğ‘— âˆ¶ ğ’³ â†’ â„} ğ‘—âˆˆ ğ’® be a collection of candidate basis (dictionary) functions indexed by ğ’® . For any subset ğ’¦ âŠ† ğ’® , denote the corresponding model subspace by span {ğœ™ ğ‘— âˆ¶ ğ‘— âˆˆ ğ’¦ }. 

Definition B.2 (Target function ğ‘“ âˆ—). Assume the ground-truth target function admits a sparse expansion over the dictionary: 

ğ‘“ âˆ—(ğ‘¥) = âˆ‘ 

ğ‘—âˆˆ ğ’® â€²

ğ‘ ğ‘— ğœ™ ğ‘— (ğ‘¥), 

where ğ’® â€² âŠ† ğ’® is the true support set (indices of nonzero terms) and satisfies |ğ’® â€²| â‰¤ ğ‘€ . Moreover, the true coefficients are bounded as 

ğ´ â‰¤ |ğ‘ ğ‘— | â‰¤ ğµ, âˆ€ğ‘— âˆˆ ğ’® â€².

Definition B.3 (Empirical inner product and empirical norm). Given a finite dataset ğ· = {ğ‘¥ ğ‘› }ğ‘ ğ‘›=1 âŠ‚ ğ’³ , for any two functions ğ‘“ , ğ‘” âˆ¶ 

ğ’³ â†’ â„ , define the empirical inner product by 

âŸ¨ğ‘“ , ğ‘”âŸ© ğ· âˆ¶= 1ğ‘ 

ğ‘ 

âˆ‘

ğ‘›=1 

ğ‘“ (ğ‘¥ ğ‘› ) ğ‘”(ğ‘¥ ğ‘› ), 

and the induced empirical norm by 

â€–ğ‘“ â€– ğ· âˆ¶= âˆšâŸ¨ğ‘“ , ğ‘“ âŸ© ğ· .

The empirical function space 

â„±ğ· âˆ¶= {(ğ‘“ (ğ‘¥ 1), â€¦ , ğ‘“ (ğ‘¥ ğ‘ )) âˆ¶ ğ‘“ âˆ¶ ğ’³ â†’ â„} 

is a finite-dimensional inner-product space under âŸ¨â‹…, â‹…âŸ© ğ· .

Definition B.4 (Empirical Orthogonality). Two functions ğ‘“ , ğ‘” âˆˆ 

â„±ğ· are empirically orthogonal if âŸ¨ğ‘“ , ğ‘”âŸ© ğ· = 0 .

Convention. Unless stated otherwise, throughout this appendix we write âŸ¨â‹…, â‹…âŸ© and â€– â‹… â€– to mean the empirical inner product âŸ¨â‹…, â‹…âŸ© ğ· 

and the empirical norm â€– â‹… â€– ğ· .

B.2 Proof of the Support Exclusion Theorem 

TheoRem B.5 (SuppoRt Exclusion TheoRem). Let ğ’¦ âŠ† ğ’® be a candidate skeleton (active set). Consider the empirical least-squares fit over ğ’¦ :

ğ‘ âˆˆ arg min 

{ğ‘ ğ‘— }ğ‘—âˆˆ ğ’¦ 

â€–ğ‘“ âˆ— âˆ’ âˆ‘ 

ğ‘—âˆˆ ğ’¦ 

ğ‘ ğ‘— ğœ™ ğ‘— â€–

2

.

Define the empirical Gram matrix ğº âˆˆ â„ |ğ’® |Ã—| ğ’® | by 

ğº ğ‘–ğ‘— âˆ¶= âŸ¨ğœ™ ğ‘– , ğœ™ ğ‘— âŸ©, ğ‘–, ğ‘— âˆˆ ğ’® ,LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

Component Symbol Value Notes Fitting log-MSE scale ğ›¼ 1.0 Scaling in ğ‘… fit = âˆ’ğ›¼ log (MSE + ğœ–) .Complexity penalty weight ğœ† len 5e-3 Weight in ğ‘ƒ cplx = ğœ† len â‹… Length (AST ).Dimensional homogeneity penalty weight ğ‘ƒ dim 1.0 Penalty/reward weight for unit inconsistency (general-level constraint). Differentiability penalty weight ğ‘ƒ diff 0.5 Penalty/reward weight for non-smoothness on the data-defined domain (general-level constraint). Domain-specific constraint weight (j-th) ğ‘ƒ (ğ‘—)  

> domain

0.5 Weight for the ğ‘— -th domain prior (e.g., realizability, wall BC, asymptotic scaling, energy consistency). Gating threshold for physics constraints ğ›¿ gate 1e-3 * MSE_{initial} Activate physics penalties only after fitting reaches a suffi-ciently low-error regime. Numerical stability constant in ğœ ğ‘– ğœ– 1e-50 Used in ğœ ğ‘– = |ğ‘ ğ‘– |âˆ‘ğ‘— |ğ‘ ğ‘— |+ğœ– to avoid division by zero. Redundancy threshold ğœŒ 1e-2 A term is considered redundant if ğœ ğ‘– â‰¤ ğœŒ .Token penalty scale ğ‘ 0.5 Scaling coefficient in ğ‘ƒ tok = ğ‘ â‹… max (0, âˆ’ log (|ğ‘ ğ‘– | + ğœ–)) .

Table 3: Hyperparameters for dual-constraint evaluation and token-level signal synthesis. 

and assume ğº ğ‘–ğ‘– > 0 . For any ğ‘– âˆˆ ğ’® , define 

ğ‘‡ ğ‘–ğ‘— âˆ¶= ğº ğ‘—ğ‘– 

ğº ğ‘–ğ‘– 

, ğ‘— âˆˆ ğ’® .

Further define the external-candidate vector 

ğ‘¢ ğ‘– âˆ¶= (|ğ‘‡ ğ‘–â„“ |) â„“âˆˆ ğ’® âˆ–ğ’¦ âˆˆ â„ |ğ’® âˆ–ğ’¦ |,

and let ğ‘ (1) â‰¥ ğ‘ (2) â‰¥ â‹¯ â‰¥ ğ‘ (| ğ’® âˆ– ğ’¦ |) be the entries of ğ‘¢ ğ‘– sorted in non-increasing order. If for some ğ‘– âˆˆ ğ’¦ ,

|ğ‘ ğ‘– | < ğ´ âˆ’ â›âœâœâœââˆ‘

> ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘–

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— |âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> Internal Interference

+ ğµ 

> |ğ’® âˆ–ğ’¦ |

âˆ‘

> ğ‘˜=1

ğ‘ (ğ‘˜) âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> External Interference

ââŸâŸâŸâ ,

then ğ‘– âˆ‰ ğ’® â€².

PRoof. Step 1: Empirical orthogonality (first-order opti-mality of least squares). Let 

ğ‘”(ğ‘¥) âˆ¶= âˆ‘ 

> ğ‘—âˆˆ ğ’¦

ğ‘ ğ‘— ğœ™ ğ‘— (ğ‘¥), ğ‘Ÿ(ğ‘¥) âˆ¶= ğ‘“ âˆ—(ğ‘¥) âˆ’ ğ‘”(ğ‘¥). 

The objective is â€–ğ‘Ÿâ€– 2 = âŸ¨ğ‘Ÿ, ğ‘ŸâŸ© . For any ğ‘– âˆˆ ğ’¦ , the first-order opti-mality condition yields 

0 = ğœ• ğœ•ğ‘ ğ‘– 

â€–ğ‘Ÿâ€– 2 = âˆ’2âŸ¨ğ‘Ÿ, ğœ™ ğ‘– âŸ© âŸ¹ âŸ¨ğ‘Ÿ, ğœ™ ğ‘– âŸ© = 0. 

Hence, the residual ğ‘Ÿ is empirically orthogonal to span {ğœ™ ğ‘– âˆ¶ ğ‘– âˆˆ ğ’¦ }.

Step 2: Set decomposition. Define three disjoint sets 

ğ’¯ âˆ¶= ğ’¦ âˆ© ğ’® â€², â„± âˆ¶= ğ’¦ âˆ– ğ’® â€², â„› âˆ¶= ğ’® â€² âˆ– ğ’¦ .

Then ğ’¦ = ğ’¯ âˆª â„± and ğ’® â€² = ğ’¯ âˆª â„›. Expanding the residual gives 

ğ‘Ÿ = âˆ‘ 

> ğ‘—âˆˆ ğ’¯

(ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— )ğœ™ ğ‘— + âˆ‘ 

> ğ‘—âˆˆ â„›

ğ‘ ğ‘— ğœ™ ğ‘— âˆ’ âˆ‘ 

> ğ‘—âˆˆ â„±

ğ‘ ğ‘— ğœ™ ğ‘— .

Step 3: An exact coefficient identity for ğ‘– âˆˆ ğ’¯ . Fix any ğ‘– âˆˆ 

ğ’¯ âŠ† ğ’¦ . Using âŸ¨ğ‘Ÿ, ğœ™ ğ‘– âŸ© = 0 and writing ğº ğ‘—ğ‘– = âŸ¨ğœ™ ğ‘— , ğœ™ ğ‘– âŸ©, we obtain 

(ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– )ğº ğ‘–ğ‘– + âˆ‘

> ğ‘—âˆˆ ğ’¯ , ğ‘—â‰ ğ‘–

(ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— )ğº ğ‘—ğ‘– + âˆ‘ 

> ğ‘—âˆˆ â„›

ğ‘ ğ‘— ğº ğ‘—ğ‘– âˆ’ âˆ‘ 

> ğ‘—âˆˆ â„±

ğ‘ ğ‘— ğº ğ‘—ğ‘– = 0. 

Dividing by ğº ğ‘–ğ‘– > 0 and using ğ‘‡ ğ‘–ğ‘— = ğº ğ‘—ğ‘– /ğº ğ‘–ğ‘– yields 

ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– = âˆ’ âˆ‘

> ğ‘—âˆˆ ğ’¯ , ğ‘—â‰ ğ‘–

(ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— )ğ‘‡ ğ‘–ğ‘— âˆ’ âˆ‘ 

> ğ‘—âˆˆ â„›

ğ‘ ğ‘— ğ‘‡ ğ‘–ğ‘— + âˆ‘ 

> ğ‘—âˆˆ â„±

ğ‘ ğ‘— ğ‘‡ ğ‘–ğ‘— .

Step 4: Upper bound via internal and external interfer-ence. Taking absolute values and applying the triangle inequality gives 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¤ âˆ‘

> ğ‘—âˆˆ ğ’¯ , ğ‘—â‰ ğ‘–

|ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— | |ğ‘‡ ğ‘–ğ‘— | + âˆ‘ 

> ğ‘—âˆˆ â„›

|ğ‘ ğ‘— | |ğ‘‡ ğ‘–ğ‘— | + âˆ‘ 

> ğ‘—âˆˆ â„±

|ğ‘ ğ‘— | |ğ‘‡ ğ‘–ğ‘— |. 

For ğ‘— âˆˆ ğ’¯ âŠ† ğ’® â€², we have |ğ‘ ğ‘— | â‰¤ ğµ and thus 

|ğ‘ ğ‘— âˆ’ ğ‘ ğ‘— | â‰¤ |ğ‘ ğ‘— | + |ğ‘ ğ‘— | â‰¤ ğµ + |ğ‘ ğ‘— |. 

Therefore, 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¤ âˆ‘

> ğ‘—âˆˆ ğ’¯ , ğ‘—â‰ ğ‘–

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— | + âˆ‘ 

> ğ‘—âˆˆ â„±

|ğ‘ ğ‘— | |ğ‘‡ ğ‘–ğ‘— | + ğµ âˆ‘ 

> ğ‘—âˆˆ â„›

|ğ‘‡ ğ‘–ğ‘— |. 

Since |ğ‘ ğ‘— | â‰¤ ğµ + |ğ‘ ğ‘— | for all ğ‘— âˆˆ â„±, we further have 

âˆ‘

> ğ‘—âˆˆ â„±

|ğ‘ ğ‘— | |ğ‘‡ ğ‘–ğ‘— | â‰¤ âˆ‘ 

> ğ‘—âˆˆ â„±

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— |. 

Combining ğ’¯ âˆ– {ğ‘–} and â„± yields the internal-interference term: 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¤ âˆ‘

> ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘–

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— |âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ 

> Internal Interference

+ğµ âˆ‘ 

> ğ‘—âˆˆ â„›

|ğ‘‡ ğ‘–ğ‘— |. 

For the external term, note that â„› âŠ† ğ’® âˆ– ğ’¦ , and the entries 

{|ğ‘‡ ğ‘–â„“ |} â„“âˆˆ ğ’® âˆ–ğ’¦ sorted in non-increasing order are ğ‘ (1) â‰¥ â‹¯ â‰¥ ğ‘ (| ğ’® âˆ–

ğ’¦ |) . Hence, for any subset â„›,

âˆ‘

> ğ‘—âˆˆ â„›

|ğ‘‡ ğ‘–ğ‘— | â‰¤ 

> |â„›|

âˆ‘

> ğ‘˜=1

ğ‘ (ğ‘˜). 

Assuming |ğ’® â€²| â‰¤ ğ‘€ and that the current support is partially cor-rect, i.e., ğ’¦ âˆ© ğ’® â€² â‰  âˆ… , we have |â„›| = | ğ’® â€² âˆ– ğ’¦ | â‰¤ | ğ’® â€²| âˆ’ 1 â‰¤ ğ‘€ âˆ’ 1 .Define ğ‘š âˆ¶= min (ğ‘€ âˆ’ 1, | ğ’® âˆ– ğ’¦ |) . Since also |â„›| â‰¤ | ğ’® âˆ– ğ’¦ |, it follows that |â„›| â‰¤ ğ‘š , and thus 

âˆ‘

> ğ‘—âˆˆ â„›

|ğ‘‡ ğ‘–ğ‘— | â‰¤ 

> |â„›|

âˆ‘

> ğ‘˜=1

ğ‘ (ğ‘˜) â‰¤ 

> ğ‘š

âˆ‘

> ğ‘˜=1

ğ‘ (ğ‘˜). Thus, 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¤ âˆ‘

ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘– 

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— | + ğµ 

ğ‘š 

âˆ‘

ğ‘˜=1 

ğ‘ (ğ‘˜). 

Step 5: Lower bound when ğ‘– âˆˆ ğ’® â€². If ğ‘– âˆˆ ğ’® â€², then |ğ‘ ğ‘– | â‰¥ ğ´ . By the reverse triangle inequality, 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¥ ||ğ‘ ğ‘– | âˆ’ |ğ‘ ğ‘– || â‰¥ ğ´ âˆ’ |ğ‘ ğ‘– |. 

Step 6: Contradiction. Assume, for contradiction, that there exists ğ‘– âˆˆ ğ’¦ âˆ© ğ’® â€² satisfying 

|ğ‘ ğ‘– | < ğ´ âˆ’ ( âˆ‘

ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘– 

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— | + ğµ 

ğ‘š 

âˆ‘

ğ‘˜=1 

ğ‘ (ğ‘˜)) . 

Rearranging yields 

ğ´ âˆ’ |ğ‘ ğ‘– | > âˆ‘

ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘– 

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— | + ğµ 

ğ‘š 

âˆ‘

ğ‘˜=1 

ğ‘ (ğ‘˜). 

However, Step 5 implies |ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¥ ğ´ âˆ’ |ğ‘ ğ‘– |, while the bound above implies 

|ğ‘ ğ‘– âˆ’ ğ‘ ğ‘– | â‰¤ âˆ‘

ğ‘—âˆˆ ğ’¦ , ğ‘—â‰ ğ‘– 

(ğµ + |ğ‘ ğ‘— |) |ğ‘‡ ğ‘–ğ‘— | + ğµ 

ğ‘š 

âˆ‘

ğ‘˜=1 

ğ‘ (ğ‘˜), 

which is a contradiction. Therefore such an ğ‘– cannot belong to ğ’® â€²,i.e., ğ‘– âˆ‰ ğ’® â€². â–¡

C Supplementary Diagnostics and Analyses C.1 Remaining Iteration Curves on Smaller Backbones 

In the main text, we report iteration curves for the 8B backbone. Here we provide the remaining curves for 3B and 1B (Figure 8), which corroborate the observations in Section 4.4. 

C.2 Time-Cost Analysis under a Fixed Wall-Clock Budget 

To complement the iteration-based efficiency analysis in Section 4.4, we evaluate search efficiency under a strict wall-clock constraint. We fix the total runtime of each method to 25,000 seconds ( â‰ˆ 6.9 hours) and track the best-so-far NMSE as a function of elapsed time (Figure 9). This protocol directly answers a practical question: given the same compute time, which method returns a more accu-rate recovered equation? Across all three backbones (Llama-3.1-8B, Llama-3.2-3B, and Llama-3.2-1B), PiT-PO consistently yields lower NMSE trajectories than LLM-SR under the same 25,000-second budget. In line with the be-havior discussed in Section 4.4, the curves exhibit a characteristic divergence in the low-error regime: LLM-SR often plateaus after an initial reduction, while PiT-PO continues to realize step-wise decreases over time, indicating more effective late-stage credit as-signment and continued progress rather than stagnation. The advantage is most pronounced on the two oscillator sys-tems. For 8B and 3B, PiT-PO achieves rapid early improvements and maintains additional phase transitions later in the run, reach-ing substantially lower NMSE within the same wall-clock budget. For the 1B model, both methods converge more slowly overall, yet PiT-PO still more reliably escapes stagnation and attains markedly 0 625 1250 1875 2500                           

> Iteration
> 10 17
> 10 13
> 10 9
> 10 5
> 10 1
> NMSE (log scale)
> Oscillation 1
> 0625 1250 1875 2500
> Iteration
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> NMSE (log scale)
> Oscillation 2
> 0625 1250 1875 2500
> Iteration
> 10 2
> 10 1
> 10 0
> NMSE (log scale)
> E. coli Growth
> 0625 1250 1875 2500
> Iteration
> 10 2
> 10 1
> NMSE (log scale)
> Stress-Strain
> LLM-SR PiT-PO

(a) Llama-3B 0 625 1250 1875 2500                        

> Iteration
> 10 5
> 10 3
> 10 1
> NMSE (log scale)
> Oscillation 1
> 0625 1250 1875 2500
> Iteration
> 10 8
> 10 6
> 10 4
> 10 2
> 10 0
> NMSE (log scale)
> Oscillation 2
> 0625 1250 1875 2500
> Iteration
> 10 1
> 10 0
> NMSE (log scale)
> E. coli Growth
> 0625 1250 1875 2500
> Iteration
> 10 2
> 10 1
> NMSE (log scale)
> Stress-Strain
> LLM-SR PiT-PO

(b) Llama-1B 

Figure 8: NMSE versus iteration on smaller backbones, con-sistent with the trends in the 8B setting. 

lower final NMSE, suggesting that the proposed in-search tuning remains effective even in the small-model regime. Similar trends hold for the E. coli Growth and Stressâ€“Strain tasks, where progress is typically more gradual. Under equal wall-clock time, PiT-PO achieves a lower final NMSE and exhibits clearer late-stage improvements, whereas LLM-SR tends to flatten earlier. Importantly, these gains are obtained without increasing runtime: despite the additional overhead introduced by in-search policy op-timization, PiT-PO delivers consistently better return-on-compute within a fixed time budget, validating the practical efficiency of the proposed approach. 

C.3 OOD Evaluation (NMSE OOD )

We report out-of-distribution performance using NMSE on the OOD split (NMSE OOD ), summarized in Table 4. Across all three back-bones (8B/3B/1B) and all four tasks, PiT-PO achieves lower NMSE OOD 

than LLM-SR under the same setting. In particular, PiT-PO main-tains extremely small OOD errors on the two oscillator systems, and yields substantially reduced OOD NMSE on E. coli growth and Stressâ€“Strain, where LLM-SR exhibits noticeably larger errors. LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 2 4 6      

> Time (hours)
> 10 26
> 10 21
> 10 16
> 10 11
> 10 6
> 10 1
> NMSE (log scale)

Oscillation 1         

> 246
> Time (hours)
> 10 12
> 10 10
> 10 8
> 10 6
> 10 4
> 10 2
> NMSE (log scale)

Oscillation 2     

> 246
> Time (hours)
> 10 2
> 10 1
> 10 0
> NMSE (log scale)

E. coli Growth     

> 246
> Time (hours)
> 10 2
> 10 1
> NMSE (log scale)

Stress-Strain  

> LLM-SR PiT-PO

(a) Llama-3.1-8B 2 4 6      

> Time (hours)
> 10 18
> 10 15
> 10 12
> 10 9
> 10 6
> 10 3
> NMSE (log scale)

Oscillation 1       

> 246
> Time (hours)
> 10 8
> 10 6
> 10 4
> 10 2
> NMSE (log scale)

Oscillation 2     

> 246
> Time (hours)
> 10 2
> 10 1
> 10 0
> NMSE (log scale)

E. coli Growth     

> 246
> Time (hours)
> 10 2
> 10 1
> NMSE (log scale)

Stress-Strain  

> LLM-SR PiT-PO

(b) Llama-3.2-3B 2 4 6      

> Time (hours)
> 10 6
> 10 5
> 10 4
> 10 3
> 10 2
> 10 1
> NMSE (log scale)

Oscillation 1       

> 246
> Time (hours)
> 10 8
> 10 6
> 10 4
> 10 2
> NMSE (log scale)

Oscillation 2    

> 246
> Time (hours)
> 10 1
> 10 0
> NMSE (log scale)

E. coli Growth     

> 246
> Time (hours)
> 10 2
> 10 1
> NMSE (log scale)

Stress-Strain  

> LLM-SR PiT-PO

(c) Llama-3.2-1B 

Figure 9: Wall-clock efficiency under a fixed 25,000-second budget. 

These results provide consistent evidence that the improvements observed in the main text persist under OOD evaluation. 

C.4 Equation Iterative Trajectories 

Figure 10 visualizes how symbolic structures evolve along the it-erative search. For PiT-PO (Figure 10a), the trajectory reveals a clear process of progressively excluding spurious terms. Under the theorem-guided mathematical constraints and the token-aware pol-icy update, tokens marked as redundant are assigned token-level penalties; as this penalty signal repeatedly accumulates across many 

Method Oscillator1 Oscillator2 E. coli growth Stressâ€“Strain 

NMSE OOD â†“ NMSE OOD â†“ NMSE OOD â†“ NMSE OOD â†“

LLM-SR (Llama-3.1-8B) 0.0114 8.05e-10 0.1676 0.1026 LLM-SR (Llama-3.2-3B) 0.0191 5.56e-5 809.94 0.0264 LLM-SR (Llama-3.2-1B) 0.1182 7.57e-3 1888.5 0.2689 PiT-PO (Llama-3.1-8B) 1.63e-30 1.36e-11 0.1163 0.0163 PiT-PO (Llama-3.2-3B) 2.63e-30 8.52e-10 0.0237 0.0144 PiT-PO (Llama-3.2-1B) 5.99e-5 8.58e-10 0.3462 0.0124 

Table 4: OOD evaluation: NMSE on the OOD split. 0 500 1000 1500 2000 2500 

Iterations                                                        

> 10 28
> 10 24
> 10 20
> 10 16
> 10 12
> 10 8
> 10 4
> 10 0
> NMSE (log scale)
> p1x+p2v+p3
> +p3sin( x)+p6v3p1v+p2x+p4cos( x)
> +p5v2+p7x2+p8v4+p9xsin( v)+p10
> +p3sin( x)+p5v3+p6xv+p8x3p1x
> +p2v+p4cos( v)+p7sin( v)
> p1x3+p2xv+p3v3+p4sin( x)+p5xcos( x)

(a) PiT-PO. 0 500 1000 1500 2000 2500 

Iterations                                                                     

> 10 28
> 10 24
> 10 20
> 10 16
> 10 12
> 10 8
> 10 4
> 10 0
> NMSE (log scale)
> LLMSR
> PiT-PO
> (p3v3)/ p4(p1x)/ p4+( p2v)/ p4
> (p7)x3+( p8)v3+( p9)xv
> (p1)x+( (p2)abs (v)v)+(( p3)sin(( p4)np .arange (len (x))))
> +( p5)x2+( p6)v2
> p5xv+p8x3+p9v3xv
> +p4x2+p6v2+p7xv2+p10
> +p3sin(2 pi p4x)+p5xv+p6v3
> p1vp2xp7cos( p9x+p8)

(b) LLM-SR. 

Figure 10: Iterative trajectories of PiT-PO and LLM-SR on Os-cillator1 with Llama-3.1-8B-Instruct. The NMSE curves (log scale) report the best-so-far value over iterations. Annotated equation snapshots illustrate how the discovered structure evolves along the search trajectory. The green-shaded terms are correct, whereas the red-shaded terms are erroneous. 

sampled equations during optimization, such spurious components become progressively less preferred and are unlikely to reappear in later generations. As a result, the search does not merely fit nu-merically; instead, it repeatedly transitions to cleaner structures, and each step-wise NMSE drop aligns with the removal of nui-sance terms and a move toward a more parsimonious functional form. This â€œerror-term eliminationâ€ behavior effectively shrinks the search space and substantially strengthens the exploration ca-pability. In contrast, LLM-SR (Figure 10b) exhibits much weaker struc-tural correction. Since its guidance is primarily prompt-level rather than parameter-level, terms that appear earlyâ€”even if structurally incorrectâ€”tend to persist and continue influencing subsequent pro-posals. Consequently, the search frequently enters a plateau where NMSE stops improving meaningfully: the method keeps revisiting or retaining earlier spurious components instead of systematically pruning them, indicating a stagnation regime with limited ability to reach the correct structure. 

D Datasets D.1 LLM-SR Suite 

We use the LLM-SR Suite, which consists of four standard scien-tific equation discovery tasks spanning physics, biology, and ma-terials science. The suite includes two nonlinear oscillator systems (Oscillation 1/2), an E. coli growth model (E. coli Growth), and a temperature-dependent stressâ€“strain dataset (Stressâ€“Strain). The first three tasks are dynamical systems and are simulated over a fixed time horizon, while the last task is a static constitutive rela-tion evaluated on experimental measurements. 

D.1.1 Oscillation 1 (Nonlinear Oscillator). Nonlinear oscillators with dissipative and non-polynomial couplings provide a demanding test for recovering interacting terms from trajectories. We define the state as ğ‘¥(ğ‘¡) and ğ‘£(ğ‘¡) =Ì‡ ğ‘¥(ğ‘¡) , and simulate the following sys-tem: Ì‡ğ‘¥ = ğ‘£,Ì‡ ğ‘£ = ğ¹ sin (ğœ”ğ‘¥) âˆ’ ğ›¼ğ‘£ 3 âˆ’ ğ›½ğ‘¥ 3 âˆ’ ğ›¾ ğ‘¥ğ‘£ âˆ’ ğ‘¥ cos (ğ‘¥). 

We set ğ¹ = 0.8 , ğ›¼ = 0.5 , ğ›½ = 0.2 , ğ›¾ = 0.5 , and ğœ” = 1.0 , with initial conditions ğ‘¥(0) = 0.5 , ğ‘£(0) = 0.5 , and simulate over ğ‘¡ âˆˆ [0, 50] .

D.1.2 Oscillation 2 (Nonlinear Oscillator). The second oscillator introduces explicit time forcing and an exponential nonlinearity, leading to a different coupling pattern between ğ‘¥ and ğ‘£ :Ì‡ğ‘¥ = ğ‘£,Ì‡ ğ‘£ = ğ¹ sin (ğœ”ğ‘¡) âˆ’ ğ›¼ğ‘£ 3 âˆ’ ğ›½ğ‘¥ğ‘£ âˆ’ ğ›¿ğ‘¥ exp (ğ›¾ ğ‘¥). 

We use ğ¹ = 0.3 , ğ›¼ = 0.5 , ğ›½ = 1.0 , ğ›¿ = 5.0 , ğ›¾ = 0.5 , and ğœ” = 1.0 . The initial conditions and simulation window are the same as Oscillation 1: ğ‘¥(0) = 0.5 , ğ‘£(0) = 0.5 , ğ‘¡ âˆˆ [0, 50] .

D.1.3 E. coli Growth (Bacterial Growth). This task models the growth rate of E. coli as a product of multiplicative factors capturing dis-tinct physiological effects from population size, nutrient availabil-ity, temperature, and acidity: 

ğ‘‘ğµ ğ‘‘ğ‘¡ = ğ‘“ ğµ (ğµ) â‹… ğ‘“ ğ‘† (ğ‘†) â‹… ğ‘“ ğ‘‡ (ğ‘‡ ) â‹… ğ‘“ pH (pH ), 

where ğµ is bacterial density, ğ‘† is nutrient concentration, ğ‘‡ is tem-perature, and pH denotes acidity. In our benchmark, the explicit functional form is:               

> ğ‘‘ğµ ğ‘‘ğ‘¡ = ğœ‡ max ğµ ( ğ‘† ğ¾ ğ‘† +ğ‘† ) ( tanh ğ‘˜(ğ‘‡ âˆ’ğ‘¥ 0)1+ğ‘(ğ‘‡ âˆ’ğ‘¥ decay )4)exp (âˆ’| pH âˆ’pH opt |) sin ((pH âˆ’pH min )ğœ‹
> pH max âˆ’pH min
> )
> 2
> .

This specification combines saturation kinetics, sharp temperature modulation, and a structured pH response, yielding a challenging multivariate nonlinear discovery setting. 

D.1.4 Stressâ€“Strain (Material Stress Behavior). To assess perfor-mance on experimental measurements, we consider tensile stressâ€“ strain data for Aluminum 6061-T651 collected at multiple tempera-tures (from room temperature up to 300 âˆ˜C). While no single exact governing equation is provided, a commonly used phenomenolog-ical approximation for temperature-dependent hardening is: 

ğœ = (ğ´ + ğµğœ€ ğ‘› ) (1 âˆ’ ( ğ‘‡ âˆ’ ğ‘‡ ğ‘Ÿ 

ğ‘‡ ğ‘š âˆ’ ğ‘‡ ğ‘Ÿ 

)

> ğ‘š

) . 

where ğœ is stress, ğœ€ is strain, ğ‘‡ is temperature, ğ‘‡ ğ‘Ÿ is a reference temperature, and ğ‘‡ ğ‘š is the melting temperature. The coefficients 

ğ´ , ğµ , ğ‘› , and ğ‘š are fitted from data for the given alloy. 

D.2 LLM-SRBench 

We additionally report results on LLM-SRBench, a benchmark specif-ically constructed to evaluate data-driven scientific equation dis-covery with LLMs while reducing the risk of trivial memorization of canonical textbook formulas. It contains 239 problems spanning four scientific domains (chemistry, biology, physics, and material science), and each task is packaged with (i) a short scientific con-text describing variables and the target quantity and (ii) tabular numerical samples used for discovery. 

Dataset composition. LLM-SRBench is organized into two com-plementary subsets. 

(1) LSR-Transform. This subset converts well-known physics equations into less-common but analytically equivalent forms by changing the prediction target. Concretely, starting from the origi-nal equation, one input variable is chosen as a pivot and promoted to the new target; the equation is then symbolically solved (e.g., via SymPy ) for that pivot variable, yielding an alternative represen-tation of the same underlying physical law. Only transformations that admit an analytic solution are retained, and the original dat-apoints are filtered to satisfy the valid domain of the transformed expression (e.g., avoiding invalid denominators or square-root do-mains). Finally, a new natural-language problem statement is gen-erated to match the transformed inputâ€“output specification. This pipeline produces 111 transformed problems. 

(2) LSR-Synth. This subset is designed to test discovery beyond memorized templates by composing equations from both known 

scientific terms and synthetic (novel yet plausible) terms. Candi-date known/synthetic terms are proposed with an LLM given the domain context, combined into full equations, and then filtered through multiple checks: numerical solvability (e.g., via standard ODE solvers for dynamical systems), novelty assessment in con-text (using an LLM-based novelty evaluator), and expert plausibil-ity validation. After passing these filters, datapoints are generated for each approved equation, yielding 128 synthetic problems across the same four domains. 

Symbolic Accuracy (SA). Besides numeric fit metrics (e.g., NMSE and Acc ğ‘ğ‘™ğ‘™ (ğœ ) ), LLM-SRBench emphasizes symbolic correctness . Sym-bolic Accuracy (SA) is computed using an LLM-based equivalence judge (GPT-4o): given a predicted hypothesis and the ground-truth expression, the evaluator first normalizes both by removing extra-neous text (e.g., comments) and replacing explicit numeric con-stants with placeholder parameters, then decides whether there exist parameter values that make the hypothesis mathematically LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

equivalent to the ground truth. SA is the fraction (percentage) of problems judged equivalent under this protocol. The benchmark authors validated this evaluation by comparing against human judg-ments on 130 sampled problems and reported 94.6% agreement be-tween GPT-4o and human evaluators. 

E Turbulence Task Setup: Periodic Hill Flow E.1 Dataset 

We consider the periodic hill flow configuration with streamwise periodic boundary conditions and no-slip walls. In this work, the features selected for the symbolic regression process are computed from the mean strain-rate ( ğ‘† ) and rotation-rate ( ğ‘… ) tensors. Following established practice, these tensors are normalized using local turbulence scales prior to invariant calcula-tion to ensure well-conditioned inputs: 

ğ‘† = 121ğ›½ âˆ—ğœ” [âˆ‡ğ‘¢ + (âˆ‡ğ‘¢) T], (13) 

ğ‘… = 121ğ›½ âˆ—ğœ” [âˆ‡ğ‘¢ âˆ’ (âˆ‡ğ‘¢) T], (14) where (â‹…) T denotes matrix transposition. The invariants are then computed as ğ¼ 1 = Tr (ğ‘† 2) and ğ¼ 2 = Tr (ğ‘… 2).The dataset provides (i) two invariant features (ğ¼ 1, ğ¼ 2) (stored in the Lambda columns), (ii) tensor bases {ğ‘‡ ğ‘š }3ğ‘š=1 (stored in the first three 3 Ã— 3 tensors in the Tensor columns), and (iii) the target anisotropy tensor field ğ‘ (and its nonlinear correction form used for training, denoted by ğ‘ âŸ‚), together with auxiliary fields such as turbulent kinetic energy ğ‘˜ when needed by the evaluator. 

E.1.1 Data Collection. 

DNS source. The DNS data are obtained from the publicly avail-able NASA turbulence modeling resource ( Turbulence Modeling Re-source / TMBWG ) for periodic hills of parameterized geometries. 1

RANS baseline generation (for non-DNS methods). For all non-DNS methods, we compute baseline solutions using OpenFOAM with the same initial and boundary conditions as the DNS. Specifi-cally, we employ the incompressible steady-state solver simple-Foam and apply the turbulence models considered in this paper (all belonging to RANS and its SST variants). The solver outputs, at each sampling point and converged iteration/time-step, the re-quired physical quantities for subsequent post-processing. 

DNSâ€“RANS grid alignment and target construction. To construct training targets consistent with the RANS discretization, we inter-polate the DNS fields onto the RANS grid points. The interpolated DNS quantities are then post-processed in the CFD workflow (i.e., as offline preprocessing prior to training) to compute the Reynolds stress tensor, which is packaged into the dataset files as the super-vision signal for training. 

Feature & basis construction. Following Popeâ€™s tensor-basis ex-pansion hypothesis, the Reynolds stress decomposition involves (i) tensor bases and (ii) scalar coefficient functions of invariant fea-tures. Since our final turbulence closure is embedded as a correc-tion to the baseline kOmegaSST model, we use the physical fields 

> 1https://tmbwg.github.io/turbmodels/Other_DNS_Data/parameterized_periodic_ hills.html

produced by the baseline kOmegaSST computation for post-processing to obtain the input features and the tensor bases. These are stored in separate files (features and bases), while the learning target is defined to match the Reynolds-stress-related quantity described in Section E.2. 

E.1.2 Pre-processing. We normalize the two invariant inputs prior to feeding them into the learned symbolic expressions: Ìƒğ¼ = tanh ( ğ¼ 2.0 ) ,Ìƒ ğ¼ 1 = tanh ( ğ¼ 1

2.0 ) ,Ìƒ ğ¼ 2 = tanh ( ğ¼ 2

2.0 ) , (15) which maps the raw values into approximately [âˆ’1, 1] and im-proves numerical stability during parameter fitting. 

E.2 Casting Turbulence Closure as a Symbolic Regression Problem 

Predicting scalar coefficients instead of tensor entries. Rather than directly searching over tensor-valued symbolic forms, we let the LLM predict three scalar coefficient functions (ğº 1, ğº 2, ğº 3), each parameterized as a symbolic expression of only two variables (ğ¼ 1, ğ¼ 2).The evaluator then reconstructs the predicted tensor via the (fixed) tensor bases: Ì‚ğ‘ = ğº 1(ğ¼ 1, ğ¼ 2) ğ‘‡ 1 + ğº 2(ğ¼ 1, ğ¼ 2) ğ‘‡ 2 + ğº 3(ğ¼ 1, ğ¼ 2) ğ‘‡ 3. (16) This design compresses the LLM input variable dimension to two, significantly reducing the symbolic search space. 

Scoring by MSE in the evaluator. Given a candidate hypothesis, we compute the fitting score by the mean squared error between the target tensor and the reconstructed tensor. In the final evalua-tor, we additionally scale both prediction and target by the sample-wise ğ‘˜ before error aggregation: Ì‚ğœ ğ‘–ğ‘— (ğ‘¥ ğ‘› ) = ğ‘˜(ğ‘¥ ğ‘› )Ì‚ ğ‘ ğ‘–ğ‘— (ğ‘¥ ğ‘› ), ğœ ğ‘–ğ‘— (ğ‘¥ ğ‘› ) = ğ‘˜(ğ‘¥ ğ‘› ) ğ‘ ğ‘–ğ‘— (ğ‘¥ ğ‘› ), (17) We compute the MSE only over a selected subset of tensor com-ponents. This is because the Reynolds-stress (and anisotropy) ten-sor is symmetric, so only six components are independent, and in the present periodic hill flow setting two off-plane shear compo-nents are typically several orders of magnitude smaller and are commonly neglected in practical turbulence modeling. Following this standard practice, we evaluate the error only on 

Î© = {(0, 0), (0, 1), (1, 1), (2, 2)}, (18) MSE = 1|Î©| âˆ‘

> (ğ‘–,ğ‘—)âˆˆÎ©

1ğ‘ 

> ğ‘

âˆ‘

> ğ‘›=1

(ğœ ğ‘–ğ‘— (ğ‘¥ ğ‘› ) âˆ’Ì‚ ğœ ğ‘–ğ‘— (ğ‘¥ ğ‘› )) 2

. (19) The resulting MSE is used as the numerical fitting score for the symbolic regression loop. 

Prompt and fairness across methods. We provide the same prob-lem specification text (task description, variable definitions, and operator constraints) to both LLM-SR and PiT-PO to ensure a fair comparison. An example prompt is shown in Figure 11â€“13. In our current experiments, we also explicitly restrict the hypothesis space by asking the LLM to avoid trigonometric functions. """ 

Task: Symbolic regression for periodic hill turbulence modeling. 

Learn three scalar functions G1(I1, I2), G2(I1, I2), G3(I1, I2). 

[Flow case context (for description only)] 

- Bottom wall profile: 

y(x) = a[1 + cos(Ï€ x / L)] for |x| â‰¤ L 

y(x) = 0 for |x| > L 

where a is the hill height (characteristic length h), and Î± = 

L/h controls hill steepness (training case: Î± = 0.8). 

- Streamwise (x) direction uses periodic boundary conditions; top 

and bottom walls use no -slip boundary conditions. 

- Reynolds number: Re_h = U_b h / Î½ = 5600. 

[Evaluation rule] 

Given tensor bases T1, T2, T3 (each 3x3), the predicted anisotropy 

is: 

b_hat = G1 * T1 + G2 * T2 + G3 * T3 

The evaluator computes MSE between b_hat and the target b. 

[Tensor bases: T1, T2, T3 (provided by the evaluator, do NOT 

compute them)] 

For each sample n, the evaluator provides three 3x3 tensor bases 

(symmetric, traceless) constructed from the 

non -dimensionalized mean strain -rate tensor S and rotation tensor 

R: 

- T1 (linear strain basis): 

T1 = S 

Interpretation: the Boussinesq /linear eddy -viscosity direction. 

It dominates in simple shear flows and acts 

as the baseline anisotropy response aligned with the mean strain. 

- T2 (strain â€“rotation coupling basis): 

T2 = S @ R - R @ S 

Interpretation: captures the interaction between mean strain and 

mean rotation (curvature/turning, strong vortices). 

This term is crucial in separated flows, reattachment, and 

swirling/shear -layer regions where linear eddy -viscosity 

assumptions tend to fail. 

- T3 (quadratic strain nonlinearity basis): 

T3 = S @ S - (1/3) * tr(S @ S) * I 

Interpretation: introduces nonlinear strain effects and normal -

stress anisotropy beyond linear models. It helps represent 

differences among normal stress components and improves 

predictions in strongly anisotropic regions. 

The predicted anisotropy is assembled as: 

b_hat = G1(I1, I2) * T1 + G2(I1, I2) * T2 + G3(I1, I2) * T3 

""" Figure 11: Problem Specification. 

E.3 Back to Turbulence Modeling 

From discovered expressions to a deployable closure. After obtain-ing an explicit expression for the Reynolds-stress-related term from symbolic regression, we rewrite the discovered formula into an OpenFOAM-readable form. 

Embedding into OpenFOAM. We embed the resulting closure into the existing kOmegaSST implementation by replacing the routine that evaluates the Reynolds stress (or its modeled contribution) with the discovered expression, thereby forming an optimized tur-bulence model within the RANS framework. 

Verification against DNS. We then run RANS simulations with the modified model under the same configuration as the baseline and compare the resulting flow statistics against the DNS refer-ence data. This completes the end-to-end turbulence modeling and validation pipeline considered in this work. def evaluate (data, equation): 

I1, I2 = data['inputs'] 

T1, T2, T3 = data['tensors'] 

b_true = data['outputs'] 

# Define Loss Function 

def loss(params): 

# Predict scalar coefficients G = [G1, G2, G3] 

# equation returns shape (N, 3) based on params 

G_pred = equation(I1, I2, params) 

# Reconstruct predicted anisotropy tensor: 

# b_hat = G1*T1 + G2*T2 + G3*T3 

b_hat = ( G_pred [:, 0, None, None] * T1 + 

G_pred [:, 1, None, None] * T2 + 

G_pred [:, 2, None, None] * T3) 

# Compute MSE against target 

return np.mean (( b_hat - b_true ) ** 2) 

# Initialize parameters (e.g., 30 params for the 3 scalar 

functions) 

initial_params = np.ones (30) 

result = minimize(loss, initial_params , method='BFGS') 

return result.fun 

Figure 12: Evaluation and Optimization. def equation (I1: np.ndarray , I2: np.ndarray , params: np.ndarray )

-> np.ndarray :

""" 

Predict three scalar coefficients G1, G2, G3 for tensor bases. 

Args :

I1, I2: numpy arrays of shape (N,), invariants. 

params: numpy array of free constants. 

Returns: 

numpy array of shape (N, 3), columns are G1, G2, G3. 

""" 

# G1 block: params[0:10] 

g1 = ( 

params[0] * I1 + 

params[1] * I2 + 

params[2] 

)

# G2 block: params[10:20] 

g2 = ( 

params[10] * I1 + 

params[11] * I2 + 

params[12] 

)

# G3 block: params[20:30] 

g3 = ( 

params[20] * I1 + 

params[21] * I2 + 

params[22] 

)

return np.stack ([g1, g2, g3], axis=1) 

Figure 13: Equation Program Example. 

E.4 Domain-Specific Constraints for Turbulence Modeling 

To encode expert knowledge as inductive biases, we augment the reward with a domain-specific penalty term: 

ğ‘ƒ domain =

4

âˆ‘

ğ‘—=1 

ğ‘¤ ğ‘— ğ‘ƒ (ğ‘—) 

domain , (20) LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization 

where each ğ‘ƒ (ğ‘—)  

> domain

â‰¥ 0 quantifies violations of the ğ‘— -th turbulence constraint. 

(1) Realizability. A physically admissible Reynolds stress tensor must be symmetric and positive semi-definite (PSD), which implies that all eigenvalues are nonnegative. Non-realizable predictions are penalized by 

ğ‘ƒ (1)  

> domain

= 1ğ‘ 

> ğ‘

âˆ‘

> ğ‘›=1

max (0, âˆ’ğœ† min (ğœ (ğ‘¥ ğ‘› ))), (21) where ğœ† min (â‹…) denotes the smallest eigenvalue. This PSD require-ment is a standard realizability condition for Reynolds stresses [32]. 

(2) Boundary-condition consistency. At a no-slip wall, the Reynolds stress tensor must decay to zero. Accordingly, non-vanishing stresses in a near-wall band ğ’² = {ğ‘¥ âˆ¶ ğ‘¦(ğ‘¥) < ğ‘¦ 0} are penalized as 

ğ‘ƒ (2)  

> domain

= 1|ğ’² | âˆ‘ 

> ğ‘¥ ğ‘› âˆˆğ’²

â€–ğœ (ğ‘¥ ğ‘› )â€– ğ¹ , (22) where ğ‘¦(ğ‘¥) is the wall distance field. In practice, ğ‘¦(ğ‘¥) can be com-puted and exported using standard CFD post-processing tools (Open-FOAM wall-distance utilities such as wallDist and checkMesh-writeAllFields ) [27]. 

(3) Asymptotic scaling in the viscous sublayer. Near-wall Taylor expansions under the no-slip constraint imply that ğ‘¢ â€² = ğ‘‚(ğ‘¦) and, for incompressible flow, ğ‘£ â€² = ğ‘‚(ğ‘¦ 2). Consequently, the Reynolds shear stress âˆ’ğ‘¢ â€²ğ‘£ â€² exhibits cubic leading-order scaling, ğ‘‚(ğ‘¦ 3), in the viscous sublayer [44]. This constraint is enforced by evaluat-ing the slope of log |ğœ +ğ‘¥ğ‘¦ | with respect to log (ğ‘¦ +) over the viscous sublayer range ğ’± = {ğ‘¥ âˆ¶ ğ‘¦ +(ğ‘¥) âˆˆ [ğ‘¦ +1 , ğ‘¦ +2 ]} :

ğ‘ = slope (log |ğœ +ğ‘¥ğ‘¦ | vs log (ğ‘¦ +)), ğ‘ƒ (3)  

> domain

= |ğ‘ âˆ’ 3|. (23) Here ğ‘¦ + = ğ‘¦ ğ‘¢ ğœ /ğœˆ is the wall unit, with friction velocity ğ‘¢ ğœ =âˆšğœ ğ‘¤ /ğœŒ . In CFD, ğ‘¦ + and ğœ ğ‘¤ can be obtained through standard post-processing utilities (OpenFOAM yPlus and wallShearStress ). 

(4) Energy consistency. Energy consistency is enforced by con-straining the turbulent kinetic energy production implied by the predicted stress: 

ğ‘ƒ ğ‘˜ (ğ‘¥) = âˆ’ğœ ğ‘–ğ‘— (ğ‘¥) ğœ•ğ‘ˆ ğ‘– 

ğœ•ğ‘¥ ğ‘— 

(ğ‘¥), (24) and penalizing mismatches with the reference production ğ‘ƒ ref  

> ğ‘˜

(com-puted from DNS-aligned fields) via 

ğ‘ƒ (4)  

> domain

= MSE (ğ‘ƒ ğ‘˜ , ğ‘ƒ ref  

> ğ‘˜

). (25) The definition in Eq. (24) is the standard production term in the TKE budget [32]. 

Efficient evaluation via elite-CFD refinement (multi-fidelity con-straint scheduling). Constraints (2)â€“(4) require additional CFD-derived fields (wall distance ğ‘¦ , wall shear stress ğœ ğ‘¤ and the associated ğ‘¦ +,and mean velocity gradients âˆ‡ğ‘ˆ ), which can be expensive to obtain at each candidate-evaluation step. To keep the symbolic regression loop efficient, we adopt a two-stage evaluation schedule. 

Stage-A (inexpensive screening, applied to all candidates) 

evaluates the data-fitting score (MSE) together with realizability 

ğ‘ƒ (1)  

> domain

using only the predicted stress tensor. 

Stage-B (expensive physics checks, triggered by an improv-ing best MSE) is activated when a candidate in the current batch achieves a new best MSE value relative to all previously evaluated candidates. In this case, the candidate is treated as an elite solu-tion, and CFD-based post-processing is performed to obtain the re-quired auxiliary fields, compute ğ‘ƒ (2) 

> domain

â€“ğ‘ƒ (4) 

> domain

, and conduct an additional online fine-tuning step using the full domain-specific penalty. All non-elite candidates are trained only with the realiz-ability constraint, whereas elite candidates receive the full turbu-lence constraint suite after CFD evaluation. This design enables rigorous physics enforcement without incurring prohibitive CFD costs during exploration.