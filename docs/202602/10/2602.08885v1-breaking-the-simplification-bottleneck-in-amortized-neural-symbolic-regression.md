---
title: Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression
title_zh: 突破摊销神经符号回归中的简化瓶颈
authors: "Paul Saegert, Ullrich Köthe"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08885v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 摊销符号回归的算法改进
tldr: 符号回归旨在发现可解释的解析表达式。摊销神经符号回归虽高效但难以处理复杂科学问题，主要瓶颈在于缺乏快速的表达式简化与归一化工具。本文提出 SimpliPy，一个比 SymPy 快 100 倍的规则化简化引擎。基于此构建的 Flash-ANSR 框架在保持高精度的同时，显著提升了训练和推理效率，生成的表达式比现有方法更简洁且更具竞争力。
motivation: 现有的摊销符号回归方法受限于通用计算机代数系统极慢的简化速度，导致难以扩展到复杂的科学计算场景。
method: 提出了一种名为 SimpliPy 的快速规则化简化引擎，并基于此构建了 Flash-ANSR 框架，实现了高效的表达式归一化和训练集去重。
result: SimpliPy 实现了比 SymPy 快 100 倍的简化速度，使 Flash-ANSR 在精度上优于同类摊销方法，并能生成比 PySR 更简洁的表达式。
conclusion: 通过突破表达式简化的计算瓶颈，摊销符号回归可以在保证推理速度的同时，实现与最先进的直接优化方法相媲美的性能。
---

## 摘要
符号回归 (SR) 旨在发现能够准确描述观测数据的可解释解析表达式。摊销 SR 有望比主流的遗传编程 SR 方法更高效，但目前在扩展到现实科学复杂度方面仍面临挑战。我们发现，一个关键障碍是缺乏将等效表达式快速约简为简洁规范形式的方法。摊销 SR 曾通过 SymPy 等通用计算机代数系统 (CAS) 来解决这一问题，但高昂的计算成本严重限制了训练和推理速度。我们提出了 SimpliPy，这是一种基于规则的简化引擎，在质量相当的情况下，其速度比 SymPy 快 100 倍。这使得摊销 SR 得到了实质性的改进，包括对更大训练集的扩展性、对每个表达式 token 预算的更高效利用，以及针对等效测试表达式的系统性训练集去污染。我们在 Flash-ANSR 框架中展示了这些优势，该框架在 FastSRB 基准测试上的准确率远高于摊销基线模型（NeSymReS、E2E）。此外，它的性能与最先进的直接优化方法 (PySR) 相当，同时随着推理预算的增加，它能恢复出更简洁而非更复杂的表达式。

## Abstract
Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.

---

## 论文详细总结（自动生成）

这是一份关于论文《Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression》的结构化深入总结：

### 1. 核心问题与研究动机
*   **核心问题**：摊销神经符号回归（Amortized SR）虽然推理速度快，但在处理复杂科学公式时面临“简化瓶颈”。
*   **研究背景**：
    *   传统的遗传编程（GP）方法（如 PySR）精度高但每次都要从零搜索，效率低。
    *   摊销方法通过预训练 Transformer 模型来预测公式，但训练需要海量高质量数据。
    *   **瓶颈所在**：现有的符号简化工具（如 SymPy）基于复杂的树遍历，速度极慢，导致在线生成简化公式（去除冗余，如 $x+x \to 2x$）成为训练速度的死穴。如果不简化，模型会学到大量冗余表达，降低推理效率和准确性。

### 2. 方法论
论文提出了 **Flash-ANSR** 框架，其核心支柱是 **SimpliPy** 引擎：
*   **SimpliPy 简化引擎**：
    *   **离线规则发现**：预先通过算法在小规模表达式空间内搜索最优约简规则，构建一个“最小生成森林”。
    *   **在线模式匹配**：在训练/推理时，将简化过程转化为高效的哈希查找和模式匹配，避免了 SymPy 沉重的对象解析开销。
    *   **效果**：在保持与 SymPy 相当的简化质量下，速度提升了 **100 倍**。
*   **Flash-ANSR 架构**：
    *   **编码器**：使用改进的 Set Transformer（引入了 Masked RMSSetNorm），处理变长的观测数据集。
    *   **解码器**：预归一化（Pre-norm）Transformer 解码器，配合 FlashAttention 优化。
    *   **在线数据生成**：利用 SimpliPy 的高速特性，在训练过程中实时生成、简化并去重（Decontamination）公式，训练量高达 5.12 亿个样本。

### 3. 实验设计
*   **数据集/基准测试**：主要使用 **FastSRB** 基准测试，该测试包含 115 个具有物理意义的科学公式，且数据域分布更符合真实科学场景。
*   **对比方法**：
    *   **摊销基线**：NeSymReS、E2E（均是目前主流的神经符号回归模型）。
    *   **直接优化基线**：PySR（目前最先进的遗传编程方法）。
*   **评估维度**：推理时间与恢复率的帕累托前沿、数据稀疏性（点数从 1 到 2048）、噪声鲁棒性（$10^{-3}$ 到 $10^{-1}$）。

### 4. 资源与算力
论文详细列出了不同规模模型的训练资源：
*   **模型规模**：提供了 3M、20M、120M 和 1B（10 亿）四种参数规模。
*   **硬件与时长**：
    *   **3M**：RTX 2080Ti，耗时 112 小时。
    *   **20M**：RTX 4090，耗时 56 小时。
    *   **120M**：A100，耗时 340 小时。
    *   **1B**：H200，耗时 657 小时。
*   **推理环境**：在 AMD Ryzen 9 9950X 和 RTX 4090 上进行统一的时间标准化评估。

### 5. 实验数量与充分性
*   **实验充分性**：研究非常深入且严谨。不仅对比了最终精度，还分析了推理预算（Compute Scaling）对结果的影响。
*   **客观性**：作者特别强调了“去污染（Decontamination）”，通过数值和符号双重检查确保训练集不包含测试集公式，这在以往的 SR 论文中常被忽视。
*   **消融实验**：针对简化引擎质量、训练先验分布（如常数范围）等进行了详细的消融研究。

### 6. 主要结论与发现
*   **性能飞跃**：Flash-ANSR 彻底碾压了之前的摊销方法（NeSymReS, E2E），并在中高预算下达到了与 PySR 持平甚至更优的水平。
*   **简洁性反转（Parsimony Inversion）**：这是一个重要发现。随着推理计算量的增加，PySR 倾向于生成更复杂的公式来拟合残差，而 Flash-ANSR 倾向于从后验分布中采样出更简洁、更接近真值的“优雅”公式。
*   **简化引擎的重要性**：SimpliPy 的高速简化使得模型能够接触到更广、更深、无冗余的公式空间，这是性能提升的核心。

### 7. 优点与亮点
*   **技术突破**：SimpliPy 解决了困扰该领域多年的“CAS 瓶颈”问题，使大规模在线训练成为可能。
*   **评估严谨**：采用了严格的机器精度恢复标准（$FVU < 10^{-7}$），而非宽松的 $R^2$ 指标。
*   **可解释性**：模型生成的公式在复杂度和结构上更符合人类科学家的审美（更简洁）。

### 8. 不足与局限
*   **噪声敏感性**：在面对高噪声数据（$\eta \ge 10^{-2}$）时，Flash-ANSR 的表现不如 PySR。这是因为模型在纯净数据上训练，会将噪声误认为是复杂的函数信号（分布偏移）。
*   **常数优化**：虽然引入了 Levenberg-Marquardt 算法优化常数，但在处理极其复杂的非线性常数依赖时，仍有改进空间。
*   **算力门槛**：1B 模型的训练需要昂贵的 H200 资源，对于普通研究者来说门槛较高。

（完）
