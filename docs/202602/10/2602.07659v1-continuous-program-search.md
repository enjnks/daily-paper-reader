---
title: Continuous Program Search
title_zh: 连续程序搜索
authors: "Matthew Siper, Muhammad Umair Nasir, Ahmed Khalifa, Lisa Soros, Jay Azhang, Julian Togelius"
date: 2026-02-07
pdf: "https://arxiv.org/pdf/2602.07659v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 遗传编程的连续程序搜索
tldr: 针对遗传编程中语法变动导致行为剧烈波动的局部性问题，本文提出了一种连续程序搜索框架。通过学习具有行为意义的连续程序嵌入空间，并设计“几何编译变异”算子，将更新限制在语义相关的子空间内。在交易策略领域的实验表明，该方法在保持进化策略不变的情况下，搜索效率提升了十倍，且在样本外表现上更具鲁棒性，证明了语义对齐变异的有效性。
motivation: 遗传编程中微小的语法突变往往导致不可预测的行为偏移，严重降低了搜索的局部性和样本效率。
method: 学习一个具备行为意义的连续程序嵌入空间，并利用流模型和语义子空间限制构建几何编译变异算子。
result: 在多项资产的交易策略搜索中，该方法以少一个数量级的评估次数实现了更高的中位样本外夏普比率。
conclusion: 通过设计与语义对齐的变异算子，可以在不改变底层进化算法的情况下显著提升程序搜索的效率和可靠性。
---

## 摘要
遗传编程（Genetic Programming）可以生成具有可解释性的程序，但微小的语法变异可能会导致巨大且不可预测的行为偏移，从而降低局部性（locality）和样本效率。我们将此建模为一个算子设计问题：学习一个连续的程序空间，使其中的潜空间距离具有行为意义，然后设计能够利用这一结构的变异算子，且无需改变进化优化器。我们通过追踪受控潜空间扰动下的动作级散度，使局部性变得可测量，并为行为局部连续变化确定了一个经验置信域。利用包含四个语义组件（多/空头入场与离场）的紧凑型交易策略领域特定语言（DSL），我们学习了一个匹配的分块因子化嵌入，并将全潜空间上的各向同性高斯变异与几何编译变异（geometry-compiled mutation）进行了对比。后者将更新限制在语义配对的入场-离场子空间内，并使用基于记录的变异结果训练的学习型流模型（flow-based model）来建议搜索方向。在相同的 (μ+λ) 进化策略和跨五种资产的固定评估预算下，学习到的变异算子仅需少一个数量级的评估次数即可发现强力策略，并实现了最高的样本外夏普比率（Sharpe ratio）中位数。尽管各向同性变异偶尔能达到更高的峰值性能，但几何编译变异带来了更快、更可靠的进展，这表明语义对齐的变异可以在不修改底层进化算法的情况下显著提高搜索效率。

## Abstract
Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation operators that exploit this structure without changing the evolutionary optimizer.   We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semantically paired entry--exit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes.   Under identical $(μ+λ)$ evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolutionary algorithm.