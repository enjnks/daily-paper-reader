Title: Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units

URL Source: https://arxiv.org/pdf/2602.07518v1

Published Time: Tue, 10 Feb 2026 01:39:22 GMT

Number of Pages: 23

Markdown Content:
# Physical Analog Kolmogorov-Arnold Networks based on Reconfigurable Nonlinear-Processing Units 

Manuel Escudero 1, Mohamadreza Zolfagharinejad 1,Sjoerd van den Belt 2, Nikolaos Alachiotis 2, Wilfred G. van der Wiel 1,3* 1NanoElectronics Group, MESA+ Institute and BRAINS Center for Brain-Inspired Computing, University of Twente, P.O. Box 217, Enschede, 7500 AE, The Netherlands. 

2CAES Group and BRAINS Center for Brain-Inspired Computing, University of Twente, Enschede, 7500 AE, The Netherlands. 

3Institute of Physics, University of M¨ unster, M¨ unster, 48149 Germany. *Corresponding author(s). E-mail(s): w.g.vanderwiel@utwente.nl; 

Abstract 

Kolmogorov-Arnold Networks (KANs) shift neural computation from linear layers to learnable nonlinear edge functions, but implementing these nonlinearities effi-ciently in hardware remains an open challenge. Here we introduce a physical analog KAN architecture in which edge functions are realized in materia using reconfig-urable nonlinear-processing units (RNPUs): multi-terminal nanoscale silicon devices whose input–output characteristics are tuned via control voltages. By combining multiple RNPUs into an edge processor and assembling these blocks into a recon-figurable analog KAN (aKAN) architecture with integrated mixed-signal interfacing, we establish a realistic system-level hardware implementation that enables compact KAN-style regression and classification with programmable nonlinear transforma-tions. Using experimentally calibrated RNPU models and hardware measurements, we demonstrate accurate function approximation across increasing task complexity while requiring fewer or comparable trainable parameters than multilayer perceptrons (MLPs). System-level estimates indicate an energy per inference of ∼250 pJ and an end-to-end inference latency of ∼600 ns for a representative workload, corresponding to a ∼10 2–10 3× reduction in energy accompanied by a ∼10 × reduction in area compared to a digital fixed-point MLP at similar approximation error. These results establish RNPUs as scalable, hardware-native nonlinear computing primitives and identify ana-log KAN architectures as a realistic silicon-based pathway toward energy-, latency-, and footprint-efficient analog neural-network hardware, particularly for edge inference. 

Multilayer perceptrons (MLPs) are the foundational building blocks of modern deep learning models, such as convolutional neural networks (CNNs) for machine vision [1] and generative pre-trained transformers (GPTs) for natural language processing tasks [2]. As illustrated in 1     

> arXiv:2602.07518v1 [cs.ET] 7 Feb 2026 σ(·)
> σ(·)
> σ(·)
> x1
> yx3x2x1
> w31w21w11

AEB DGCF                        

> x2
> x3
> x1
> x2
> x3
> w11 1
> w21 1
> w33 1Hidden layer Hidden layer Output layer
> Kolmogorov-Arnold Network (KAN) Edge Processor (EP) Analogue KAN (aKAN) Multilayer perceptron (MLP)
> Output layer RNPU
> Iout
> Vin Vc1 Vc2
> Vc3
> Vc4
> Vc5
> Vc6
> w11 2
> w12 2
> w13 2Register Multiplier Adder
> σ(·)
> x1:V1
> x2:V2
> x3:V3G11 G12 G13
> Iout :a1,a2,a3
> ^
> y
> ^
> a1ALU σ(·)
> σ(·) A/D Ctrl. Register file
> σ(·) Ctrl. Register file Crossbar
> On-chip mem. Controller
> Ctrl. σ(·) A/D Crossbar Ctrl.
> ADC + σ (·)
> Digital PEs Analogue PEs ... ... ... ... ... ... ... ... ... ... ... ... ...
> f11
> a12
> a22
> a32
> f21
> f31
> x1:V1
> Iout :a1
> Bias gen. & Gain reg. ...  ...
> ks
> k3
> k2
> k1
> kin
> Bias generator & Gain registers
> (1)
> I/V scale EP EP EP EP I/V scale
> y
> ^
> vmin
> xmin
> Iout
> xmax
> vmax Input to EPs Output prediction
> x1
> x2
> x1x2
> y^
> Input router 111
> f11
> f12
> f13 222ALU ALU ALU

Fig. 1 : Hardware implementations of neural networks and analog Kolmogorov-Arnold networks. (A) Example of a multilayer perceptron (MLP) with three input neurons, a hidden layer and a single output neuron. Hidden neuron j computes aj =

σ

P

k w(1) 

jk xk + bj



, where w(1) 

jk denotes the weight from input neuron k to hidden neu-ron j, xk the k-th input, bj an optional bias term, and σ(·) a fixed nonlinear activation function. The network output is ˆ y = P

j w(2) 1j aj . ( B) Minimal block diagram of a digital circuit implementing a neuron. The dot product P

k w(1) 

jk xk is computed using multiply-accumulate (MAC) operations, followed by σ(·) to produce the activation aj . ( C) Analog in-memory computing (AIMC) implementation of an MLP layer using a memristive cross-bar array. Inputs ( xj ) are mapped to input voltages Vk that are fed to memristive devices with conductances Gjk , generating output currents Ij = P

k Gjk Vk, which are digitized by an analog-to-digital converter (ADC) and passed through a digital activation function 

σ(·) to produce activations aj . ( D) Generic hardware architecture for accelerating neural-network operations, organizing multiple digital and/or analog processing elements (PEs) with local control logic (Ctrl.), arithmetic logic units (ALUs), register files, and, for analog PEs, crossbar arrays and ADCs. The PEs interface with an on-chip memory unit and are coordinated by a controller to parallelize MAC operations. ( E) Schematic of a Kolmogorov-Arnold Network (KAN) [14], with learnable nonlinear edge functions f lij (·) replacing fixed node activation functions, and node operations primarily consisting of summing incoming edge outputs. ( F) Edge processor (EP) implementing KAN edge functions using parallel reconfigurable nonlinear-processing units (RNPUs). Each RNPU is configured by selecting an input electrode (via an input router) and tuning its nonlinear transfer characteristic using control voltages ( Vc,i ); output gains and optional skip connections further increase edge-function flexibility. ( G) Analog KAN (aKAN) architecture comprising a reconfigurable array of EPs interconnected via programmable switch matrices. Input variables xi are lin-early encoded as voltages within the RNPU operating range and applied to EP terminals to generate nonlinear transformations. Bias generator and gain registers configure EP charac-teristics. Intermediate I/V scaling blocks rescale signals for subsequent layers. A final linear readout produces the prediction ˆ y.Fig. 1A, an MLP consists of fully connected layers of neurons organized into an input layer, one or more hidden layers, and an output layer. Each layer applies an affine transformation followed by a nonlinear activation function, σ(·), such as the hyperbolic tangent (tanh) or the rectified linear unit (ReLU). Based on the universal approximation theorem, MLPs can 2approximate any function to an arbitrary degree of accuracy under suitable conditions [3]. The computations performed by an MLP can be decomposed into linear operations, i.e., multiply–accumulate (MAC) operations, and nonlinear operations, i.e., activation functions. In conventional digital hardware, this corresponds to first computing dot products as a sequence of MAC operations (Fig. 1B), followed by evaluation of the nonlinear activation function. Although the linear part accounts for the vast majority of operations in deep neural networks, nonlinear activations are essential for expressivity and learning capacity [4]. Massive computational resources have driven the rise of deep neural networks, supported by parallel accelerators such as graphics processing units and, more recently, application-specific integrated circuits and analog in-memory computing (AIMC) platforms based on memristive crossbar arrays (Fig. 1C) [5–7]. These platforms have enabled both the training of larger models and their deployment for on-device and edge inference, typically by orga-nizing multiple digital and/or analog processing elements (PEs) around on-chip memory and a central controller to parallelize MAC operations (Fig. 1D). Crucially, both the the-ory and practice of modern deep learning are dominated by linear algebra at scale: most floating-point operations arise from matrix–vector and matrix–matrix multiplications, which are naturally mapped onto parallel hardware. As a result, today’s hardware ecosystem is optimized primarily for linear operations, reflecting the tight coupling between algorith-mic progress and prevailing compute substrates [8–10]. By contrast, nonlinear operations are critical for representational power and effective learning, yet more difficult to imple-ment in conventional digital and mixed-signal circuits. Therefore, they have received far less architectural specialization. Although linear operations in MLPs can be effectively accelerated on specialized hard-ware, their computational cost is increasing in a non-sustainable way [11, 12], limiting deployment in resource-constrained environments. Moreover, their limited interpretabil-ity renders them black boxes, hindering scientific discovery and trustworthy deployment in domains such as healthcare, finance, and security [13]. Kolmogorov-Arnold Networks (KANs) [14] have recently emerged as a promising alternative by shifting the rep-resentational emphasis from large linear transformations followed by fixed activation functions to learnable univariate nonlinear functions placed on the edges. Rooted in the Kolmogorov–Arnold representation theorem [15], KANs express multivariate continuous functions as structured compositions and sums of learned univariate functions, as illustrated 3in Fig. 1E. When combined with modern training techniques such as backpropagation, KAN variants have demonstrated regression performance comparable to and in some cases, exceeding that of MLP baselines while often using substantially fewer parameters [14, 16]. This shift in representation has important hardware implications. Whereas current AI hardware is optimized for linear, matrix-style MAC computations, KANs - in contrast to MLPs - devote a significant fraction of their computation to nonlinear operations such as univariate spline evaluations. As a result, reduced model size does not necessarily trans-late into higher energy efficiency, motivating a careful energy analysis. Recent studies have proposed hardware acceleration of KAN models, including digital implementations using look-up tables (LUTs) [17, 18] and compute-in-memory architectures that map KAN edge functions to piecewise-linear segments evaluated directly in hardware [19]. Beyond purely digital acceleration, photonic approaches have also been proposed, mapping KAN edge nonlinearities to integrated optical building blocks such as Mach-Zehnder interferometers and ring-assisted MZI units [20, 21]. More recently, a fully analog KAN concept based on negative-differential-resistance tunnel-diode characteristics has been introduced and evalu-ated using simulated device I-V curves combined with surrogate/spline fitting [22]. These developments motivate a complementary direction: implementing KAN nonlinearities via physical computing, i.e., leveraging the intrinsic properties of matter to compute directly in the physical domain. We have articulated such perspectives in the context of “intelligent matter” [23] and computation with physical dynamical systems [24]. See also [25, 26] for reviews of physical reservoir computing with emerging electronics and Momeni et al. for a review of training strategies for physical neural networks, spanning both simulation-based (digital-twin) workflows and experimental demonstrations across physical platforms [27]. From this viewpoint, a promising route toward efficient KAN hardware is to develop scal-able physical computing primitives that realize trainable nonlinear edge functions natively through tunable device physics, rather than through LUT-style emulation or circuit-level approximations. Here, we substantially extend our preliminary account [28] and demonstrate a physical 

analog KAN architecture using reconfigurable nonlinear-processing units (RNPUs) as phys-ical computing primitives. (During the finalization of this manuscript, we became aware of related independent work by Taglietti et al. [29], which also explores physical KAN imple-mentations using reconfigurable nonlinear silicon devices.) Specifically, we implement the 4defining operation of KANs by replacing fixed activation functions with learnable nonlin-ear functions on the network edges, realized in materia by RNPUs. Figure 1F shows the RNPU concept: a multi-terminal nanoscale doped-silicon device whose steady-state input-output response can be continuously tuned by applying control voltages to its electrodes. In our earlier work, we pioneered the use of RNPUs as physical function approximators and demonstrated that a single device can be trained to realize a variety of time-independent computational tasks, including classification and regression in static settings at low temper-ature [30–35]. More recently, we extended the concept of RNPUs to time-dependent signal processing tasks at room temperature, where RNPUs implement nonlinear temporal trans-formations that enable efficient processing of dynamic input streams [36]. Our prior results thus establish RNPUs as a versatile platform for programmable nonlinear computation. This makes KANs a natural architectural match: unlike MLPs, which rely primarily on linear layers combined with simple fixed activations, KANs place the computational empha-sis on expressive trainable nonlinear edge functions. RNPUs directly provide such nonlinear primitives in hardware, enabling edge functions to be realized in materia without reducing them to digital approximations or LUT-based emulations. In this study, we use each RNPU in a single-input, single-output configuration, thus effectively implementing an in-materia 

nonlinear data processor, while the remaining electrodes are biased with control voltages that tune the nonlinear characteristics of the device. The RNPU is operated in steady-state mode (see Materials and Methods). To increase expressivity and precision, multiple RNPUs can be combined to form a reconfigurable edge processor (EP) whose aggregate response implements the desired learned nonlinearity (see Fig. 1F). Multiple EPs can then be com-bined to form an RNPU-based analog KAN (aKAN) architecture, as schematically presented in Fig. 1G. The resulting aKAN architecture follows a design philosophy similar to that of a field-programmable analog array [37]: programmable interconnections provide flexible routing between EPs, enabling multi-layer, variable-width networks. In addition, interme-diate I/V conversion and rescaling blocks ensure that EP output currents are mapped to voltage ranges compatible with subsequent EP stages. In the example shown in the lower part, each input variable xi is first linearly encoded as a voltage within the operating range of the RNPU [ Vmin , V max ] and applied to an EP, which acts as a nonlinear map to an out-put current Iout . These EP outputs are then combined by the network to generate the final prediction ˆ y, illustrated here for a two-input target function. 5We evaluate our aKANs architecture on regression and classification benchmarks using experimentally calibrated surrogate models together with experimental hardware measure-ments. For representative function-approximation tasks, aKANs reach MSE ∼ 10 −2 (and below for selected cases) and approach the performance of software MLP baselines, includ-ing MLPs with tanh activation function, at comparable model sizes. Using measured RNPU characteristics ( ∼50 nW power, ∼1 μm2 footprint, ∼10 ns intrinsic response time [36]) and realistic mixed-signal peripheral assumptions, our system-level hardware estimates indicate up to three orders of magnitude lower energy per inference than a digital fixed-point MLP at similar error levels, while reducing the estimated silicon area by roughly one order of magni-tude at sub-micosecond latency. These results position aKANs as a route toward compact, low-power edge inference where programmable nonlinear transformations are implemented directly in matter. We employ a hybrid training and validation workflow that (a) trains the RNPUs to iden-tify control voltages that give a desired nonlinear edge function, and (b) validates nonlinear functions by performing experimental measurements. Thus, we first develop a data-driven surrogate model of the devices that allows for end-to-end training in software using back-propagation [33]. This surrogate model captures how steady-state output current of an RNPU depends on the applied input voltage at a chosen input electrode and on the set of control voltages applied to the remaining six electrodes, which makes gradient-based optimization possible. We then embed the surrogate model into Brains-Py, an open-source material-learning framework [38], which can include multiple RNPUs as well as the sur-rounding circuit-level degrees of freedom required by our architecture (for example, electrode selection via the input router, programmable output gains, and optional skip connections inside the edge processor). 

Function approximation with analog KANs 

We use standard backpropagation to optimize two coupled sets of parameters: (1) per-RNPU control voltages, which shape the nonlinear transfer characteristics and therefore implement the learned univariate edge nonlinearities, and (2) the linear combination coef-ficients (output gains) that determine how multiple RNPU responses are summed to form each edge function. After training, the same parameter set can be evaluated purely in soft-ware (using the surrogate model), or transferred to the physical device for experimental validation. Because our current measurement setup includes a single RNPU, multi-RNPU 6ADB CNumber of parameters Number of parameters Number of parameters MSE 

E FMSE MSE MLP (relu) MLP (tanh) aKAN −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 Software 2 RNPUs 3 RNPUs                     

> ...  ... ... ksk3 k2
> k1kin Input router
> RNPU1 EP Output Input (V)
> Avg. MSE
> Input (V) Input (V) Output (V) Output (V) Output (V) RNPU2 −1.0 −0.5 0.0 0.5 1.0 −0.5 0.0 0.5 1.0 Software 5 RNPUs 10 RNPUs
> 10 310 3
> 10 210 210 210 310 410 510 -2
> 10 -1 10 -3
> 10 -3
> 10 -2
> 10 -1 10 -3
> 10 -4
> 10 -5
> f(x) = J 0(20 x)f(x) = e f(x) = e
> 3.2 × 10 -2 MSE
> 1.7 × 10 -2
> 6.6 × 10 -3
> (sin(π x1+x22)) (sin(π( x12+x22)) + sin(π( x13+x24)))
> Avg. MSE 6.5 × 10 -3
> 9.5 × 10 -4

Fig. 2 : Function approximation with RNPU-based edge processors. (A) Experi-mental sine-wave fitting using an edge processor (EP) composed of two RNPUs in parallel. A linear combination of the individual RNPU outputs (left panels) approximates (blue curve) the target sine function (black curve). ( B) Sine-wave approximation using EPs composed of 2 (blue shading) and 3 (orange shading) RNPUs in parallel. The shaded regions indicate the spread among the best five fits obtained from independent training runs with random-ized input-electrode selection.( C) Bessel-function approximation (J 0(20 x)) using EPs with 5 (blue shading) and 10 (orange shading) parallel RNPUs. The shaded regions show the spread among the best five fits, as in panel B. ( D–F ) Scalability of aKANs and MLP base-lines (with ReLU or tanh activations), showing approximation error as a function of the number of trainable parameters for three target functions: J 0(20 x) in panel D e(sin( πx 1)+ x22)

in panel E and e(sin( π(x21+x22))+sin( π(x23+x24))) in panel F. Both aKANs and MLPs are simu-lated across a range of network depths and widths; for aKANs, we additionally vary the number of RNPUs per EP and randomly select RNPU input electrodes. networks are realized via time multiplexing: each RNPU instance in the trained aKAN is emulated sequentially by applying its corresponding input and control voltages to the same physical device and recording the output current, while intermediate node values and sum-mations are handled externally. In this way, we exploit in-materia nonlinear computation at the edges, while the summation of edge outputs at the nodes and the bookkeeping needed to route and rescale signals across layers are still done conventionally. We first demonstrate the RNPUs’ capability to approximate continuous nonlinear func-tions, using the EP concept introduced in Fig. 2F. As a simple univariate example, we fit a sine wave by combining the outputs of multiple RNPUs with trainable output gains. This choice is deliberate: we find that a single RNPU does not provide sufficient functional complexity to accurately reproduce this target function. Figure 2A shows the resulting fit together with the individually learned RNPU responses, obtained by tuning the control volt-ages using our training procedure. For the case of two devices, we obtain a mean-squared error (MSE) of 1 .7 × 10 −2, using 14 trainable parameters in total (7 per RNPU: six control voltages and one output gain). 7Next, we investigate how the approximation accuracy scales with the number of RNPUs and varies depending on the selected input electrodes. Each RNPU operates using a single chosen input electrode, and different choices of input electrodes among the RNPUs in an EP can lead to markedly different performance. Increasing the number of RNPUs per EP, as well as varying the chosen input electrodes among RNPUs, improves expressivity and enables more complex function representations. We illustrate this by approximating both the sine wave and the Bessel function J 0(20 x) using different numbers of RNPUs (see Figs. 2B and C, respectively). The Bessel function is a challenging and relevant (ubiquitous in wave and diffraction problems) oscillatory benchmark with slowly decaying amplitude, which probes the ability of the network to represent nontrivial nonlinear structure beyond simple periodic functions. For each configuration, we perform multiple training runs with randomized input electrodes and report the best five results; the shaded region indicates the spread among these solutions. In both cases, using more devices ( e.g. , three for the sine wave and ten for J 0(20 x)) systematically lowers the approximation error, consistent with the increased nonlinear expressivity afforded by larger RNPU ensembles. Furthermore, we assess how the proposed aKAN architecture scales relative to conven-tional MLP baselines. To this end, we compare algorithmic performance, quantified as MSE, as a function of the number of trainable parameters ( i.e. , model size). We evaluate approx-imation performance on representative 1-, 2-, and 4-variable target functions adopted from Ref. [14], which provide well-defined benchmarks with increasing functional complexity. For each target, we simulate both aKANs and MLPs across a range of depths and layer widths. For aKANs, we additionally sweep the number of RNPUs per edge function and randomly select the input electrodes for each RNPU. For the MLP baselines, we consider networks with either ReLU or tanh activation functions at the nodes, evaluated in software using 32-bit floating-point precision. The resulting scaling trends are summarized in Figs. 2D–F. For all three target functions, the approximation error systematically decreases with increasing model size (number of trainable parameters), for both aKANs and MLPs. When comparing error versus parameter count, the aKAN performance lies between that of ReLU- and tanh-MLPs, approaching the tanh baseline for sufficiently large aKANs. This behavior is expected: the 2- and 4-variable functions require a minimum aKAN architecture to represent the target as compositions of univariate edge functions. Once this minimum size is reached, the aKAN can effectively exploit the expressivity of learned nonlinear edges, and its performance improves rapidly. 8In this regime, aKANs substantially outperform ReLU-based MLPs at comparable model sizes. Although tanh-MLPs remain competitive in this numerical comparison, it should be noted that tanh provides a highly expressive nonlinearity and is not straightforward to implement efficiently in hardware, particularly with the software baseline assumed here (32-bit floating-point arithmetic). 

Classification with analog KANs 

We next evaluate aKANs for two binary classification tasks: (i) synthetic two-dimensional tunable datasets and (ii) real-world benchmark datasets. Because these problems involve few input features and a low-dimensional output, compact network architectures are sufficient. In all cases, the model produces a single scalar output whose magnitude determines the assigned class. We first consider the Moons and Spirals datasets (Fig. 3A) [39, 40], which provide con-trolled settings with progressively more nonlinear decision boundaries. For each dataset, we generate two variants of increasing complexity by varying a single generative parameter: the noise level for Moons (top panels) and the number of spiral turns for Spirals (bottom panels). Across tasks, we experimentally explore a range of aKAN architectures, from min-imal to larger configurations, and stop increasing complexity once accuracy gains saturate. Similarly, we consider ReLU-MLPs with minimal learnable parameters and hidden layers that achieve comparable classification accuracies (within a < 1%-point difference). Figure 3A summarizes the aKAN performance under the specified conditions. As expected, the required model size increases with the geometric complexity of the decision boundary for both aKANs and MLPs. We also evaluate the performance of aKANs in three benchmark datasets: MAGIC Gamma Telescope [41], COD-RNA [42], and Skin Segmentation [43]. Our aim is not only to demonstrate feasibility, but also to determine where aKANs offer advantages over conven-tional neural networks. As before, we identify aKANs that balance architectural complexity and accuracy. For comparison, we pair each aKAN with a ReLU-MLP whose average accu-racy differs by no more than 1%-point from that of its aKAN counterpart. Simulations (Fig. 3B) show that aKANs achieve software-iso accuracy, although the Skin and MAGIC datasets require a larger number of learnable parameters. Notably, here we are limited to six learnable parameters per RNPU. Thus, learnable parameter count is not optimal for 9A B    

> C
> Skin Moons (noisy) 151 47 541 112 26 38 101 74 91 162 MLP (ReLU) aKAN Spirals Moons
> Learnable parameter count
> Spirals (1.5 turns) COD-RNA Accuracy (%) MAGIC MLP aKAN 100 20 60 80 40 [3, 5, 1] [8, 10, 1] [10, 5, 5, 1] [10, 1]  2[8, 1]  1[3, 1, 1]  187.3 96.2 95.4 100 100 86.4
> w/o noise w/ noise
> MAGIC COD-RNA Skin
> 1 turn 1.5 turns

Fig. 3 : Benchmarking aKANs for binary classification. (A) Experimental classifi-cation and decision boundaries on the Moons and Spirals datasets by aKANs. Top left: low perturbed (noise = 0.05) Moons, where a [2 , 1] 2 aKAN achieves 99.5% accuracy, com-pared to 99.1% for a [2 , 9, 9, 1] MLP (not shown). Top right: highly perturbed (noise = 0.15) Moons, where a [2 , 1] 3 aKAN reaches 99.5% accuracy versus 98.7% for a [2 , 10 , 10 , 1] MLP (not shown). Bottom left: 1-turn spiral, where a [2 , 3, 1] 4 aKAN achieves 99.75% accu-racy compared to 98.5% for a [2 , 10 , 10 , 1] MLP (not shown). Bottom right: 1.5-turns spiral, where a [2 , 4, 1] 5 aKAN achieves 96.8% accuracy, matching 91% for a [2 , 15 , 15 , 15 , 1] MLP (not shown). Networks are denoted [ nI , n H1, . . . , n HL , n O ]d, with nI , nO the input/output sizes, nH1 . . . n HL the hidden layer widths, and d the RNPU number per EP in aKANs. (B) Simulated aKAN accuracy on Skin Segmentation, COD-RNA, and MAGIC datasets, compared with ReLU-MLPs; architectures indicated. ( C) Comparison of learnable param-eter count for aKANs and ReLU-MLPs across the classification tasks. aKANs. Instead, comparable accuracies can often be achieved with simpler architectures that have no hidden layer or use only a single RNPU per edge. We compare parameter efficiency with ReLU-MLP baselines and summarize the cor-responding parameter counts in Fig. 3C. For the geometric datasets, aKANs achieve competitive accuracy using compact architectures with substantially fewer trainable param-eters, consistent with their ability to represent nonlinear decision boundaries through expressive learned edge functions. For the real-world datasets, accurate classification is already obtained with very simple aKANs, likely because these benchmarks are largely separable through a small number of dominant features and low-order statistics, a regime well matched to the representational capacity of single RNPUs. Under such conditions, the parameter-efficiency advantage of larger aKANs is not fully revealed, and the architec-ture may effectively be over-parameterized for these tasks. Even when reducing the number of control parameters (e.g., by using a single RNPU per edge function and fewer control electrodes per device) the impact on classification accuracy is limited. From a system-level perspective, however, such parameter reductions do not substantially change overall efficiency, because, as discussed below, system-level performance is for now dominated by mixed-signal overhead rather than by the number of nonlinear control parameters. 10 A       

> L1 regularization L1 regularization + pruning No regularization, no pruning
> BCyyx1x2
> x1x2x1
> x1
> x2
> x2
> ^y
> ^
> y
> ^

Fig. 4: Regularization and pruning of analog KANs. (A) aKAN fit to y =

e(sin( πx 1)+ x22) without regularization or pruning (MSE = 8 × 10 −3). ( B) Same task with L1 regularization applied to RNPU output gains (MSE = 23 .8 × 10 −3); the contributions of many EPs are suppressed and the dominant EPs are highlighted. ( C) Pruned network derived from ( B) after fine-tuning the remaining EPs (MSE = 23 .3 × 10 −3). 

Pruning analog KANs 

In the previous examples, prior task knowledge enabled the straightforward selection of suitable aKAN architectures. However, in practice, this information is often not avail-able, and MLPs are typically oversized to facilitate training convergence, at the expense of higher computational costs. In KANs, however, interpretable edge functions enable struc-tural pruning that minimizes network size and is more efficient in deployment. In particular, L1 and entropy-based regularizations, which promote sparsity and interpretability, have been already discussed in the literature that enable the removal of edges with negligible contribution [14]. In our proposed aKAN architecture, pruning yields more compact models and frees up EPs, which improves hardware utilization. Thus, we investigate whether aKANs can be pruned without sacrificing performance. Following the node-pruning philosophy [14], we prune EPs using a simplified scheme: we add an output gain per EP and apply L1 regularization to these gains to suppress EP contributions to the next layer. After training, the regularization gains are absorbed into EP gains ki. As a case study, we revisit the 2-variable function e(sin( πx 1)+ x22) (Fig. 2E). Although it admits a minimal aKAN[2,1,1] 3

representation, we start from an oversized aKAN[2,3,3,1] 3 and compare training without regularization versus L1 regularization applied to all EP output gains, see Fig. 4A and Fig. 4B, respectively. 11 L1 regularization promotes sparsity by driving many EP gains toward near-zero values, thus, suppressing their contribution to the output. The result is a more interpretable model (Fig. 4B): the remaining active EPs align with meaningful input-output dependencies of the target function. These are reflected by learned transfer characteristics that resemble elementary components such as sine, square, and exponential operations. For pruning, we remove an EP if it (i) is insufficiently activated by the preceding layer (below a threshold) or (ii) contributes only weakly to the subsequent activation. Apply-ing this rule to the regularized network in Fig. 4B results in substantial pruning. After fine-tuning, the compact network (Fig. 4C) retains the essential EP transfer characteris-tics needed for approximation. The second-layer EP becomes largely linear and effectively serves as a skip connection, consistent with the fact that the initial aKAN[2,3,3,1] 3 includes an extra layer relative to the minimal target composition. While the regularized and pruned models achieve similar MSE, both remain less accurate than the unpruned network. Although based on a single case study, this suggests that pruning can reduce expressivity, whereas larger aKANs preserve more flexibility to learn heterogeneous nonlinearities. 

System-level hardware comparison with digital neural networks 

Having established the regression and classification performance of aKANs, we now bench-mark an RNPU-based aKAN against a conventional digital tanh-MLP at the system level in terms of energy per inference, silicon area, and latency. Because the two approaches rely on fundamentally different computational primitives and design architectures and our goal is not to evaluate a general-purpose implementation, we perform the comparison for a representative nonlinear workload: approximation of e(sin( πx 1)+ x22) (Fig. 2E). We there-fore adopt the aKAN and MLP configurations from Fig. 2E and estimate energy, area, and latency for a batch of 1,000 samples based on the hardware components required by these implementations. We begin by characterizing the response of a single RNPU, which provides the physical basis for the system-level estimates. Applying representative input and control biases yields an average power of 50 nW. The footprint of an individual RNPU is approximately 1 μm2,including device area and routing overhead, and the intrinsic inference latency is ∼10 ns [36]. In addition to the RNPUs, aKANs require peripheral circuitry to generate input and control voltages, acquire RNPU outputs, and interface with the surrounding system. This 12 Mean Square Error 10 -1 10 -2 10 -4 10 -3 Mean Square Error 10 -1 10 -2 10 -4 10 -3          

> AB
> 10 -3 10 -4 10 -5 10 510 610 710 -6 10 -7 Energy (J) Area ( μm2 )
> aKAN MLP (tanh)
> aKAN MLP (tanh)

Fig. 5 : System-level energy and silicon area comparison between aKANs and MLPs for nonlinear function evaluation. Each function evaluation corresponds to the evaluation of 1,000 samples of e(sin( πx 1)+ x22). For the digital baseline, MLP multiply-accumulate units are implemented using the NanGate45 Open Cell library and the tanh activation function is realized by a look-up table. ( A) Estimated energy per inference as a function of mean-squared error (MSE). The transimpedance amplifier (TIA) is the dominant contribution to the aKAN energy consumption. ( B) Estimated silicon area versus MSE. Multiple network widths and depths are included for both aKAN and MLP architectures. Solid red lines serve as guides to the eye. mixed-signal overhead, comprising digital-to-analog converters (DACs), ADCs, and current-to-voltage conversion, contributes to the overall energy consumption, silicon area, and latency. We assume 10-bit DACs with a power consumption of 1.46 μW at 2 MHz, both for quasi-static control voltages and input ports [44], and a 12-bit, 100 MS/s ADC (2.6 mW) for output digitization [45]. Programmable output gains are implemented using memristive elements and one transimpedance amplifier (TIA) per node. For this stage, we adopt an integrated low-power TIA design with a power consumption of 94 μW, which satisfies the required bandwidth and gain constraints [46]. In practice, a custom on-chip implementation would likely reduce this power budget. Because the area of this TIA is not reported, we conservatively estimate a footprint of 7,000 μm2. Inference in the proposed aKAN occurs in O(1) time, as all computational elements are physically deployed in parallel. The end-to-end inference latency is therefore determined by DAC and ADC conversion times together with the intrinsic RNPU response. We assume that TIA settling and any additional current-to-voltage scaling stages are designed to match the RNPU timescale. Under these conditions, the total inference latency is estimated to be approximately 600 ns. For comparison, we consider a 16-bit fixed-point tanh-MLP using the NanGate45 Open Cell Library [47]. To enable a controlled architectural comparison, we evaluate the neuron datapath while normalizing across implementations, excluding interconnect and layer-sequencing control that are architecture-dependent. Each neuron is implemented as a 4-lane single instruction, multiple data (SIMD) unit performing one weight-input multiplication 13 per lane per cycle (four MACs per clock cycle) at moderate hardware cost. The tanh activation is implemented using a 4-lane look-up table evaluated in a single cycle . The accelerator is described in Verilog and synthesized using open-source design tools to extract area and power, which are then scaled to the full MLP configuration. In paral-lel, an equivalent quantized network is trained in PyTorch on the same benchmark task, enabling a direct comparison of approximation error under consistent numerical precision. In this digital architecture, inference latency scales with network depth and width because multiply–accumulate operations must be accumulated sequentially across layers. For the representative networks considered here, operation at a 500 MHz clock frequency yields an estimated inference latency of approximately 100 ns for MLPs achieving moderate accuracy. Figure 5A summarizes the estimated system-level energy per inference as a function of MSE for representative aKAN and MLP configurations (see Materials and Methods). Across moderate-accuracy regimes, aKANs exhibit substantially more favorable energy scaling, reducing the energy per inference by up to three orders of magnitude relative to the digital MLP baseline. Under the assumptions detailed above, the total energy consumption of present aKAN implementations is dominated by mixed-signal peripheral overhead (DACs, ADCs, and TIAs), leading to an estimated energy of approximately 250 pJ per inference, corresponding to about 250 nJ for evaluation of 1,000 samples. Because this contribution is primarily determined by peripheral circuitry, further reductions in TIA power would proportionally lower the overall inference energy. By contrast, the digital MLP distributes energy consumption across multi-ply–accumulate operations, activation evaluation, and supporting circuitry, as reflected in Fig. 5A. The corresponding silicon-area comparison (Fig. 5B) shows that aKAN imple-mentations achieve roughly an order-of-magnitude smaller footprint than their MLP counterparts at MSE = 10 −3, consistent with the parallel, device-level realization of nonlinear computation. 

Discussion 

Our results point to a clear message: when learnable nonlinear transformations are treated as primary computational primitives, neural inference can be organized around physical response rather than linear algebra. By realizing nonlinear edge functions directly in mat-ter, aKANs replace the conventional paradigm of linear multiply–accumulate operations 14 combined with digitally emulated nonlinearities with hardware-native nonlinear computa-tion based on tunable device characteristics. This shift defines an alternative design space for edge intelligence, in which efficiency arises not only from accelerating linear operations but from embedding expressive nonlinear mappings directly within the physical substrate. More broadly, reconfigurable nonlinear devices emerge as modular building blocks for scalable analog learning systems. Composability, through the use of multiple devices per edge, and structural sparsity, enabled by pruning, provide complementary routes to com-pactness, robustness, and dynamic resource allocation on reconfigurable arrays. System-level energy analysis further identifies a central architectural constraint: present efficiency is limited primarily by the mixed-signal interface rather than by the RNPU core itself, highlighting the need for joint optimization of devices, circuits, and architectures. Looking forward, a transition from trained physics toward co-designed physics offers a natural path for progress, in which electrode geometry, biasing, and learning objectives are optimized simultaneously to replace heuristic configuration with principled physical design. Additional gains in energy efficiency are expected from integrated peripheral cir-cuitry and scalable multi-device arrays that remove the need for time-multiplexed operation. Together, these advances outline a realistic route toward compact, low-power edge-inference hardware in which nonlinear computation is native, programmable, and scalable, and sug-gest a broader paradigm in which information processing is co-designed with the physical properties that implement it. 

# REFERENCES AND NOTES 

[1] A. Krizhevsky, I. Sutskever, G.E. Hinton, ImageNet classification with deep convolu-tional neural networks. Communications of the ACM 60 (6), 84–90 (2017) [2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need , in Proceedings of the 31st International Con-ference on Neural Information Processing Systems (Curran Associates Inc., 2017), NIPS’17, pp. 6000–6010 [3] K. Hornik, M. Stinchcombe, H. White, Multilayer feedforward networks are universal approximators. Neural networks 2(5), 359–366 (1989) 15 [4] K. Hornik, Approximation capabilities of multilayer feedforward networks. Neural networks 4(2), 251–257 (1991) [5] D. Markovi´ c, A. Mizrahi, D. Querlioz, J. Grollier, Physics for neuromorphic computing. Nature Reviews Physics 2(9), 499–510 (2020) [6] M. Zolfagharinejad, U. Alegre-Ibarra, T. Chen, S. Kinge, W.G. van der Wiel, Brain-inspired computing systems: a systematic literature review. The European Physical Journal B 97 (6), 70 (2024) [7] A. Singh, M. Le Gallo, A. Vasilopoulos, J. Luquin, P. Narayanan, G.W. Burr, A. Sebas-tian, The design of analogue in-memory computing tiles. Nature Electronics 8,1156–1169 (2025) [8] R. Sutton, The bitter lesson. Incomplete Ideas (blog) (2019). URL http://www. incompleteideas.net/IncIdeas/BitterLesson.html [9] S. Hooker, The hardware lottery. Communications of the ACM 64 (12), 58–65 (2021) [10] J. Laydevant, L.G. Wright, T. Wang, P.L. McMahon, The hardware is the software. Neuron 112 (2), 180–183 (2024) [11] N.C. Thompson, K. Greenewald, K. Lee, G.F. Manso, The Computational Limits of Deep Learning (2020). Arxiv.2007.05558 [12] A. Mehonic, A.J. Kenyon, Brain-inspired computing needs a master plan. Nature 

604 (7905), 255–260 (2022) [13] Y. Zhang, P. Tino, A. Leonardis, K. Tang, A Survey on Neural Network Interpretability. IEEE Transactions on Emerging Topics in Computational Intelligence 5(5), 726–742 (2021) [14] Z. Liu, Y. Wang, S. Vaidya, F. Ruehle, J. Halverson, M. Soljaˇ ci´ c, T.Y. Hou, M. Tegmark, Kan: Kolmogorov-arnold networks (2024). ArXiv:2404.19756 [15] A. Kolmogorov, On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition. Dokl. Akad. Nauk SSSR 114 (5), 935–956 (1957) 16 [16] R. Yu, W. Yu, X. Wang, Kan or mlp: A fairer comparison (2024). ArXiv:2407.16674 [17] W.H. Huang, J. Jia, Y. Kong, F. Waqar, T.H. Wen, M.F. Chang, S. Yu, Hardware acceleration of kolmogorov-arnold network (kan) for lightweight edge inference , in Pro-ceedings of the 30th Asia and South Pacific Design Automation Conference (2025), pp. 693–699 [18] D. Hoang, A. Gupta, P. Harris, Kanel´ e: Kolmogorov-arnold networks for efficient lut-based evaluation ArXiv:2512.12850 [19] C. Sudarshan, P. Manea, J.P. Strachan, A kolmogorov–arnold compute-in-memory (ka-cim) hardware accelerator with high energy efficiency and flexibility (2025). Reseach Square preprint [20] K. Sozos, D. Spanos, S. Deligiannidis, G. Sarantoglou, N. Passalis, N. Pleros, C. Mesar-itakis, A. Tefas, A. Bogris, Photonic kolmogorov-arnold networks based on self-phase modulation in nonlinear waveguides. Optics Letters 51 (3), 664–667 (2026) [21] Y. Peng, S. Hooten, X. Yu, T. Van Vaerenbergh, Y. Yuan, X. Xiao, B. Tossoun, S. Che-ung, M. Fiorentino, R. Beausoleil, Photonic kan: a kolmogorov-arnold network inspired efficient photonic neuromorphic architecture (2024). ArXiv:2408.08407 [22] S. Li, T. Wang, J. Tang, R. Liu, H. Li, Y. Lu, F. Xu, B. Gao, C. Xie, X. Zhu, Fully analogue in-memory neural computing via quantum tunneling effect. arXiv:2510.23638 (2025) [23] C. Kaspar, B.J. Ravoo, W.G. van der Wiel, S.V. Wegner, W.H.P. Pernice, The rise of intelligent matter. Nature 594 (7863), 345–355 (2021) [24] H. Jaeger, B. Noheda, W.G. van der Wiel, Toward a formal theory for computing machines made out of whatever physics offers. Nature Communications 14 (1), 4911 (2023) [25] X. Liang, J. Tang, Y. Zhong, B. Gao, H. Qian, H. Wu, Physical reservoir computing with emerging electronics. Nature Electronics 7(3), 193–206 (2024) [26] S. Kasai, Semiconductor technologies and related topics for implementation of elec-tronic reservoir computing systems. Semiconductor Science and Technology 37 (10), 17 103001 (2022) [27] A. Momeni, B. Rahmani, B. Scellier, L.G. Wright, P.L. McMahon, C.C. Wanjura, Y. Li, A. Skalli, N.G. Berloff, T. Onodera, et al., Training of physical neural networks. Nature 

645 (8079), 53–61 (2025) [28] M. Zolfagharinejad, in Information processing with silicon-based nonlinear computing units (University of Twente, Enschede, The Netherlands, 2025). https://doi.org/10. 3990/1.9789036568104 [29] During the finalization of this manuscript, we became aware of related indepen-dent work by Taglietti, which also explores physical KAN implementations using reconfigurable nonlinear silicon devices. F. Taglietti, A. Pulici, M. Roxburgh, G. Seguini, I. Vidamour, S. Menzel, E. Franco, M. Laus, E. Vasilaki, M. Perego, T.J. Hayward, M. Fanciulli, J.C. Gartside, Learning Nonlinear Heterogeneity in Physical Kolmogorov-Arnold Networks (2026). ArXiv:2101.12345 [30] T. Chen, J. van Gelder, B. van de Ven, S.V. Amitonov, B. de Wilde, H.-C. Ruiz Euler, H.J. Broersma, P.A. Bobbert, F.A. Zwanenburg, W.G. van der Wiel, Classification with a disordered dopant-atom network in silicon. Nature 577 (7790), 341–345 (2020) [31] T. Chen, P.A. Bobbert, W.G. van der Wiel, 1/ f Noise and Machine Intelligence in a Nonlinear Dopant Atom Network. Small Science 1(3), 2170006 (2021) [32] H.-C. Ruiz Euler, M.N. Boon, J.T. Wildeboer, B. van de Ven, T. Chen, H.J. Broersma, P.A. Bobbert, W.G. van der Wiel, A deep-learning approach to realizing functionality in nanoelectronic devices. Nature Nanotechnology 15 (12), 992–998 (2020) [33] H.-C. Ruiz Euler, U. Alegre-Ibarra, B. van de Ven, H.J. Broersma, P.A. Bobbert, W.G. van der Wiel, Dopant network processing units: towards efficient neural network emulators with high-capacity nanoelectronic nodes. Neuromorphic Computing and Engineering 1(2), 024002 (2021) [34] B. van de Ven, U. Alegre-Ibarra, P.J. Lemieszczuk, P.A. Bobbert, H.C. Ruiz-Euler, W.G. van der Wiel, Dopant network processing units as tuneable extreme learning machines. Frontiers in Nanotechnology 5, 1055527 (2023) 18 [35] M.N. Boon, L. Cassola, H.-C. Ruiz Euler, T. Chen, B. van de Ven, U. Alegre-Ibarra, P.A. Bobbert, W.G. van der Wiel, Gradient descent in materia through homodyne gradient extraction. Nature Communications 16 (1), 10272 (2025) [36] M. Zolfagharinejad, J. B¨ uchel, L. Cassola, S. Kinge, G. Sarwat Syed, A. Sebastian, W.G. van der Wiel, Analogue speech recognition based on physical computing. Nature 

645 , 886–892 (2025) [37] S. George, S. Kim, S. Shah, J. Hasler, M. Collins, F. Adil, R. Wunderlich, S. Nease, S. Ramakrishnan, A Programmable and Configurable Mixed-Mode FPAA SoC. IEEE Transactions on Very Large Scale Integration (VLSI) Systems pp. 1–9 (2016) [38] U. Alegre-Ibarra, H.-C. Ruiz Euler, H. A.Mollah, B.P. Petrov, S.S. Sastry, M.N. Boon, M.P. de Jong, M. Zolfagharinejad, F.M.j. Uitzetter, B. van de Ven, A.J.S. de Almeida, S. Kinge, W.G. van der Wiel, brains-py, a framework to support research on energy-efficient unconventional hardware for machine learning. Journal of Open Source Software 8(90), 5573 (2023) [39] Moons dataset. URL https://scikit-learn.org/stable/modules/generated/sklearn. datasets.make moons.html [40] Spirals dataset. URL https://conx.readthedocs.io/en/latest/Two-Spirals.html [41] R. Bock. MAGIC Gamma Telescope (2004). https://doi.org/10.24432/C52C8B [42] A.V. Uzilov, J.M. Keegan, D.H. Mathews, Detection of non-coding RNAs on the basis of predicted secondary structure formation free energy change. BMC Bioinformatics 

7(1), 173 (2006) [43] A.D. Rajen Bhatt. Skin Segmentation (2009). https://doi.org/10.24432/C5T30C [44] R. Rubino, P.S. Crovetti, O. Aiello, Design of Relaxation Digital-to-Analog Converters for Internet of Things Applications in 40nm CMOS , in 2019 IEEE Asia Pacific Con-ference on Circuits and Systems (APCCAS) (IEEE, Bangkok, Thailand, 2019), pp. 13–16 [45] J. Luo, J. Li, N. Ning, Y. Liu, Q. Yu, A 0.9-V 12-bit 100-MS/s 14.6-fJ/Conversion-Step SAR ADC in 40-nm CMOS. IEEE Transactions on Very Large Scale Integration 19 (VLSI) Systems 26 (10), 1980–1988 (2018) [46] M. Mathew, B. Hart, K. Hayatleh, Low input-resistance low-power transimpedance amplifier design for biomedical applications. Analog Integrated Circuits and Signal Processing 110 (3), 527–534 (2022) [47] FreePDK45: 45nm variant from NCSU. URL https://eda.ncsu.edu/freepdk/freepdk45/ 

# ACKNOWLEDGMENTS 

We thank M. H. Siekman, J. G. M. Sanderink and M. Schremb for technical support, U. Alegre-Ibarra, A. J. Annema, P. A. Bobbert, L. Cassola, T. Chen, R. J. C. Cool and J. Kareem for stimulating discussions. Funding: We acknowledge financial support from Toyota Motor Europe N.V., the IMAGINE project funded by the Dutch Research Council (NWO) KIC grant no. KICH1.ST04.22.033, the HYBRAIN project funded by the European Union’s Horizon Europe research and innovation programme under grant agreement no. 101046878. This work was further funded by the Deutsche Forschungsge-meinschaft (DFG, German Research Foundation) grant no. SFB 1459/2 2025–433682494 . Author contributions: M.Z. and W.G.v.d.W. conceived the project. M.E., M.Z., and S.v.d.B. carried out the experiments. All authors contributed to data analysis and inter-pretation. All authors contributed to the writing of the manuscript and to revisions. W.G.v.d.W. supervised the project. Competing interests: The authors declare that they have no competing interests. Data, code, and materials availability: . License infor-mation: Copyright © 2026 the authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original US government works. https://www.science.org/about/science-licenses-journal-article-reuse. 20 SUPPLEMENTARY MATERIALS 

Materials and Methods 

Experimental setup 

The RNPU used in this work is a boron-doped silicon slab with eight electrodes placed on top in a circular geometry with a diameter of 300 nm. The device is mounted in a measurement enclosure, where seven electrodes are connected to DACs (cDAQ NI-9264, 25 kS/s per channel) and the remaining electrode is connected first to a 2 V/nA gain transimpedance amplifier, enabling indirect measurement of the RNPU output current, and subsequently to a DAC (cDAQ NI-9252, 50 kS/s per channel). The RNPU is operated in steady state, i.e., the output current is acquired after a waiting time of 10 μs following the application of control and input voltages (due to setup limita-tions, not fundamental). This procedure is used both to collect samples for constructing the surrogate model and for hardware verification. Due to measurement-setup limitations, only a single device is available for hardware verification. Consequently, inference is evaluated in a time-multiplexed fashion, in which the single physical device emulates the different RNPUs in the KAN. The output of each RNPU is measured sequentially by applying the corresponding input and control voltages. 

Training and Verification 

All examples are realized using in-silico training, i.e., a surrogate RNPU model is used to train the full KAN off-chip. The surrogate model consists of a seven-input, single output MLP with ReLU activations and five hidden layers containing 90 neurons each. Both aKANs and MLPs are trained using backpropagation with the Adam optimizer, with learning rates ranging from 10 −3 to 10 −2. Initialization includes both random parameter initialization and random selection of the input electrode for each RNPU in the aKAN. For regression tasks, we use a batch of 1,000 input samples uniformly distributed across the voltage range of the input electrode for training. For classification tasks, we scale the data to the RNPU operating range and split the data into 80% training and 20% validation sets. The Spirals dataset is generated using a custom script that includes both noise (noise = 1.0 in this work) and a configurable number of turns. 21 Configurations considered in the approximation examples 

The simulation results in Fig. 2D–F include multiple configurations and five random ini-tializations per configuration. The selected configurations are summarized in Tables 1 and 2. 

Table 1 : Simulated aKAN configurations for the function approx-imation task. 

Function Networks RNPUs per EP Fig. 2D [1,1] 5, 10, 15, 20, 25 30, 35, 40, 45, 50 Fig. 2E [2,1,1], [2,2,1], [2,3,1], [2,4,1], [2,5,1] 1, 2, 3, 4, 5 [2,1,1,1], [2,2,2,1], [2,3,3,1], [2,4,4,1], [2,5,5,1] Fig. 2F [4,2,1], [4,3,1], [4,4,1], [4,1,1,1], [4,2,2,1], [4,3,3,1] 1, 2, 3 [4,4,4,1], [4,1,1,1,1], [4,2,2,2,1], [4,3,3,3,1], [4,4,4,4,1] 

Table 2 : Simulated MLP configurations for the function approximation task. 

Function Networks Fig. 2D [1,50,1], [1,100,1], [1,150,1], [1,200,1], [1,250,1], [1,300,1], [1,350,1], [1,400,1] Fig. 2E [2,5,1], [2,10,1], [2,20,1], [2,50,1], [2,100,1], [2,200,1], [2,300,1], [2,400,1], [2,500,1] Fig. 2F [4,50,1], [4,100,1],[4,150,1],[4,200,1],[4,250,1],[4,300,1],[4,10,10,1], [4,15,15,1], [4,20,20,1],[4,50,50,1],[4,200,200,1],[4,300,300,1],[4,400,400,1] 

Energy Estimation Details 

The total estimated aKAN energy is expressed as the sum of contributions of the main building blocks: 

ETOTAL = EADC + EDAC + ETIA + ERNPU .

For the evaluation of a function at P sample points, we explicitly count the conversions performed by the DACs and the ADC. The DACs set the control voltages once, while the inputs are converted for each sample. Therefore, the total energy consumed by the DACs during the function evaluation is 

EDAC = NIN EACONV P + NCONTROL ECONV ,

where EACONV is the energy per conversion of the selected DAC. The ADC converts each inference at EDCON V energy per conversion, which yields EADC = NOUT EDCONV P .22 The remaining energy contributions are computed from their estimated power and the inference latency td. More concretely, ETIA = NNODES PTIA td and 

ERNPU = NRNPU PRNPU td. The energy consumed by the programmable resistors (mem-ristors) is neglected relative to the TIA consumption. The inference latency is the sum of DAC, RNPU layers and ADC latencies, i.e., td = tDAC + tRNPU NLAYER + tADC 

23