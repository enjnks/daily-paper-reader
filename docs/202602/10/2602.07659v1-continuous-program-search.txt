Title: Continuous Program Search

URL Source: https://arxiv.org/pdf/2602.07659v1

Published Time: Tue, 10 Feb 2026 01:50:26 GMT

Number of Pages: 11

Markdown Content:
# Continuous Program Search 

## Matthew Siper 

Nof1 New York, NY, USA m@thenof1.com 

## Muhammad Umair Nasir 

Nof1 New York, NY, USA 2396876@students.wits.ac.za 

## Ahmed Khalifa 

Nof1 New York, NY, USA aak538@nyu.edu 

## Lisa Soros 

Nof1 New York, NY, USA lisa.soros@gmail.com 

## Jay Azhang 

Nof1 New York, NY, USA j@thenof1.com 

## Julian Togelius 

Nof1 New York, NY, USA julian@togelius.com 0 20 40 60 80 100 120  

> % of Evaluation Budget Used
> Dual-block
> Isotropic
> Flow Matching
> 100.0%
> 88.5%
> 13.7% (7.3√ó faster)
> Computational Efficiency
> (Lower = More Efficient)
> Full Budget

(a) Evaluation budget required to discover strong solutions. Dual-block  

> (n=100)
> Isotropic
> (n=100)
> Flow Matching
> (n=100)
> 0.0
> 0.5
> 1.0
> 1.5
> 2.0
> Final Sharpe Ratio
> Final Performance Comparison

(b) Final out-of-sample Sharpe distributions. 

Figure 1: Geometry-aware mutation improves search efficiency and typical performance under a fixed optimizer. Both panels compare mutation operators under the same (ùúá + ùúÜ ) evolution strategy and identical evaluation budgets. (Left) shows that geometry-compiled flow-based mutation reaches high-quality strategies using an order of magnitude fewer evaluations than isotropic and dual-block Gaussian baselines. (Right) reports final out-of-sample Sharpe distributions at the full budget, where the learned operator achieves the highest median performance while remaining competitive in peak outcomes. 

## Abstract 

Genetic Programming yields interpretable programs, but small syntactic mutations can induce large, unpredictable behavioral shifts, degrading locality and sample efficiency. We frame this as an operator-design problem: learn a continuous program space where latent distance has behavioral meaning, then design mutation oper-ators that exploit this structure without changing the evolutionary optimizer. We make locality measurable by tracking action-level divergence under controlled latent perturbations, identifying an empirical trust region for behavior-local continuous variation. Using a compact trading-strategy DSL with four semantic components (long/short entry and exit), we learn a matching block-factorized embedding 

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. 

Conference‚Äô17, Washington, DC, USA 

¬© 2026 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YYYY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn 

and compare isotropic Gaussian mutation over the full latent space to geometry-compiled mutation that restricts updates to semanti-cally paired entry‚Äìexit subspaces and proposes directions using a learned flow-based model trained on logged mutation outcomes. Under identical (ùúá + ùúÜ ) evolution strategies and fixed evaluation budgets across five assets, the learned mutation operator discovers strong strategies using an order of magnitude fewer evaluations and achieves the highest median out-of-sample Sharpe ratio. Although isotropic mutation occasionally attains higher peak performance, geometry-compiled mutation yields faster, more reliable progress, demonstrating that semantically aligned mutation can substantially improve search efficiency without modifying the underlying evolu-tionary algorithm. 

## Keywords 

Genetic Programming, Evolutionary Search, Latent Space Optimiza-tion, Disentanglement, Algorithmic Trading. 

ACM Reference Format: 

Matthew Siper, Muhammad Umair Nasir, Ahmed Khalifa, Lisa Soros, Jay Azhang, and Julian Togelius. 2026. Continuous Program Search. In . ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn  

> arXiv:2602.07659v1 [cs.LG] 7 Feb 2026 Conference‚Äô17, July 2017, Washington, DC, USA Siper et al.

## 1 Introduction 

Symbolic programs are a natural representation for policies and de-cision rules in program synthesis, control, and algorithmic trading: they are interpretable, compositional, and amenable to grammati-cal and type-based constraints. A persistent challenge for Genetic Programming (GP), however, is locality . Small syntactic edits‚Äîsuch as operator changes or subtree replacements‚Äîcan induce large, unpredictable behavioral shifts, degrading stability, credit assign-ment, and sample efficiency. As a result, evolutionary search often expends substantial evaluation budget before discovering useful solutions. Embedding programs into a continuous search space offers a potential remedy, as it enables incremental variation through small latent perturbations. However, continuous search is only effec-tive when the representation provides a reliable correspondence between latent distance and behavioral change . When this link is weak, even powerful optimizers struggle: inefficiency arises not from the evolutionary loop itself, but from a mismatch between how variation is applied and how program behavior responds. We therefore focus on latent behavioral geometry : the relation-ship between movements in latent space and changes in decoded program behavior. Continuous variation is meaningful only within regions where small perturbations produce small, predictable be-havioral effects. Outside these regions, decoding failures and unin-tended cross-talk between program components can render search unstable and inefficient. Understanding and exploiting this geome-try is essential for designing effective mutation operators. Our approach is to make latent behavioral geometry explicit and measurable, then leverage it to design mutation operators under a 

fixed evolutionary algorithm. We first characterize locality using controlled latent perturbations, measuring (i) decode validity, (ii) structural change via normalized AST edit distance, and (iii) be-havioral change via action-sequence divergence. Together, these diagnostics identify an empirical trust region in which continuous mutations remain valid and behavior-local. We then exploit this structure using a compact, typed trading-strategy DSL that decomposes each strategy into four semantic com-ponents: long entry (LE), short entry (SE), long exit (LX), and short exit (SX). We learn continuous embeddings with a transformer-based VAE whose latent representation is explicitly factorized along these components. Perturbation and swap tests (Section 6) verify that this factorization provides reliable, low cross-talk control over decoded behavior. Rather than mutating all latent dimensions simultaneously, we design structured mutation operators that restrict updates to se-mantically meaningful subspaces. In particular, we exploit the di-rectional structure of trading strategies by pairing long entry with long exit and short entry with short exit. These paired subspaces correspond to coherent trade lifecycles and enable targeted explo-ration that avoids incoherent offspring which simultaneously alter unrelated signals. Finally, we ask whether mutation quality can be improved within 

these semantically constrained subspaces. Instead of sampling isotropic noise, we learn a geometry-compiled mutation operator that pre-dicts behavior-improving update directions from logged evolution-ary traces. The resulting flow-based mutation model proposes up-dates in a single forward pass and is applied only through direction-paired masks. Crucially, it serves as a drop-in replacement for stan-dard mutation: the evolutionary loop, selection mechanism, and evaluation budget remain unchanged. Across multiple assets and fixed evaluation budgets, this learned operator consistently discovers strong strategies using substantially fewer evaluations and achieves higher median out-of-sample per-formance than unstructured isotropic mutation. While unrestricted mutation occasionally attains higher peak outcomes, geometry-compiled mutation prioritizes efficiency and reliability, yielding faster and more robust evolutionary search. 

Contributions. This paper makes the following contributions: 

‚Ä¢ Behavioral geometry diagnostics for program latent spaces. 

We introduce practical measurements of decode validity, struc-tural change, and behavioral divergence to identify trust regions for behavior-local continuous variation. 

‚Ä¢ A symbolic trading DSL enabling semantically aligned embeddings. We design a closed, typed language whose de-composition into entry and exit signals supports interpretable, component-aligned latent representations. 

‚Ä¢ Structured mutation operators under a fixed optimizer. We show that restricting mutation to semantically paired subspaces improves search efficiency and reliability relative to isotropic full-latent mutation, without modifying the evolutionary algorithm. 

‚Ä¢ A learned geometry-compiled mutation operator. We pro-pose a flow-based mutation model trained on logged evolutionary traces that achieves faster discovery and higher median perfor-mance under identical (ùúá + ùúÜ ) evolution strategies and budgets. Together, these results demonstrate that aligning mutation struc-ture with latent behavioral geometry can substantially improve the efficiency and robustness of continuous program search without changing the underlying evolutionary algorithm. 

## 2 Related Work 

Our work sits at the intersection of four areas. These are Genetic Programming and symbolic policy search, continuous and latent-space evolution, disentangled representation learning, and learned variation operators. Across these areas, many methods rely on some notion of locality. Our focus is to make locality behavioral, measurable, and directly usable for operator design. 

## 2.1 Genetic Programming and Symbolic Policy Search 

Genetic Programming (GP) has a long history of producing inter-pretable programs for program synthesis, control, and decision-making [ 12 , 18 ]. A persistent challenge is locality. Small syntactic edits, such as replacing an operator or swapping a subtree, can cause large and hard-to-predict changes in behavior or fitness. This can make evolution unstable and sample-inefficient as programs grow. Many GP systems address this by constraining or structuring the representation. Examples include automatically defined func-tions [ 13 ], grammar-guided GP [ 23 ], and strongly typed GP [ 16 ]. Continuous Program Search Conference‚Äô17, July 2017, Washington, DC, USA 

These methods improve validity and can make certain edits safer, but the search space remains discrete, and the behavioral meaning of a small syntactic edit is still difficult to quantify. Other lines of work attack locality more directly through seman-tics. Geometric Semantic GP defines operators in semantic space to induce smoother fitness landscapes [ 17 ]. Related work also uses semantic information to shape operators and improve search dy-namics [ 19 ]. These approaches highlight an important point. Better locality often comes from aligning variation with what the program does, not only with how it is written. Our approach targets the same locality problem, but through a continuous representation that we evaluate in behavioral terms. In-stead of proposing a new GP optimizer, we hold the optimizer fixed and study how representation geometry and mutation structure affect behavior. We then use measured latent-to-behavior relation-ships to design mutation operators that are targeted, behavior-local, and easy to plug into standard evolutionary loops. 

## 2.2 Continuous and Latent-Space Evolution 

Evolving solutions in continuous spaces is common in neuroevolu-tion and black-box optimization [ 6, 22 ]. Evolution strategies and related methods can be effective when small parameter changes lead to reasonably smooth changes in outcomes. Widely used ex-amples include OpenAI-ES [ 20 ] and CMA-ES [ 7]. A few recent papers explore embedding programs into continuous spaces and then optimizing those embeddings. Neural Program Optimization (NPO) trains an autoencoder for programs and uses a continuous optimizer in the latent space, decoding candidates back to programs during search [ 14 ]. Lynch et al. [ 15 ] combine grammars with a vari-ational autoencoder to construct a continuous space of programs, then apply evolutionary search in that space. These papers demon-strate that latent-space search can work for program synthesis and related tasks [ 3]. They also motivate a key practical question. What does a small latent move actually do to the resulting program? Our work makes this question explicit and central. We do not treat the latent space as a black box. We measure decode valid-ity under perturbation, structural change under perturbation, and behavioral change under perturbation. This yields an empirical trust region that defines when continuous variation is behaviorally meaningful. We then hold the optimizer fixed and change only the mutation operator. This isolates the effect of representation geometry and operator design from optimizer tuning. 

## 2.3 Disentangled Representations and Structured Latents 

Disentangled representation learning aims to separate underlying factors of variation in a way that supports independent control [ 2 ]. In VAEs, common approaches include objectives that encourage factor separation, such as ùõΩ -VAE [ 8] and FactorVAE-style methods [ 11 ]. The literature also proposes metrics to quantify disentangle-ment. Examples include DCI [ 5 ] and MIG [ 4 ], along with critiques that show metrics can disagree and can be brittle in practice [1]. In many optimization settings, disentanglement is treated as a general quality of the embedding. The hope is that a better repre-sentation will make search easier. In our setting, we use a more operational notion that is directly tied to mutation. We build an ex-plicitly block-factorized latent aligned to the four semantic strategy parts in our DSL. We then test whether perturbing one latent block changes only the corresponding decoded program part, including targeted perturbations and swap tests. This turns disentanglement into a practical resource for operator design rather than a purely descriptive score. We do not claim full statistical independence. Our goal is controllable, behavior-local edits that are useful for evolution. 

## 2.4 Learning Variation Operators and Generative Mutation Models 

Learning better variation is a long-standing theme in evolution-ary computation. Model-based GP and estimation-of-distribution ideas replace hand-designed mutation with learned sampling mech-anisms. A recent example is Denoising Autoencoder Genetic Pro-gramming (DAE-GP). It uses a denoising autoencoder model to generate new candidate programs, and it studies how the corrup-tion process controls exploration and exploitation [ 24 ]. Mutation Models is an earlier approach by Khalifa et al. [ 10 ] where the muta-tion function is learned during evolution. This is done by training a small network to learn the mutation function using the evolution-ary history while evolution is happening, and then reusing it as the mutation function itself to help direct the search. We do not learn to generate whole programs directly. Instead, we learn a conditional distribution over latent perturbations inside a measured trust region. This keeps program validity in the existing decoder and makes the learned model a drop-in replacement for a mutation kernel. Concretely, we train a conditional flow-based mutation model [ 9, 21 ] that proposes targeted block-level latent deltas. 

## 3 Genetic Programming Trading Language (GPTL) 

To study continuous program search in a controlled and repro-ducible way, we define the Genetic Programming Trading Language 

(GPTL). GPTL is a small domain-specific language for expressing algorithmic financial trading strategies as symbolic programs. It serves two purposes. First, it provides a GP search space with strong safety guarantees. Second, its structure cleanly separates a strat-egy into meaningful parts, which later supports a block-factorized continuous representation and block-wise mutation. 

## 3.1 Language Design Goals 

GPTL was designed with five goals in mind. First, it must be expres-sive enough to represent a broad set of common technical trading rules, including indicator-based entry and exit logic (a statistical rule-based system that uses price data determine when to buy or sell trades). Second, every generated program must be syntacti-cally and semantically valid. This allows large-scale search without runtime failures caused by malformed programs. Third, program execution must be deterministic and bounded in time and memory. Fourth, programs must have a compact representation that can be embedded by sequence models. Fifth, and most important for this paper, the language must expose a modular strategy structure that makes it possible to treat different parts of a strategy independently Conference‚Äô17, July 2017, Washington, DC, USA Siper et al. 

during analysis and mutation. To satisfy this final goal, each GPTL strategy is decomposed into four Boolean signal expressions: 

‚Ä¢ Long Entry (LE): if true, buy-to-enter a long position. 

‚Ä¢ Short Entry (SE): if true, sell-to-enter a short position. 

‚Ä¢ Long Exit (LX): if true, sell-to-close an existing long posi-tion. 

‚Ä¢ Short Exit (SX): if true, buy-to-close an existing short posi-tion. This decomposition mirrors the lifecycle of a trading position. It also gives us four clearly defined semantic components that can be represented as separate subtrees and, later, as separate latent blocks. 

## 3.2 Syntax, Grammar, and Type System 

A GPTL signal is a Boolean expression. Boolean expressions are built from comparisons between numeric expressions, then com-bined with logical operators. Numeric expressions are formed from three sources. These are price fields, technical indicator calls, and constants. We use a closed, context-free grammar, which means programs can be parsed deterministically, and no out-of-grammar constructs are allowed. GPTL uses a simple static type system with two primitive types. The Numeric type includes price fields, constants, and indicator outputs. The Boolean type includes com-parison results and logical expressions. Relational operators map 

Numeric inputs to Boolean . Logical operators take Boolean inputs. No implicit type conversions are allowed. All operators have fixed arity and fixed type signatures. All programs are fully parenthe-sized during serialization to remove ambiguity. During generation and mutation, we enforce bounds on maximum tree depth and minimum structural complexity. These choices guarantee closure under the language rules. Any generated or mutated program is type-correct, syntactically valid, and executable. 

## 3.3 Program Semantics and Execution Model 

Programs are evaluated over discrete time series containing OHLCV fields (Open, High, Low, Close, and Volume) and precomputed technical indicators. These fields and values give you an indica-tor of the current state of the financial market at the given time. At each timestep, the four signals (LE, SE, LX, SX) are evaluated independently to Boolean values. Trading follows a deterministic event-driven model. A long (short) position is opened when the corresponding entry signal is true and no long (short) position is already open. A position is closed when the corresponding exit signal is true. If multiple signals fire at the same timestep, fixed priority rules ensure deterministic behavior. All orders execute at the next-bar open. Given the same market data and parameters, evaluation is fully deterministic. 

## 3.4 Abstract Syntax Tree Representation 

Internally, each GPTL strategy is represented as a typed abstract syntax tree (AST). Leaf nodes correspond to numeric terminals such as price fields and constants. Internal nodes correspond to indicator calls, relational operators, and logical operators. The four signal ex-pressions are stored as four disjoint subtrees under dedicated signal roots. This enables modular discrete mutation in program space and later enables a block-structured continuous representation aligned to the same four components. 

## 3.5 Discrete Mutation Operators 

GPTL includes a set of type-preserving mutation operators de-fined over the AST. These operators include subtree replacement, operator mutation, terminal mutation, and insertion or deletion (subjected to depth constraints). All operators preserve syntactic validity, type correctness, and grammar closure by construction. Mutations are local, bounded, and well-defined. They provide a baseline for discrete program-space evolutionary search. They also serve as a reference point for the continuous latent-space mutation operators studied later, which allows us to isolate the effects of representation geometry from the effects of the optimizer. 

## 4 Learning Continuous Program Embeddings 

To search over programs using continuous mutation, we need a continuous representation of a GPTL strategy. We learn this repre-sentation by training a transformer-based variational autoencoder (VAE) that maps a symbolic strategy to a real-valued latent vector and back again. The model is designed to match the structure of GPTL. Each strategy is composed of four signals (LE, SE, LX, SX), and our latent representation is explicitly organized into four cor-responding latent blocks. This gives us a representation that is easy to interpret at a high level and is directly usable for block-wise mutation during evolution. 

## 4.1 Transformer VAE Architecture 

Each GPTL strategy consists of four Boolean signal expressions. These are long entry (LE), short entry (SE), long exit (LX), and short exit (SX). We encode and decode each signal separately. Con-cretely, each signal expression is serialized into a token sequence and passed through a shared transformer encoder. The encoder outputs a latent distribution for that signal. We then concatenate the four signal latents to obtain the full strategy embedding. This process of dividing the latent is called latent factorization. Let ùëß ùëôùëí , ùëß ùë†ùëí , ùëß ùëôùë• , ùëß ùë†ùë• denote the latent vectors for the four signals. The full strategy latent is 

ùëß = [ùëß ùëôùëí , ùëß ùë†ùëí , ùëß ùëôùë• , ùëß ùë†ùë• ] ‚àà Rùëë total latent ,

where ùëë total latent = 4 √óùëë signal latent . This block structure aligns the representa-tion with the four semantic components of the strategy and enables block-wise mutation operators during evolutionary search. Token sequences are embedded into a ùëë model = 512 dimensional space and augmented with sinusoidal positional encodings. The encoder uses four transformer encoder layers. Each layer contains multi-head self-attention with eight heads and a feedforward network with hidden dimension 1024. Residual connections, layer normalization, and dropout with probability 0.1 are applied throughout. To pro-duce a single vector per signal, we aggregate the encoder outputs using masked mean pooling over non-padding tokens. This pooled vector is projected into the parameters of a diagonal Gaussian la-tent distribution (ùúá, log ùúé 2) for that signal. The decoder mirrors the encoder with four transformer decoder layers. Each signal latent vector is projected to the model dimension and provided to the decoder through cross-attention as a single-token memory. We Continuous Program Search Conference‚Äô17, July 2017, Washington, DC, USA 

decode autoregressively with causal masking to generate the to-ken sequence corresponding to the GPTL expression. A final linear projection maps decoder states to vocabulary logits. 

## 4.2 Tokenization and Serialization 

We use a grammar-aware tokenizer tailored to GPTL. The vocabu-lary includes special tokens (PAD, SOS, EOS, UNK), logical and rela-tional operators, price fields, indicator names, punctuation symbols, and discretized numeric tokens. Numeric values are represented with fixed tokens such as <NUM:20> drawn from a bounded set. All programs are fully parenthesized during serialization to ensure deterministic parsing and remove ambiguity. 

## 4.3 Training Procedure 

We generate a training set by randomized GPTL program synthesis. Each strategy contains four independently generated signal trees represented as ASTs. We use a maximum depth of eight and en-force minimum depth constraints and valid operator arity to avoid degenerate expressions. Each generated strategy is compiled and evaluated on historical market data. Strategies that fail to compile or that produce no trades in any walk-forward fold are discarded. This filtering step removes inactive or ill-formed strategies. It does not apply any selection pressure toward profitability or any specific trading behavior. The final dataset contains 20,000 valid strategies. We split it into 80% training and 20% validation using a fixed random seed per model replicate. 

Objective and optimization. We train the VAE with teacher forc-ing using a reconstruction loss plus a KL regularizer, 

L = Lrecon + ùõΩ LKL ,

where Lrecon is token-level cross-entropy averaged across all four signals, and 

LKL = ‚àí 12 E1 + log ùúé 2 ‚àí ùúá 2 ‚àí ùúé 2  .

We linearly anneal the KL weight ùõΩ from 0 to 0.1 over the first 50 training epochs to mitigate posterior collapse. We optimize using a weighted decay Adam optimizer (AdamW) with learning rate 10 ‚àí4 and weight decay 10 ‚àí5. We use cosine learning-rate annealing, gradient clipping at norm 1.0, and auto-matic mixed precision. All models are trained for 50 epochs with a batch size 128. We retain the checkpoint with the lowest validation loss. 

## 5 Latent Quality and Behavioral Geometry Diagnostics 

This section answers two questions. First, does the VAE provide a usable continuous representation of programs, and second, when we take a small step in latent space, what happens to the decoded program? These questions determine whether continuous mutation is meaningful and what mutation scales are safe. We tested the quality across different latent dimension sizes by training models with ùëë total latent ‚àà { 16 , 32 , 64 , 128 , 256 , 512 , 1024 , 2048 }, where for each latent dimension, we train five independent VAE models using different random initialization seeds. 0.01 0.03 0.05 0.1 0.2 0.3 0.5 0.7 1.0 2.0                  

> Perturbation Magnitude ( )
> 4
> 8
> 16
> 32
> 64
> 128
> 256
> 512
> Latent Dimension
> behavior-local breakdown
> 0.049 0.277 0.497
> 0.050 0.246 0.458
> 0.054 0.270 0.418
> 0.069 0.246 0.469
> 0.047 0.239 0.431
> 0.045 0.239 0.406
> 0.073 0.267 0.431
> 0.042 0.261 0.486
> 0.1
> 0.2
> 0.3
> 0.4
> 0.5
> 0.6
> 0.7
> Action Divergence (Normalized Hamming)

Figure 2: Action-sequence divergence between parent and perturbed strategies. Dark colors indicate behavior-local ed-its. For ùúñ ‚â§ 0.1, divergence remains low, defining a trust re-gion. Beyond ùúñ ‚â• 0.5, divergence increases sharply and de-coding often fails (gray). This boundary motivates mutation scales in later experiments. 

## 5.1 Latent Quality Across Dimensions 

We evaluate VAE quality across the latent-dimension sizes. For each latent size, we train five independent models and report aggregated results. Reconstruction accuracy measures strict round-trip fidelity, counted correctly only if all four signals match exactly after AST canonicalization. Normalized edit distance captures token-level dis-tance between original and reconstruction, averaged across signals. Validity is the fraction of prior samples that decode into executable programs. Uniqueness detects mode collapse; novelty detects mem-orization. Table 2 reveals a capacity trade-off: very small latents reconstruct poorly, very large latents decode less robustly, and in-termediate sizes provide the best balance. We use intermediate sizes in subsequent analyses. 

## 5.2 Locality and Behavioral Geometry 

We measure latent behavioral geometry by asking: if we perturb a latent vector by a controlled amount, how does it change the program‚Äôs behavior? For each held-out strategy, we encode using the posterior mean and apply an isotropic Gaussian perturbation, 

ùëß ‚Ä≤ = ùëß + ùúñùúÇ, ùúÇ ‚àº N ( 0, ùêº ),

with ùúñ ‚àà { 0.01 , 0.05 , 0.1, 0.5, 1.0, 1.5, 2.5, 3.0, 3.5, 5.0}. For each per-turbed latent, we measure three quantities: decode success (fraction producing valid programs), structural change (normalized AST edit distance), and behavioral change (action-sequence divergence on shared market data). Figure 2 reports behavioral divergence as a function of ùúñ and latent dimensionality. Decode success and behavioral locality re-main high for small perturbations ( ùúñ ‚â§ 0.1), indicating that each strategy has a neighborhood where decoding is stable, and behavior changes predictably. Beyond ùúñ ‚â• 0.5, decode success drops, struc-tural distance grows, and behavioral divergence increases sharply. We treat this transition as an empirical trust region for continuous search: inside it, small latent edits yield predictable behavioral re-finements; outside, mutations become unreliable. This motivates the geometry-aware operators in 7.4. Conference‚Äô17, July 2017, Washington, DC, USA Siper et al. 

## 6 Disentanglement and Latent Factorization 

The mutation operators in this paper rely on a simple idea. A strat-egy has four meaningful parts (LE, SE, LX, SX). Our latent rep-resentation is explicitly split into four corresponding blocks. For block-wise mutation to be reliable, changing one latent block should mainly change the matching strategy part, without unintentionally changing the other three parts. In this section, we test that this is true in practice. We treat la-tent factorization as the representation design. We treat signal-level disentanglement as an empirical property of the trained model. We present two direct tests. The first asks whether small changes to one latent block stay confined to the intended signal. The second asks whether we can swap blocks between two strategies and trans-fer only the corresponding signal. Together, these tests show that the representation supports predictable, component-level control, which is the key requirement for the mutation operators studied later. These tests both prove disentanglement and zero cross-talk, respectively, and are presented in D.2 

## 7 Geometry-Aware Mutation Operators under a Fixed Optimizer 

This section evaluates three mutation operators for continuous program search. All methods use the same evolutionary loop, the same evaluation budget, and the same program embedding model. The only difference is how new candidate latents are proposed. This isolates the effect of mutation design from optimizer choice. All flow-based mutations (GCM) are generated using the inference procedure described in Algorithm 1. 

## 7.1 Experimental Setup 

All methods use the same (ùúá + ùúÜ ) evolution strategy (ES) in the learned latent space. We fix ùúá = 34 and ùúÜ = 66 . Each run evaluates a total of 1320 offspring, which corresponds to 100 generations of ùúÜ 

offspring. Each candidate latent is decoded into a GPTL program and evaluated by deterministic backtesting. We report out-of-sample Sharpe on held-out test data under a walk-forward protocol ex-plained in the following section. We aggregate results over multiple independent seeds. All mutation operators act in the same 128-dimensional latent space. The isotropic baseline perturbs the full latent vector, while the proposed operator restricts updates to se-mantically paired entry‚Äìexit subspaces corresponding to long-side or short-side trading logic. 

## 7.2 Data Splits, Evaluation Protocol, and Trading Assumptions 

All experiments use a fixed five-fold walk-forward evaluation pro-tocol spanning 2008‚Äì2025. Each fold consists of approximately 2.5 years of training data, followed by a validation window of approx-imately 6.5 months and an out-of-sample test window of approx-imately 6 months. A strict 10-day embargo is enforced between consecutive splits to prevent lookahead bias. All reported results are based exclusively on test folds, and the same splits are used across all assets, mutation operators, and random seeds (see Table 3). Strategies are evaluated using a deterministic backtesting engine with an initial equity of $10,000. Only one position may be held at a time, and position scaling is not allowed. Orders execute at the next-bar open with a transaction slippage cost of 0.1% and exchange fees of 0.05% per trade. Experiments are conducted independently on five liquid futures contracts: S&P 500 (ES), Natural Gas (NG), Crude Oil (CL), Silver (SI), and Euro FX (E6). No model selection, hyperparameter tuning, or operator design choices use information from the test folds. 

## 7.3 Isotropic Gaussian Mutation 

The isotropic baseline applies Gaussian mutation uniformly across the entire latent space. Given a parent latent z ‚àà R128 , offspring are generated as 

z‚Ä≤ = z + ùúé ùùê , ùùê ‚àº N ( 0, I128 ),

so that all 128 latent dimensions are perturbed simultaneously without regard to semantic signal boundaries. This baseline reflects standard practice in latent-space evolutionary search and serves as an unstructured exploration reference. 

## 7.4 Geometry-Compiled Mutation via Dual-Block Directional Flow (GCM) 

We use the term geometry-compiled mutation (GCM) to refer to the general approach of learning behavior-aware mutation proposals from logged evolutionary traces; in this work, GCM is instantiated using a flow-matching model. The isotropic Gaussian baseline per-turbs all 128 latent dimensions simultaneously, without regard to trading semantics. We now introduce a learned mutation operator that restricts updates to semantically meaningful subspaces while improving proposal quality within those subspaces. We refer to this operator as geometry-compiled mutation (GCM). 

Dual-block directional mutation. Trading strategies exhibit direc-tional structure: long positions require coordinated entry and exit rules, as do short positions. Accordingly, we define two direction-paired mutation subspaces. Long Entry (LE) and Long Exit (LX) form the long-side pair, while Short Entry (SE) and Short Exit (SX) form the short-side pair. Let the latent be ordered as 

z = [z(LE ) , z(SE ) , z(LX ) , z(SX ) ] ‚àà R128 ,

with each block in R32 . We define two non-contiguous binary masks, 

mlong = [132 , 032 , 132 , 032 ], mshort = [032 , 132 , 032 , 132 ],

which activate the long-side and short-side subspaces respectively. At each generation, the mutation direction alternates deterministi-cally between these two masks. 

Learned mutation proposal. Rather than sampling isotropic noise within the active subspace, GCM uses a learned flow-matching model to predict a behavior-improving update direction. The model predicts a velocity field 

vùúÉ (z, ùùì ) ‚àà R128 ,

conditioned on the full parent latent z and an 8-dimensional behav-ioral embedding ùùì computed from the parent strategy‚Äôs execution trace. Unlike diffusion-based approaches, this formulation requires no iterative denoising or timestep conditioning: the update direc-tion is predicted in a single forward pass. Continuous Program Search Conference‚Äô17, July 2017, Washington, DC, USA 

Algorithm 1 DBD-Flow mutation (inference-time) within (ùúá + ùúÜ )-ES 

Require: Flow model ùêπ ùúÉ (z, ùùì ) ‚Üí ùúπ full (predicts a 128-dim im-provement delta) 

Require: Parent latent z ‚àà R128 , behavioral embedding ùùì ‚àà R8,generation index ùëî 

Require: Delta scale ùõº (default 1.0), input noise ùúé in (default 0.0), output noise ùúé out (default 0.0)

Require: Masks mlong = [132 , 032 , 132 , 032 ] and mshort =

[032 , 132 , 032 , 132 ]

Ensure: Mutated latent z‚Ä≤ ‚àà R128  

> 1:

Select direction-paired mask:  

> 2:

if ùëî mod 2 = 0 then  

> 3:

m ‚Üê mlong {Mutate (LE, LX)}  

> 4:

else  

> 5:

m ‚Üê mshort {Mutate (SE, SX)}  

> 6:

end if  

> 7:

Optional input exploration:  

> 8:

Sample ùùê ‚àº N ( 0, I128 ) 

> 9:

Àúz ‚Üê z + ùúé in ùùê  

> 10:

Predict full delta (one forward pass):  

> 11:

ùúπ full ‚Üê ùêπ ùúÉ ( Àúz, ùùì ) 

> 12:

Mask to the active direction-paired subspace:  

> 13:

ùúπ masked ‚Üê m ‚äô ùúπ full  

> 14:

Optional output noise (active blocks only):  

> 15:

Sample ùùê ‚Ä≤ ‚àº N ( 0, I128 ) 

> 16:

ùúπ ‚Üê ùõº ùúπ masked + ùúé out (m ‚äô ùùê ‚Ä≤) 

> 17:

Apply mutation:  

> 18:

z‚Ä≤ ‚Üê z + ùúπ  

> 19:

return z‚Ä≤

At search time, the predicted velocity is applied only through the active direction-paired mask, 

z‚Ä≤ = z + ùõº mùëë (ùëî ) ‚äô vùúÉ (z, ùùì ) + ùúº , ùúº ‚àº mùëë (ùëî ) ‚äô N ( 0, ùúé 2I),

where ùëë (ùëî ) ‚àà { long , short } denotes the active direction at genera-tion ùëî . Dual-block mutation updates 64 of the 128 latent dimensions per generation, focusing exploration on a coherent trade lifecycle rather than perturbing all signals simultaneously. 

Training data and usage. The flow model is trained offline using logged mutation traces collected under the same (ùúá + ùúÜ ) evolution strategy used in our main experiments, but with isotropic Gauss-ian mutation as the proposal mechanism. We collect traces across five assets and five independent runs per asset. Each run evaluates 

ùúÜ = 66 offspring per generation for 100 generations, yielding 6600 

mutation attempts per run. Each mutation record contains the par-ent latent z, the child latent z‚Ä≤, the associated behavioral embedding 

ùùì , validity flags, and parent and child fitness values. During evolu-tion, GCM is used solely as a drop-in replacement for the mutation kernel; the optimizer, evaluation budget, and selection procedure remain unchanged. Algorithm 1 summarizes the inference-time mutation procedure used by the geometry-compiled flow operator within the (ùúá + ùúÜ ) evolution strategy. 

Table 1: Median and maximum out-of-sample Sharpe ratios and evaluation budget usage across all assets. Lower values of 

Pct. Budget Used indicate faster discovery of strong solutions. 

Method Median Sharpe Max Sharpe Pct. Budget Used Flow (GCM) 1.152 1.518 13.7 

Isotropic 1.005 1.607 88.5 Dual-block 0.890 1.941 100.0 

## 7.5 Controlled Comparison 

We compare mutation operators under identical (ùúá + ùúÜ ) evolution strategy settings, evaluation budgets, and deterministic seeds to isolate the effect of mutation design. Specifically, we evaluate: (1) Isotropic Gaussian mutation , which perturbs all 128 latent dimensions simultaneously without regard to semantic struc-ture; (2) Dual-block Gaussian mutation , which restricts updates to semantically paired entry‚Äìexit subspaces (long-side or short-side) using isotropic noise; (3) Geometry-compiled mutation (GCM) , which uses a learned flow-based model to propose update directions within the same dual-block subspaces. All methods are evaluated across five assets using the same eval-uation budget. Results are reported exclusively on out-of-sample test folds. Table 1 summarizes median and maximum Sharpe ratios along with evaluation budget usage. 

## 7.6 Interpretation 

The results reveal a clear trade-off between unstructured explo-ration and semantically aligned mutation. Isotropic Gaussian mu-tation occasionally attains high peak Sharpe values, but typically requires most of the evaluation budget and exhibits high variance across runs. In contrast, restricting mutation to semantically mean-ingful entry‚Äìexit subspaces substantially improves search efficiency and reliability. The learned geometry-compiled mutation operator achieves the highest median out-of-sample Sharpe while using an order of mag-nitude less evaluation budget than both isotropic and dual-block Gaussian baselines. This indicates that learning behavior-aware mutation directions prioritizes consistent improvement over rare, high-variance outcomes. Although GCM does not always achieve the highest absolute Sharpe, it discovers strong strategies much earlier and with greater consistency across assets and seeds. Taken together, these results show that the primary benefit of geometry-aware mutation lies in faster and more reliable discovery rather than extreme peak optimization. 

## 8 Discussion 

This work examines continuous program search through the lens of 

latent behavioral geometry . Rather than assuming that continuous embeddings yield smooth or well-behaved search landscapes, we explicitly measure how latent perturbations translate into decoded program behavior. These measurements identify trust regions in Conference‚Äô17, July 2017, Washington, DC, USA Siper et al. 

which continuous mutation remains valid and behavior-local, pro-viding a principled basis for mutation operator design. Our results show that performance differences arise primarily from how mutation operators interact with this geometry, not from changes to the evolutionary optimizer. Unstructured isotropic mutation perturbs all signals simultaneously, often producing in-coherent edits and requiring large evaluation budgets. Restricting mutation to semantically aligned entry‚Äìexit subspaces substantially improves efficiency by focusing exploration on coherent trade life-cycles. Learning mutation directions within these subspaces further accelerates discovery by biasing search toward behaviorally mean-ingful changes. Importantly, the learned operator replaces only the mutation ker-nel. Selection, evaluation, and population dynamics are unchanged. The resulting gains therefore reflect more effective use of limited evaluation budgets rather than increased optimizer complexity or computational effort. While our experiments are conducted in a trading domain, the underlying principles are more general. Many GP systems rely on mutation operators whose behavioral effects are difficult to predict. Our results suggest that measuring latent behavioral geometry and aligning mutation structure with known semantic decompositions can significantly improve the efficiency and reliability of evolutionary search. There are limitations to this approach. Our DSL admits a clear semantic decomposition into entry and exit signals; other domains may require different representations. Continuous mutation re-mains effective only within trust regions that preserve validity, limiting exploration radius. Extending these ideas to domains with-out obvious semantic partitions remains an important direction for future work. 

## 9 Conclusion 

Continuous program embeddings are only useful when small la-tent changes correspond to small, interpretable behavioral effects. Rather than assuming this property, we measured it directly us-ing controlled perturbations that track decode validity, structural change, and behavioral divergence. These diagnostics identify trust regions where continuous mutation is reliable. We used this information to design semantically aligned mu-tation operators under a fixed evolutionary algorithm. Restrict-ing mutation to paired entry-exit subspaces improves efficiency and reliability relative to isotropic full-latent mutation. Learning geometry-compiled mutation directions within these subspaces fur-ther accelerates discovery, yielding higher median out-of-sample performance while using substantially less evaluation budget. The central takeaway is that aligning mutation structure with latent behavioral geometry can trade rare peak outcomes for faster, more robust evolutionary search‚Äîwithout modifying the underlying evo-lutionary algorithm. 

## References 

[1] Amir H Abdi, Purang Abolmaesumi, and Sidney Fels. 2019. A preliminary study of disentanglement with insights on the inadequacy of metrics. arXiv preprint arXiv:1911.11791 (2019). [2] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 8 (2013), 1798‚Äì1828. [3] Philip Bontrager, Aditi Roy, Julian Togelius, Nasir Memon, and Arun Ross. 2018. Deepmasterprints: Generating masterprints for dictionary attacks via latent variable evolution. In 2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS) . IEEE, 1‚Äì9. [4] Ricky T. Q. Chen, Xuechen Li, Roger B. Grosse, and David K. Duvenaud. 2018. Isolating Sources of Disentanglement in Variational Autoencoders. Advances in Neural Information Processing Systems 31 (2018). [5] Cian Eastwood and Christopher K. I. Williams. 2018. A Framework for the Quan-titative Evaluation of Disentangled Representations. In International Conference on Learning Representations .[6] Dario Floreano, Peter D√ºrr, and Claudio Mattiussi. 2008. Neuroevolution: From Architectures to Learning. Evolutionary Intelligence 1, 1 (2008), 47‚Äì62. [7] Nikolaus Hansen and Andreas Ostermeier. 2001. Completely Derandomized Self-Adaptation in Evolution Strategies. Evolutionary Computation 9, 2 (2001), 159‚Äì195. [8] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. In 

International Conference on Learning Representations .[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems , Vol. 33. [10] Ahmed Khalifa, Julian Togelius, and Michael Cerny Green. 2022. Mutation models: Learning to generate levels by imitating evolution. In Proceedings of the 17th International Conference on the Foundations of Digital Games . 1‚Äì9. [11] Hyunjik Kim and Andriy Mnih. 2018. Disentangling by Factorising. In Interna-tional Conference on Machine Learning . PMLR, 2649‚Äì2658. [12] John R. Koza. 1992. Genetic Programming: On the Programming of Computers by Means of Natural Selection . MIT Press, Cambridge, MA. [13] John R. Koza. 1994. Genetic Programming II: Automatic Discovery of Reusable Programs . MIT Press, Cambridge, MA. [14] Pawe≈Ç Liskowski, Krzysztof Krawiec, Nihat Engin Toklu, and Jerry Swan. 2020. Program Synthesis as Latent Continuous Optimization: Evolutionary Search in Neural Embeddings. In Proceedings of the 2020 Genetic and Evolutionary Compu-tation Conference . ACM, 359‚Äì367. doi:10.1145/3377930.3390213 [15] David Lynch, James McDermott, and Michael O‚ÄôNeill. 2020. Program Synthesis in a Continuous Space Using Grammars and Variational Autoencoders. In Parallel Problem Solving from Nature (PPSN XVI) (Lecture Notes in Computer Science, Vol. 12270) . Springer, 33‚Äì47. doi:10.1007/978-3-030-58115-2_3 [16] David J. Montana. 1995. Strongly Typed Genetic Programming. Evolutionary Computation 3, 2 (1995), 199‚Äì230. [17] Alberto Moraglio, Krzysztof Krawiec, and Colin G. Johnson. 2012. Geometric Semantic Genetic Programming. In Parallel Problem Solving from Nature (PPSN) .Springer, 21‚Äì31. [18] Michael O‚ÄôNeill. 2009. Riccardo Poli, William B. Langdon, Nicholas F. McPhee: A Field Guide to Genetic Programming: Lulu. com, 2008, 250 pp, ISBN 978-1-4092-0073-4. [19] Tomasz P. Pawlak, Bartosz Wieloch, and Krzysztof Krawiec. 2014. Semantic Backpropagation for Designing Search Operators in Genetic Programming. IEEE Transactions on Evolutionary Computation 19, 3 (2014), 326‚Äì340. [20] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. Evolution Strategies as a Scalable Alternative to Reinforcement Learning. arXiv preprint arXiv:1703.03864 (2017). [21] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In International Conference on Learning Representations .[22] Kenneth O. Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. 2019. De-signing Neural Networks through Neuroevolution. Nature Machine Intelligence 

1, 1 (2019), 24‚Äì35. [23] Peter A. Whigham. 1995. Grammatically-based Genetic Programming. In Pro-ceedings of the Workshop on Genetic Programming: From Theory to Real-World Applications .[24] David Wittenberg, Franz Rothlauf, and Christian Gagn√©. 2023. Denoising Autoen-coder Genetic Programming: Strategies to Control Exploration and Exploitation in Search. Genetic Programming and Evolvable Machines 24, 2 (2023), 17. 

## A Appendix 

The appendices provide full technical details necessary for repro-ducibility and to support the empirical claims made in the main paper. All experiments can be reproduced using the specifications, datasets, and hyperparameters described below. Continuous Program Search Conference‚Äô17, July 2017, Washington, DC, USA 

## B Full GPTL Grammar and Mutation Specifications B.1 Complete Grammar 

GPTL programs are composed of four Boolean signal expressions (LE, SE, LX, SX), each defined by the following context-free gram-mar: 

signal ::= expr expr ::= expr '&' expr | expr '|' expr | '~' expr | comparison comparison ::= numeric_expr relop numeric_expr relop ::= '>' | '<' | '>= ' | '<= ' | '== '

numeric_expr ::= indicator_call | field | constant indicator_call ::= IND '(' field ',' INT ')'

field ::= open | high | low | close | volume constant ::= FLOAT 

All expressions are fully parenthesized during serialization. Op-erator precedence is fixed as NOT > AND > OR. The grammar is closed and guarantees syntactic validity. 

## B.2 Static Typing Rules 

GPTL enforces a strict static type system with two primitive types: 

‚Ä¢ Numeric : price fields, constants, indicator outputs 

‚Ä¢ Boolean : comparison results and logical expressions Typing rules prohibit implicit coercions. All operators have fixed arity and type signatures, ensuring closure under mutation. 

## B.3 Mutation Operators 

All discrete mutation operators are type-preserving and grammar-safe: 

‚Ä¢ Subtree replacement: Replace a randomly selected subtree with a newly sampled compatible subtree. 

‚Ä¢ Operator mutation: Replace a logical or comparison oper-ator with another operator of the same arity and type. 

‚Ä¢ Terminal mutation: Resample numeric constants or indi-cator parameters within predefined bounds. 

‚Ä¢ Insertion/deletion: Insert or remove subtrees subject to maximum depth constraints. All mutations preserve syntactic validity, type correctness, and bounded execution. 

## B.4 Program Dataset Generation 

The GPTL training dataset is generated via randomized program synthesis. Each strategy consists of four independently generated signal trees with: 

‚Ä¢ maximum AST depth: 8 

‚Ä¢ minimum depth: 2 

‚Ä¢ bounded numeric constants and indicator periods Programs are compiled and evaluated on historical data. Strate-gies that fail to compile or produce zero trades in any walk-forward fold are discarded. No selection pressure toward profitability is applied. 

## C Complete Latent Quality Tables 

Table 2 reports the full latent quality metrics across all evaluated latent dimensions, including reconstruction accuracy, normalized edit distance, consistency, validity, uniqueness, and novelty. Results are aggregated across five independently trained VAE models per dimension. 

Table 2: Complete latent quality metrics across latent dimen-sionalities.                                                                                                                                                       

> Latent Dim. Recon. Acc. Norm. Edit Dist. Consistency Validity Uniqueness Novelty 16 0.006 ¬±0.003 0.746 ¬±0.022 1.000 ¬±0.000 0.896 ¬±0.021 0.991 ¬±0.006 0.952 ¬±0.010 32 0.057 ¬±0.007 0.596 ¬±0.017 1.000 ¬±0.000 0.954 ¬±0.019 0.999 ¬±0.001 0.965 ¬±0.009 64 0.242 ¬±0.016 0.385 ¬±0.008 1.000 ¬±0.000 0.975 ¬±0.014 1.000 ¬±0.000 0.977 ¬±0.005 128 0.414 ¬±0.012 0.289 ¬±0.007 1.000 ¬±0.000 0.978 ¬±0.007 1.000 ¬±0.000 0.974 ¬±0.005 256 0.446 ¬±0.017 0.278 ¬±0.009 1.000 ¬±0.000 0.942 ¬±0.025 1.000 ¬±0.000 0.983 ¬±0.008 512 0.496 ¬±0.008 0.280 ¬±0.003 1.000 ¬±0.000 0.926 ¬±0.021 1.000 ¬±0.000 0.991 ¬±0.007 1024 0.504 ¬±0.024 0.271 ¬±0.006 1.000 ¬±0.000 0.909 ¬±0.040 1.000 ¬±0.000 0.996 ¬±0.004 2048 0.519 ¬±0.010 0.270 ¬±0.003 1.000 ¬±0.000 0.893 ¬±0.029 1.000 ¬±0.000 1.000 ¬±0.000

## D Additional Disentanglement Visualizations 

This appendix includes additional heatmaps and visualizations sup-porting Section 6, including: 

‚Ä¢ Signal disentanglement heatmaps across multiple latent di-mensions 

‚Ä¢ Swap test results for all signal pairs 

‚Ä¢ Sensitivity of disentanglement metrics to perturbation scale All visualizations are computed using the same evaluation pro-tocols described in the main text. 

## D.1 Behavioral Embedding Œ¶ and Trust-Region Binning 

GCM conditions on a compact, interpretable behavioral embedding 

Œ¶(¬∑) ‚àà R8 computed from a strategy‚Äôs execution trace under deter-ministic backtesting. Let ùëá denote the number of evaluated bars in the backtest window. Let pos ùë° ‚àà {‚àí 1, 0, 1} denote the position at time ùë° , where 1 indicates long, ‚àí1 indicates short, and 0 indicates flat. 

Market regime. We define a simple market regime indicator using a 100-bar moving average of the close price. Let ùëê ùë° be the close at time ùë° and let MA 100 (ùëê )ùë° be the 100-bar moving average. We define 

ùëü ùë° = I [ùëê ùë° > MA 100 (ùëê )ùë° ] ,

where ùëü ùë° = 1 indicates an up regime and ùëü ùë° = 0 indicates a down regime. We compute all regime-based features over timesteps where MA 100 is defined. 

Regime exposure features. Define indicators ‚Ñìùë° = I[pos ùë° > 0]

and ùë† ùë° = I[pos ùë° < 0]. The first four components of Œ¶ measure the fraction of time spent in long or short positions during up or down regimes 

ùúô 1 = 1

ùëá 

> ùëá

‚àëÔ∏Å  

> ùë° =1

‚Ñìùë° (1 ‚àí ùëü ùë° ), ùúô 2 = 1

ùëá 

> ùëá

‚àëÔ∏Å  

> ùë° =1

ùë† ùë° (1 ‚àí ùëü ùë° ),Conference‚Äô17, July 2017, Washington, DC, USA Siper et al. 

ùúô 3 = 1

ùëá 

> ùëá

‚àëÔ∏Å 

> ùë° =1

‚Ñìùë° ùëü ùë° , ùúô 4 = 1

ùëá 

> ùëá

‚àëÔ∏Å 

> ùë° =1

ùë† ùë° ùëü ùë° .

Event statistics. We define an entry event as a transition from flat to non-flat. We define an exit event as a transition from non-flat to flat. Let ùëÅ entry be the number of entry events and ùëÅ exit be the number of exit events. We define normalized entry and exit rates 

ùúô 5 = ùëÅ entry 

ùëá , ùúô 6 = ùëÅ exit 

ùëá .

We also compute hold durations in bars for each completed trade. Let {‚Ñéùëó }ùêΩ ùëó =1 be the set of hold durations for ùêΩ completed trades. We define normalized hold statistics 

ùúô 7 = mean ({ ‚Ñéùëó }) 

ùëá , ùúô 8 = std ({ ‚Ñéùëó }) 

ùëá .

Behavioral step size, ùúå bins, and trust region. Given a parent strat-egy and a decoded child strategy, we compute 

ùúô ùêø 2 = ‚à•Œ¶(child ) ‚àí Œ¶(parent )‚à• 2 .

We discretize ùúô ùêø 2 into a four-level bin ùúå using fixed thresholds 

ùúå =

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

tiny ùúô ùêø 2 ‚àà [ 0, 0.05 )

small ùúô ùêø 2 ‚àà [ 0.05 , 0.15 )

medium ùúô ùêø 2 ‚àà [ 0.15 , 0.35 )

large ùúô ùêø 2 ‚àà [ 0.35 , ‚àû) .

We define the behavioral trust region as ùúô ùêø 2 ‚â§ 0.35 . GCM is trained on mutations within this trust region so that the learned operator focuses on behavior-local edits. At inference time we typically request ùúå = small to remain within the empirically stable locality regime. 

## D.2 Signal Disentanglement Test 

The goal of this test is straightforward. We perturb one latent block and check whether the decoded program changes only in the matching signal expression. For a held-out set of encoded strategies, we use the block-factorized latent representation 

ùëß = [ùëß ùëôùëí , ùëß ùë†ùëí , ùëß ùëôùë• , ùëß ùë†ùë• ],

corresponding to long entry (LE), short entry (SE), long exit (LX), and short exit (SX). For each strategy and each latent block ùëß ùëò , we perturb only that block, 

ùëß ‚Ä≤ 

> ùëò

= ùëß ùëò + ùúñùúÇ, ùúÇ ‚àº N ( 0, ùêº ),

and keep the other three blocks fixed. We use ùúñ = 0.1. This value lies within the trust-region regime identified in Section 5.2. Each perturbed latent is decoded using greedy decoding. For each per-turbation, we compute the normalized token-level edit distance between the original and perturbed programs separately for each of the four signals. From these per-signal edit distances, we compute two summary metrics. Cross-talk is the fraction of total observed change that appears in non-target signals. The target-only rate is the fraction of perturbations for which all observed changes are confined to the intended signal. Figure 3 shows the average per-signal edit distance produced by perturbing each latent block. The result is strongly diagonal. Perturbing ùëß ùëôùëí changes LE, perturbing LE SE LX SX               

> Measured Signal ( )
> LE
> SE
> LX
> SX
> Perturbed Latent (z)
> 0.07 0.00 0.00 0.00
> 0.00 0.15 0.00 0.00
> 0.00 0.00 0.05 0.00
> 0.00 0.00 0.00 0.16
> Signal-Factor Disentanglement
> ( =0.3, latent dim = 128)
> 0.0
> 0.2
> 0.4
> 0.6
> 0.8
> 1.0 Normalized AST Distance

Figure 3: Signal-level disentanglement under single-block perturbations ( ùúñ = 0.3, per-signal latent dimension = 32). Rows indicate which latent block is perturbed. Columns in-dicate which program signal changes. Color shows the nor-malized token edit distance. Perturbations affect only the intended signal, with negligible cross-talk. 

ùëß ùë†ùëí changes SE, and so on. Off-diagonal effects are negligible. Cross-talk is effectively zero, and the target-only rate is 100% across all evaluated signals. These results show that the learned representation provides reli-able component-level control. Each latent block governs a distinct semantic part of the strategy. Continuous Program Search Conference‚Äô17, July 2017, Washington, DC, USA 

Table 3: Walk-forward train, validation, and test folds used for all experiments. All results are reported on out-of-sample test folds with a 10-day embargo between splits to prevent lookahead bias. 

Fold Train Start Train End Val Start Val End Test Start Test End Train Days Val Days Test Days 1 2008-01-01 2010-06-30 2010-06-30 2011-01-11 2011-01-21 2011-07-25 910 195 185 2 2011-07-25 2014-01-20 2014-01-20 2014-08-03 2014-08-13 2015-02-14 910 195 185 3 2015-02-14 2017-08-12 2017-08-12 2018-02-23 2018-03-05 2018-09-06 910 195 185 4 2018-09-06 2021-03-05 2021-03-05 2021-09-16 2021-09-26 2022-03-30 910 195 185 5 2022-03-30 2024-09-25 2024-09-25 2025-04-08 2025-04-18 2025-10-20 910 195 185