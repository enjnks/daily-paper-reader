---
title: Foundation Inference Models for Ordinary Differential Equations
title_zh: 常微分方程的基础推理模型
authors: "Maximilian Mauel, Johannes R. Hübers, David Berghaus, Patrick Seifner, Ramses J. Sanchez"
date: 2026-02-09
pdf: "https://arxiv.org/pdf/2602.08733v1"
tags: ["keyword:SR", "query:SR"]
score: 6.0
evidence: 在ODE推断中与符号回归进行对比
tldr: 针对从噪声轨迹中推断常微分方程（ODE）向量场的难题，本文提出了 FIM-ODE。这是一种预训练的基础推理模型，通过在低阶多项式向量场分布上进行预训练，实现了仅需单次前向传播即可从噪声数据中直接预测向量场。FIM-ODE 在零样本任务中表现优异，且通过微调能快速适应复杂系统，在性能和效率上均优于传统的符号回归和神经 ODE 方法。
motivation: 现有的 ODE 推断方法通常需要复杂的训练流程或深厚的专业知识，且对系统先验依赖性强，难以高效处理噪声数据。
method: 提出 FIM-ODE 模型，利用神经算子在低阶多项式 ODE 先验分布上进行预训练，实现从噪声轨迹到向量场的直接映射。
result: FIM-ODE 在零样本测试中达到或超过了 ODEFormer 等基准，且微调后的性能优于现代神经和高斯过程基准。
conclusion: FIM-ODE 证明了基础模型在低维 ODE 推断中的有效性，为科学建模提供了一种无需复杂调优的高效、稳定工具。
---

## 摘要
常微分方程（ODEs）是科学建模的核心，但从含噪声的轨迹中推断其向量场仍然具有挑战性。目前的方法，如符号回归、高斯过程（GP）回归和神经常微分方程（Neural ODEs），通常需要复杂的训练流程和大量的机器学习专业知识，或者强烈依赖于特定系统的先验知识。我们提出了 FIM-ODE，这是一种预训练的基础推理模型，它通过在单次前向传递中直接从含噪声的轨迹数据预测向量场，从而实现了低维 ODE 推理的摊销。我们在具有低阶多项式向量场的 ODE 先验分布上预训练 FIM-ODE，并使用神经算子表示目标场。FIM-ODE 展现了强大的零样本性能，在多种场景下，尽管使用了更简单的预训练先验分布，其表现仍能与最近的预训练符号基准模型 ODEFormer 持平，且往往更优。预训练还为微调提供了强大的初始化，实现了快速且稳定的自适应，在无需机器学习专业知识的情况下，其性能优于现代神经和 GP 基准模型。

## Abstract
Ordinary differential equations (ODEs) are central to scientific modelling, but inferring their vector fields from noisy trajectories remains challenging. Current approaches such as symbolic regression, Gaussian process (GP) regression, and Neural ODEs often require complex training pipelines and substantial machine learning expertise, or they depend strongly on system-specific prior knowledge. We propose FIM-ODE, a pretrained Foundation Inference Model that amortises low-dimensional ODE inference by predicting the vector field directly from noisy trajectory data in a single forward pass. We pretrain FIM-ODE on a prior distribution over ODEs with low-degree polynomial vector fields and represent the target field with neural operators. FIM-ODE achieves strong zero-shot performance, matching and often improving upon ODEFormer, a recent pretrained symbolic baseline, across a range of regimes despite using a simpler pretraining prior distribution. Pretraining also provides a strong initialisation for finetuning, enabling fast and stable adaptation that outperforms modern neural and GP baselines without requiring machine learning expertise.