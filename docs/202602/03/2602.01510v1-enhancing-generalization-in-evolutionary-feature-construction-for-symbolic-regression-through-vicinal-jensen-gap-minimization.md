---
title: Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization
title_zh: 通过邻域 Jensen 间隙最小化增强符号回归中演化特征构建的泛化能力
authors: "Hengzhe Zhang, Qi Chen, Bing Xue, Wolfgang Banzhaf, Mengjie Zhang"
date: 2026-02-02
pdf: "https://arxiv.org/pdf/2602.01510v1"
tags: ["keyword:SR", "query:SR"]
score: 10.0
evidence: 直接针对符号回归的进化特征构建
tldr: 针对符号回归中遗传编程特征构造的过拟合问题，本文提出一种基于邻域Jensen间隙最小化的进化框架。研究证明了邻域风险可分解为经验风险与正则项之和，并据此设计了动态噪声估计和流形侵入检测机制。实验表明，该方法在58个数据集上优于15种主流算法，显著提升了模型的泛化能力。
motivation: 遗传编程在特征构造中容易出现过拟合，限制了其在自动化机器学习中的广泛应用。
method: 提出一种联合优化经验风险与邻域Jensen间隙的进化框架，并引入噪声估计和流形侵入检测机制。
result: 在58个数据集上的实验证明，该方法在泛化性能上优于其他复杂性度量指标及15种主流机器学习算法。
conclusion: 通过最小化邻域Jensen间隙并结合动态正则化策略，可以有效控制符号回归特征构造中的过拟合，提升模型泛化性。
---

## 摘要
近年来，基于遗传编程的特征构建作为一种增强学习性能的自动机器学习技术取得了显著成功。然而，过拟合仍然是限制其更广泛应用的一个挑战。为了提高泛化能力，我们证明了通过噪声扰动或基于 mixup 的数据增强所估计的邻域风险，受经验风险与正则化项（有限差分或邻域 Jensen 间隙）之和的约束。利用这种分解，我们提出了一种演化特征构建框架，该框架共同优化经验风险和邻域 Jensen 间隙以控制过拟合。由于数据集的噪声水平可能各不相同，我们开发了一种噪声估计策略来动态调整正则化强度。此外，为了减轻流形侵入（即数据增强可能会产生超出数据流形的不真实样本），我们提出了一种流形侵入检测机制。在 58 个数据集上的实验结果表明，与其他复杂度度量相比，Jensen 间隙最小化具有有效性。与 15 种机器学习算法的对比进一步表明，采用所提过拟合控制策略的遗传编程实现了卓越的性能。

## Abstract
Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

---

## 论文详细总结（自动生成）

这篇论文提出了一种通过最小化“邻域 Jensen 间隙”（Vicinal Jensen Gap）来增强遗传编程（GP）在符号回归特征构建中泛化能力的新框架。以下是对该论文的深度结构化总结：

### 1. 核心问题与研究背景
*   **核心问题**：基于遗传编程（GP）的特征构建（Feature Construction, FC）在自动化机器学习中表现出色，但面临严重的**过拟合**挑战。
*   **研究动机**：传统的过拟合控制方法（如限制模型大小/节点数）无法充分描述函数的语义复杂度（例如，无法区分 $x_1 \times x_2$ 与 $\sin(\sin(x))$ 的复杂度差异）。
*   **背景**：深度学习中常用的“邻域风险最小化”（VRM）通过数据增强提升泛化性，但其在回归任务和演化计算中的理论分解与应用尚不成熟。

### 2. 方法论
论文的核心思想是将邻域风险（Vicinal Risk）分解为经验风险和正则化项，并将其引入 GP 的多目标优化过程中。
*   **理论证明**：
    *   证明了基于**噪声扰动**的 VRM 可分解为“经验风险 + 有限差分（Finite Difference）”。
    *   证明了基于 **Mixup（线性插值）** 的 VRM 可分解为“经验风险 + 邻域 Jensen 间隙”。
*   **关键技术细节**：
    *   **VJM-GP 框架**：采用多目标优化（NSGA-II），同时最小化留一交叉验证（LOOCV）损失和邻域 Jensen 间隙。
    *   **邻域 Jensen 间隙计算**：通过对训练样本进行 Mixup 插值，计算模型预测值与插值标签之间的偏差，以此衡量函数的局部平滑度。
    *   **噪声估计策略**：利用极随机树（Extra Trees）在训练集上的 $R^2$ 评分来估计数据集噪声，动态调整正则化权重 $\tau$。
    *   **流形侵入检测**：引入检测机制，剔除那些偏离原始数据流形的虚假合成样本，防止过度惩罚有效的非线性模型。
    *   **高效评估**：利用线性回归的杠杆值（Leverage values）快速计算 LOOCV，无需重复训练。

### 3. 实验设计
*   **数据集**：使用了来自 PMLB（Penn Machine Learning Benchmark）的 **58 个真实世界回归数据集**。
*   **对比方法（复杂度度量）**：
    *   标准 GP、简约压制（Parsimony Pressure）、Tikhonov 正则化、Rademacher 复杂度、WCRV、IODC 等 8 种 GP 变体。
*   **对比方法（机器学习算法）**：
    *   与 15 种主流算法对比，包括 **XGBoost、LightGBM、Random Forest、MLP、SVR**，以及符号回归算法 **Operon、FFX** 和针对表格数据的深度学习模型 **ExcelFormer**。
*   **评价指标**：测试集 $R^2$ 评分、模型大小（树节点数）、训练时间。

### 4. 资源与算力
*   **算力说明**：论文未明确说明具体的 GPU 或 CPU 型号及数量。
*   **训练时长**：
    *   VJM-GP 的平均训练时间约为 **1041 秒**（约 17 分钟）。
    *   引入“早期停止”（Early Stopping）加速策略后，平均时间可缩短至 **381 秒**。
    *   相比之下，简单的简约压制（PP）方法耗时约 250 秒。

### 5. 实验数量与充分性
*   **实验规模**：每个算法在每个数据集上独立运行 **30 次**，以确保统计显著性。
*   **统计检验**：使用了 Wilcoxon 符号秩检验（显著性水平 0.01）和 Benjamini & Hochberg 校正。
*   **消融实验**：详细测试了流形侵入检测、噪声估计策略、不同选择算子（Lexicase vs. Tournament）以及超参数敏感性（如 Beta 分布的 $\alpha$ 值）。
*   **充分性评价**：实验设计非常充分，涵盖了从理论验证到实际算法对比的全过程，样本量大且遵循了符号回归领域的标准 Benchmark 协议。

### 6. 主要结论与发现
*   **泛化性能显著提升**：VJM-GP 在 58 个数据集中的 36 个上显著优于标准 GP，且在整体表现上优于所有对比的复杂度控制方法。
*   **优于主流 ML 算法**：在小样本场景下，VJM-GP 的测试 $R^2$ 评分优于 XGBoost 和 ExcelFormer 等强力基准。
*   **模型更简洁**：虽然 VJM 并不直接惩罚树的大小，但它隐式地倾向于生成更平滑、更简单的函数，中位模型大小从标准 GP 的 55.25 降至 20。
*   **Jensen 间隙 vs. 有限差分**：实验证明 Jensen 间隙作为正则项比有限差分更有效，因为它更能促进局部线性。

### 7. 优点
*   **理论支撑扎实**：通过数学证明将 VRM 与 Jensen 间隙联系起来，为 GP 的正则化提供了坚实的理论基础。
*   **动态自适应**：噪声估计策略解决了不同数据集需要不同正则化强度的问题，增强了算法的鲁棒性。
*   **语义关注**：超越了简单的结构惩罚，从函数行为（语义）层面控制复杂度。

### 8. 不足与局限
*   **计算开销**：由于需要对合成的邻域数据进行多次特征构建和预测，计算成本显著高于标准 GP。
*   **超参数依赖**：虽然对 $\alpha$ 不敏感，但对于 Mixup 的采样范围（Spread 参数 $\gamma$）和噪声扰动标准差 $\sigma$ 仍存在一定的依赖性。
*   **应用范围**：目前主要针对回归任务，在分类任务或其他复杂演化任务中的表现尚待验证。

（完）
