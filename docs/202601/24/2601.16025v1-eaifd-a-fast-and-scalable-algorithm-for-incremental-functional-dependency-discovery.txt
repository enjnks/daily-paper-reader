Title: EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery

URL Source: https://arxiv.org/pdf/2601.16025v1

Published Time: Fri, 23 Jan 2026 01:52:24 GMT

Number of Pages: 14

Markdown Content:
> 1

# EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery 

Yajuan Xu, Xixian Han, Xiaolong Wan 

Abstract —Functional dependencies (FDs) are fundamental in-tegrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremen-tal algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ( M HT ) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages M HT to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery. 

Index Terms —Database, Algorithm, Functional dependency, Incremental data 

I. I NTRODUCTION 

# FUNCTIONAL dependencies (FDs) are fundamental in-tegrity constraints in relational databases [1], which are widely applied in many areas, e.g., schema design and nor-malization [2], [3], query optimization [4], data cleaning [5] and data integration [6]. Formally, for a relation instance r of schema R, a functional dependency (FD) X → A is valid iff for any two tuples in r, if their values on X are the same, then they must have the same value on A, where X ⊆ R and A ∈ R

are referred to as left-hand side (LHS) and right-hand side (RHS), respectively. The FD X → A is non-trivial if A ∉ X,and minimal if there is no proper subset X′ ⊂ X such that 

X′ → A holds in r. FD discovery focuses on the complete set 

F of minimal and non-trivial FDs .

Example I.1. A student table is shown in Table 1 of Fig. 1. SName → SAge is invalid, as the tuple pairs ( s2

and s6) share the same name but have different ages. 

{SNO , SName } → SMajor is a valid but not minimal FD. And 

F = {SNO → SName , SNO → SAge , SNO → SGrade , SNO →

SMajor , SNO → SDorm , SMajor → SDorm }.

Yajuan Xu, Xixian Han and Xiaolong Wan are with the School of Computer Science and Technology, Harbin Institute of Technology, China. (e-mail: xuyajuan@stu.hit.edu.cn, hanxx@hit.edu.cn, wxl@hit.edu.cn) Manuscript received XX XX, XXXX; revised XX XX, XXXX. 

Fig. 1: The figure of two student information tables. Table 1 contains 

6 student information tuples, Table 2 contains 4 student information tuples, and the tuples in Table 2 are incrementally added to Table 1.

The real-life datasets typically grow continuously through the addition of tuples. For example, the modern newspaper database 1 adds at least 100 new newspaper titles annually. The Ncvoter dataset 2 is incrementally updated as North Carolina counties finalize voter histories after each election. The incre-mental updates of a database often lead to changes in F. As illustrated in Fig. 1, when new tuples from Table 2 are added to Table 1, previously valid FD SMajor → SDorm is invalid, due to the tuple pair (s6, s 9). {SGrade , SMajor } → SDorm becomes a new minimal valid FD. Traditional FD discovery algorithms [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] are designed for static datasets and must be re-executed upon data inserts, rendering them impractical for continuous data growth. Therefore, researchers have explored FD discovery on incremental databases. While incremental algorithms, DynFD [17] and DHSFD [18], per-form better than static FD discovery algorithms, they still face the following challenges. Specifically, (1) Long preprocessing time. Both DynFD and DHSFD require costly preprocessing, with time complexity of O(m ⋅ 2m) and O(n2), which is impractical in real-life scenarios; (2) Poor scalability on wide schema. As ∣R∣ increases, the candidate search space grows exponentially, degrading validation and maintenance cost; (3) 

Excessive memory consumption. They need to maintain many auxiliary data structures, which grow rapidly with data and cause high memory usage. 

> 1

https://www.nlcpress.com/DigitalPublishingView.aspx?DPId=10. 

> 2

https://www.ncsbe.gov/results-data/voter-history-data. 

> arXiv:2601.16025v1 [cs.DB] 22 Jan 2026 2

To address these challenges, this paper presents EAIFD, an efficient algorithm for incremental FDs discovery, that provides fast initialization, scalability on high-dimensional datasets, and low memory consumption. EAIFD systemati-cally tackles the aforementioned challenges through three key contributions . Firstly , EAIFD addresses the challenge of long preprocessing time by performing a sampling-based initializa-tion. It computes difference sets only on the sampled subset and builds partial hypergraphs, which significantly reduces 

O(n2) time complexity and greatly reduces preprocessing time. EAIFD generates candidate FDs by enumerating hitting sets on partial hypergraphs. Secondly , EAIFD optimizes the validation phase to reduce the high costs caused by the exponential growth of candidates. Specifically, it employs iterative grouping hash validation (IGHV) combined with a multi-attribute hash table ( M HT ) that stores mappings between LHS and RHS values. After data updates, IGHV compares the updated data with items in pre-built M HT and restricts validation to relevant blocks and validates candidates in batches. Valid FDs are added to F, while invalid FDs are used to update the hypergraph and iteratively generate new candidates. Thirdly , to control memory usage, EAIFD applies a high-frequency mapping items preservation strategy , keeping only mapping items that appear above a specified frequency threshold, ensuring M HT remains lightweight and balancing memory consumption with performance efficiency. Compre-hensive experiments are conducted on real-life datasets. The experimental results verify that EAIFD is more efficient than the incremental FD discovery algorithms. Notably, compared with the existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime, while reducing memory usage by over two orders of magnitude. The rest of this paper is organized as follows. Section II surveys the related work, followed by problem statement and preliminaries in Section III. Section IV introduces EAIFD algorithm. Section V provides a comprehensive experimental evaluation. Section VI concludes the paper. II. R ELATED WORK 

This section surveys the related work of FD discovery, and introduces two related research lines to our work: (i) FD discovery algorithms designed for static datasets, (ii) FD discovery algorithms that support incremental datasets update. 

A. FD discovery for static data 

Researchers propose many FD discovery algorithms for static relational databases [7], [8], [19], [20], [12], [9], [10], [11], which consist of two key phases: (i) candidate FDs generation and (ii) candidate FDs validation [13]. The existing algorithms improve FD discovery efficiency from different aspects and can be roughly divided into the following three categories. 

Attribute-based algorithms [7], [21], [9], [12], [15], [8] enumerate candidate FDs in powerset lattices of attribute combinations and traverse lattices with level-wise or depth-first strategies. They usually use position list indexes (PLIs) to validate candidate FDs with many pruning rules to reduce the search space. They perform well on datasets with many tuples, but due to their candidate-driven search strategy, they scale poorly on datasets with many attributes. 

Tuple-based algorithms [11], [14], [19], [20] are based on the value comparisons of all tuple pairs. These algorithms discover FDs by constructing agree sets of attributes with identical tuple values and difference sets of attributes with distinct values. These algorithms scale well with the number of attributes. However, comparing all tuple pairs often leads to a prohibitive O(n2) complexity, which severely degrades performance on long datasets. 

Hybrid algorithms [10], [16] integrate the advantages of both attribute-based and tuple-based algorithms. By employing a row-based strategy on sampled data and a column-based strategy on the full dataset, they effectively avoid numerous ineffective candidate FDs validations and tuple comparisons. Experimental results demonstrate that this hybrid design out-performs both attribute-based and tuple-based algorithms. 

B. FD discovery for incremental data 

Traditional static FD discovery algorithms must re-execute the entire discovery process after each data update. In recent years, different researchers have turned their efforts to devel-oping FD discovery algorithms suitable for incremental data scenarios. These methods aim to discover the complete set of minimal and non-trivial FDs at a lower computational cost after data updates without needing to re-execute the entire algorithm. DynFD [17] and DHSFD [18] are proposed for incremental FD discovery, which perform incremental FD dis-covery from the attribute and tuple perspectives, respectively. 

Attribute-based algorithm: DynFD is the first algorithm capable of maintaining the complete set of minimal non-trivial FDs for incrementally updated data. Instead of frequently recomputing all FDs, DynFD detects changes in the data and infers their impact on FDs by incrementally updating the FDs set based on prior results and a batch of update operations. It maintains a positive cover of minimal FDs and a negative cover of maximal non-FDs. After incremental data updates, DynFD validates whether the most general FDs still hold. If an FD is invalid, it is removed from the positive cover, added to the negative cover, and specialized by adding new attributes to its LHS. The specialized candidates are then validated using only PLI clusters that include newly inserted tuples, improving efficiency. If the specialized FD holds, it is added to the positive cover. When the proportion of invalidated FDs in a validation round exceeds 10% , DynFD switches to a violation-driven strategy that compares new tuples with relevant existing records to locate invalid FDs. For each invalid FD, the positive and negative covers are updated accordingly to maintain consistency. 

Tuple-based algorithm: DHSFD transforms the problem of incremental FD discovery into a dynamic hitting set enumer-ation problem over hypergraphs constructed from difference sets of entire dataset. DHSFD maintains auxiliary structures, including PLIs and difference sets with weights. After incre-mental data updates, PLIs are used to update the difference sets and their weights. These updates are treated as edge additions 3

in the hypergraph, where each edge represents a difference set. DHSFD then computes minimal hitting sets over the updated hypergraph. It validates minimality using critical edges and extends candidate FDs using a depth-first strategy called WalkDown. The resulting minimal hitting sets form the LHSs of newly discovered minimal FDs. By comparing only tuples in clusters containing incremental tuples and avoiding reconstruction of the search tree, DHSFD achieves higher efficiency than static algorithms. 

Discussion. This paper focuses on the problem of FD dis-covery for incremental data, which are continuously updated by adding tuples. Based on the current data scenarios, current incremental FD discovery faces the following challenges. 

Preprocessing time is too long. In the preprocessing phase, DynFD derives the negative cover from the positive cover, while DHSFD needs to compare all the tuples pair to compute difference sets of the dataset, this step has a time complexity 

O(n2) for a dataset with n tuples. When the dataset or ∣F∣ is large, both methods severely impact the startup time. 

Inefficient algorithm performance. DynFD is inefficient in high-dimensional datasets due to the exponential growth in the search space and increased memory usage for main-taining positive and negative covers. When the invalidation rate exceeds a threshold, DynFD switches to violation-driven validation and the cost of traversing PLI clusters containing new tuples becomes extremely expensive. DHSFD still faces a significant increase in the cost of updating its difference sets and hypergraphs. 

Auxiliary data structure memory space consumption is high. 

Two algorithms often rely on auxiliary data structures that consume a large amount of memory. DynFD maintains the positive cover, negative cover, position list indexes (PLIs), and dictionary encodings. Likewise, DHSFD needs to maintain PLIs, difference sets and hypergraphs. As new data is contin-uously appended, these structures grow in size, resulting in a significant increase in memory usage. Motivated by these limitations, our goal is to design an incremental FD discovery algorithm that supports high-dimensional datasets, avoids expensive preprocessing, and operates with minimal memory overhead. III. P ROBLEM DEFINITION 

Let r be a relation instance over schema R. We denote incremental data as ∆r. After incremental update, the updated dataset is denoted as r ∪ ∆r. ∀t ∈ r, we use t[X] or t[A] to denote the projection of t on attribute set X ⊆ R or single attribute A ∈ R, respectively. 

Definition III.1. (FD). The FD X → A with X ⊆ R and 

A ∈ R is valid for instance r of R, iff ∀t1, t 2 ∈ r ∶ t1[X] =

t2[X] ⇒ t1[A] = t2[A]. If there is no Y ⊂ X, Y → A and 

A ∉ X, then X → A is minimal and non-trivial. 

Given FDs X → A and Y → A where X ⊂ Y , X → A is a generalization of Y → A and Y → A is a specialization 

of X → A. FD discovery focuses on the complete set F of minimal and non-trivial FDs, as all other valid FDs can be derived from them using Armstrong’s axioms [22]. 

Definition III.2. (Violated tuple pair). If a candidate FD X →

A is invalid, then there must exist a pair of tuples t1, t 2 ∈ r

where t1[X] = t2[X] but t1[A] ≠ t2[A]. The (t1, t 2) is called a violated tuple pair for the candidate FD X → A.

This paper transforms FD discovery into hitting set enumer-ation in hypergraphs, following tuple-based algorithms [14], [23], [18]. The necessary definitions are introduced below. 

Definition III.3. (Difference set). For t1, t 2 ∈ r, their differ-ence set is D(t1, t 2) = {A ∈ R ∣ t1[A] ≠ t2[A]} , representing the set of attributes on which two tuples have different values. 

The difference sets of r is denoted as Dr = {D(t1, t 2) ∣ 

t1, t 2 ∈ r, D (t1, t 2) ≠ ∅}. For attribute A ∈ R, the difference sets of r modulo A is denoted as DAr = {D ∖ {A} ∣

D ∈ Dr ∧ A ∈ D}. A difference set D ∈ Dr is minimal if there does not exist D′ ∈ Dr and D′ ⊂ D. Dr =

{D ∈ Dr ∣ D′ ∈ Dr ∧ D′ ⊆ D ⇒ D′ = D}, where Dr consists of all minimal difference sets of Dr . The hypergraph of r

is denoted as Hr = (R, D r ), where R is the vertex set and 

Dr is the hyperedge set. The sub-hypergraph of attribute 

A is denoted by HA = (R, D Ar ). The minimal hypergraph 

Hr = (R, D r ) contains only the minimal hyperedges of Hr .

Definition III.4. (Hitting set). Given a sub-hypergraph HA =

(R, D Ar ), X is a hitting set of HA iff it intersects with every hyperedge in DAr . If no subset of X is a hitting set, X is a minimal hitting set. HS (⋅) denotes the set of minimal hitting sets of a hypergraph. 

Property III.1. If an FD X → A (A ∉ X) holds on r, X must be a hitting set of HA. Moreover, X → A is a minimal and non-trivial FD iff X is a minimal hitting set of HA.

Property III.2. Any hitting set of HA is also a hitting set of 

HA. The complete minimal hitting sets of HA are equal to the complete minimal hitting sets of HA.

It suffices to consider Hr when enumerating hitting sets. Actually, Hr is usually much smaller than Hr . In the following sections, both hypergraphs and the sub-hypergraphs refer to their minimal forms. 

Problem statement. Given a relation instance r of schema 

R, the incremental FD discovery aims to find the complete set 

F of minimal and non-trivial FDs that hold on the dataset after each data update, without re-executing the algorithm. 

IV. EAIFD A LGORITHM 

In this section, we introduce EAIFD, a novel FD discovery algorithm designed for incremental databases. EAIFD can efficiently discover the complete set F of minimal and non-trivial FDs on a dataset and rapidly update F after each data increment. Compared with existing incremental methods, EAIFD enables fast initialization, scales efficiently on high-dimensional datasets, and reduces memory consumption. The overall framework of EAIFD is illustrated in Fig. 2. The core idea of EAIFD is to generate candidate FDs through hitting set enumeration on hypergraphs constructed from difference sets and to validate candidate FDs. The EAIFD 4

is composed of two core components: a one-time discovery process for the initial dataset, and an efficient incremental update mechanism for all subsequent data increments. The 

one-time discovery process leverages sampling for rapid ini-tialization and builds the crucial multi-attribute hash table (M HT ) to accelerate future updates. The incremental update mechanism intelligently utilizes M HT for rapid validation pruning and employs the IGHV on data blocks for validating remaining candidates efficiently. The details of two compo-nents are described in Sections IV-A and IV-B, respectively. 

A. One-time discovery process on initial dataset r

Preprocessing . The input dataset r is first sorted in as-cending order by each attribute and stored on disk, with the index range of each distinct value recorded for rapid loading data blocks in following steps. Then EAIFD applies uniform random sampling without replacement to r at a sampling ratio ε = 0.3, selecting (( n

> 2

)) ε record pairs from total pairs in a dataset with n records. This approach balances data coverage with efficiency and has proved high effectiveness in FD discovery [11], [24]. We denote the sampled data as s.

One-time discovery process , as listed in Algorithm 1, gener-ates the complete set F of minimal non-trivial FDs holding on 

r and constructs M HT through two core steps: candidate FD generation (Section IV-A1) and validation (Section IV-A2). 

1) Generating candidate FDs with sampled data s:

EAIFD generates candidate FDs by processing each at-tribute A ∈ R as RHS (Lines 2-28). For each A, it constructs a partial sub-hypergraph HA = (R, D As ) from sampled data s (Line 2), where hyperedges in DAs =

D(ti, t j ) ∖ A ∣ ti, t j ∈ s ∧ A ∈ D(ti, t j ) represent difference sets of tuple pairs that differ on A, with A itself removed. DAs

captures differences involving A and defines the search space for candidate LHSs with A as the RHS. EAIFD then applies MMCS algorithm for each HA to enumerate minimal LHS candidates for A as RHS (Line 6), forming candidate FDs set 

CF Ds . Here, MMCS [25] is a hyperedge-based branching algo-rithm for enumerating minimal hitting sets from a hypergraph, which explores the search space efficiently by constructing a search tree and integrating minimality checking to avoid unnecessary branching. Since these candidate FDs are derived from partial sub-hypergraphs based on the sampled dataset s,they require validation to ensure validity on r.

2) Validating candidate FDs with IGHV: We propose an efficient validation method with a low memory usage, called iterative grouping hash validation (IGHV), to check the va-lidity of candidate FDs derived from partial sub-hypergraphs on r. Fig. 3 illustrates its main process. IGHV applies to both static data and incremental updates with few differences. To understand the novelty and advantages of IGHV, it is helpful to first review the principles and limitations of a standard hash-based validation approach. Let B denote the complete set of hash buckets, and BX represent the hash bucket set for candidate FD X → A. ∀t ∈ r, it applies a hash function f to t[X] to compute the hash value h, and assigns t to the bucket BhX . Any two tuples ti, t j in the same bucket BhX satisfy ti[X] = tj [X]. If ti[A] ≠ tj [A], a violated 

Algorithm 1: EAIFD initial FD discovery                                                                                                                                             

> Input: Sample data sand frequency threshold θ
> Output: The complete set of valid FDs F, multi-attribute hash table M HT , and sub-hypergraphs H
> 1F,∆dif f ,Binitialize to empty sets
> 2Build Hcontaining sub-hypergraphs for each attribute
> 3for A∈Rdo
> 4repeat
> 5Update Hby ∆dif f and clear ∆dif f
> 6CF Ds ←MMCS (HA)
> 7G←Group (CF Ds )
> 8foreach g∈Gdo
> 9Select a common attribute Bof all LHSs of g
> 10 foreach block riin rsorted by Bdo
> 11 foreach tuple t∈rido
> 12 foreach Xi→A∈gdo
> 13 h←f({ t[Xi]})
> 14 BhXi←BhXi∪{t}
> 15 foreach BXi∈Bdo
> 16 foreach BhXi∈BXido
> 17 if ∃t1, t 2∈BhXiwith t1[A]≠t2[A]
> then
> 18 Compute difference sets of
> (t1, t 2)and add to ∆dif f
> 19 g←g∖{Xi→A}
> 20 break
> 21 if Xi→Ais valid then
> 22 foreach BhXi∈BXido
> 23 if the count of tuples in
> BhXi≥θ×∣r∣then
> 24 Add (h, t [A]) to
> M HT (X→A)
> 25 Clear BXi
> 26 F←F∪g
> 27 until ∆dif f is empty
> 28 return F, M HT, H

tuple pair (ti, t j ) is detected, and X → A is declared invalid. To validate X → A, this method traverses the entire dataset to build BX . It works well on small datasets. However, as the number of candidate FDs increases or the dataset scale expands, the memory consumption of hash buckets becomes excessive and may lead to memory overflow. IGHV significantly improves validation efficiency while ef-fectively controlling memory usage. Algorithm 1 describes the IGHV process for validating candidate FDs CF Ds obtained by the MMCS tree search (Lines 7-26). IGHV first groups CF Ds 

such that the FDs in each group share at least one common LHS attribute (Line 7). If two candidate FDs X′ → A and 

X′′ → A satisfy X′ ∩ X′′ ≠ ∅, they can be placed in the same group g. For each group g = {X1 → A, X 2 → A, . . . , X k →

A}, where X1 ∩ X2 ∩ ⋅ ⋅ ⋅ ∩ Xk ≠ ∅, a common attribute with uniform value distribution B ∈ X1 ∩X2 ∩⋅ ⋅ ⋅∩ Xk is selected as a common sort attribute (Line 9). This ensures balanced data block sizes, thereby avoiding excessive memory consumption from overly large blocks and frequent loading overhead from too many small blocks. During preprocessing, data is sorted by B and can be divided into blocks r = r1 ∪ r2 ∪ ⋅ ⋅ ⋅ ∪ rm,5

Fig. 2: Overview of EAIFD and its components. 

where all tuples in a block ri share the same value for B.During validation, the dataset is loaded in the order sorted by B. Each block ri is processed as follows (Lines 10-25): (i) For each tuple t ∈ ri, IGHV computes the hash values f (t[X1]) , f (t[X2]) , . . . , f (t[Xk]) of the LHSs of all candidate FDs in the current group g, and inserts them into the corresponding hash bucket sets BX1 , BX2 , . . . , BXk (Lines 11-14). (ii) For each candidate FD X → A ∈ g, it checks each bucket in the hash bucket set BX to validate if all tuples in the bucket have the same value on attribute A (Lines 15-20). If a hash bucket contains tuples with different values on A, a violated tuple pair (tp, t q ) is identified, where tp[X] = tq [X]

but tp[A] ≠ tq [A], indicating that X → A does not hold. The difference set of these violated tuple pairs is added to the new hyperedge set ∆dif f for subsequent hypergraph updates. To avoid redundant validation, the invalid candidate FD is then removed (Lines 17-19). After validating data block ri, IGHV clears the correspond-ing hash buckets (Line 25) and proceeds to validate the next block. Once all data blocks have been processed, the validation for this group ends, the valid FDs from the group are added to F (Line 26). Before validating the next group, the newly collected difference set ∆dif f is used to update the partial sub-hypergraphs in H corresponding to the difference sets (Line 5). MMCS then continues to generate new candidate FDs, which undergo the same validation process until no new difference sets are produced. Finally, the same procedure is applied to the remaining attribute sub-hypergraphs. 

3) Constructing multi-attribute hash table: During one-time discovery process , we introduce multi-attribute hash table (M HT ), which keeps LHS to RHS value mappings of valid FDs, and integrates with IGHV to prune the search space and accelerate candidate FD validation in incremental scenarios. The M HT is designed for FD validation and conflict detection, based on the principle for a valid FD X → A, all tuples agreeing on X must also agree on A. For each tuple t

in r, a mapping item (f (t[X]) , t [A]) is formed by applying a hash function f to the LHS projection t[X], generating a hash key which is then paired with the RHS value t[A].This approach reduces memory usage and speeds up lookups. To ensure correctness, the hash function f ensures that the hash key uniquely identifies an equivalence class of tuples with respect to X, for any tuples ti, t j ∈ r if ti[X] = tj [X],then f (ti[X]) = f (tj [X]) ; and if ti[X] ≠ tj [X], then 

f (ti[X]) ≠ f (tj [X]) . The complete M HT is built by computing mapping items for all tuples, clearly reflecting the value mapping from LHS to RHS of valid FDs. If X → A

holds, each hash key f (ti[X]) must correspond to a unique value t[A]. M HT allows fast conflict detection, if a hash key maps to distinct values, indicating a violated tuple pair 

ti, t j such that ti[X] = tj [X] but ti[A] ≠ tj [A]. In validation phase, the M HT supports O(k) time complexity, making it particularly efficient for FD validation. For large datasets with many candidate FDs, the number of mapping items in M HT may grow rapidly, leading to high memory usage. For this reason, EAIFD employs a high-frequency mapping items preservation strategy , caching only the mapping items whose frequency in the dataset exceeds a frequency threshold θ. For any valid FD X → A, IGHV counts the mapping frequency between X and A (Lines 21-24). A hash mapping item (h, a ) is cached in M HT (X → A) if it satisfies the frequency condition: ∣t∈rj ∣rj ∈r,f (t[X]) =h,t [A]=a∣∣r∣ ≥

θ. The frequent mappings represent commonly observed value associations between LHS and RHS. The choice of the threshold θ must balance a high thresh-old, which creates a sparse M HT with a weak pruning effectiveness, against a low threshold, which results in an oversized M HT and excessive memory use. In this paper, we set θ = 80% based on the three considerations. First, adopting a high-frequency threshold to retain only the most significant elements is well-established practice [26]. Sec-ond, value mappings with occurrence frequencies above 80% 

can effectively capture the most representative attribute-value combinations in real-life datasets. Third, the effectiveness of this choice is further confirmed by the experimental results presented later. This strategy allows for efficient pruning of the validation space using high-frequency mapping items while avoiding M HT becoming excessively large, thereby achieving a balance between performance efficiency and memory usage. Once all sub-hypergraphs are processed, one-time discovery process ends. It derives F holding on r, multi-attribute hash table M HT for each valid FD X → A which contains only high frequency value mapping items, and the hypergraph set 

H containing sub-hypergraphs for each attribute. 6

Fig. 3: The process of IGHV. After grouping, select some CF Ds in g2 and show its core validation steps on data blocks r1 and r4

B. Incremental update mechanism on r ∪ ∆r

When the initial dataset r is updated with incremental data 

∆r, incremental update mechanism is triggered to efficiently update F and M HT over dataset r ∪ ∆r. Similar to one-time discovery process , this mechanism also uses an iterative process where candidate FDs are generated by MMCS and validated by a two-step validation strategy. The mechanism it-eratively updates hypergraphs until no new candidates are pro-duced, completing the update of F. The complete procedure of incremental update mechanism is outlined in Algorithm 2. 

1) Generating candidate FDs with incremental data ∆r:

After the incremental update, tuples in the incremental data ∆r

are compared pairwise to generate a new difference set ∆dif f 

(Line 1). Since the scale of ∣∆r∣ is typically small, the time cost of this operation remains acceptable. Based on ∆dif f ,the corresponding sub-hypergraphs are updated (Line 4), and MMCS tree search is performed on the updated hypergraphs to generate candidate FDs, which is also denoted as CF Ds 

(Line 5). MMCS searches from the tree nodes corresponding to the minimal hitting sets of the hypergraphs constructed from the initial dataset r obtained in the one-time discovery process 

to avoid a full restart. MMCS first updates the hypergraphs by 

∆dif f as new hyperedges, and identifies the affected initial minimal hitting sets that fail to cover these new hyperedges. Subsequently, for each affected initial minimal hitting set, MMCS selects candidate attributes from uncovered attributes and extends it to generate new candidate FDs. However, the candidate FDs CF Ds obtained through the MMCS are only guaranteed to hold locally on r and ∆r but not on r ∪∆r. This is because the hypergraphs are constructed solely from the difference sets generated from r during the 

one-time discovery process and from pairwise comparisons within incremental data ∆r, and exclude difference sets de-rived from tuple pairs between r and ∆r. A candidate FD 

X → A from MMCS, valid on r and ∆r separately, but may be invalid on r ∪ ∆r if tuples ti ∈ r and tj ∈ ∆r satisfy 

ti[X] = tj [X] but ti[A] ≠ tj [A]. Therefore, each candidate FD in CF Ds must undergo further validation to confirm its validity on r ∪ ∆r.The candidate FD CF Ds must hold on r due to two reasons. First, the one-time discovery process iteratively expands partial hypergraphs until their minimal hitting sets coincide with those of the global hypergraph constructed from all minimal difference sets of r, ensuring that F is complete for all 

Algorithm 2: EAIFD incremental FD discovery                                                                   

> Input: ∆r,F,M HT ,H
> Output: F,M HT
> 1: Compute difference sets of ∆rand add to ∆dif f
> 2: for A∈Rdo
> 3: while ∆dif f ≠∅do
> 4: Update Hby ∆dif f and then clear ∆dif f
> 5: CF Ds ←MMCS (HA)
> 6: C1←CF Ds ∩F
> 7: C2←CF Ds ∖C1
> // validate candidate FDs in C1
> 8: res1 ←VCF (C1,“C1_M ODE ”)
> // validate candidate FDs in C2
> 9: res2 ←VCF (C2,“C2_M ODE ”)
> 10: Update F,∆dif f ,M HT by res1 and res2 11: return F,M HT

minimal non-trivial FDs on r. It is demonstrated that, during this iterative expansion, each minimal hitting set of the partial hypergraph is either a minimal hitting set of the global hyper-graph, or it can be extended into one [17]. Once the expansion yields no new hyperedges, the sets of minimal hitting sets in the partial and global hypergraphs are same, indicating that all minimal non-trivial FDs on r have been captured in F. Second, during incremental updates, the hypergraphs are dynamically updated with difference sets ∆dif f from ∆r.Unless ∆dif f introduces new minimal hyperedges, the original minimal hitting sets remain unchanged. It is proven [18] that each new minimal hitting set in the updated hypergraph must contain a previous one, implying it is a specialization of the previous FD. This ensures that candidate FDs generated from updated hypergraphs only exclude those in F invalidated by 

∆r, while all remaining and new candidates still hold on r.To improve validation efficiency, incremental update mech-anism divides CF Ds into two categories: (i) minimal FDs that hold on r, denoted as C1 = CF Ds ∩F; and (ii) non-minimal FDs that also hold on r, denoted as C2 = CF Ds ∖ C1, which can be regarded as specializations of C1. Two categories are validated separately based on the availability of M HT (Lines 8-9). The distinct validation mechanisms for C1 and C2 are detailed in Section IV-B2 and Section IV-B3, outlined in Algorithm 3. 

2) Validating candidate FDs in C1 with IGHV and M HT :

Since ∆r is typically much smaller than r, most updates only affect the validity of a small portion of FDs. Most tuples still 7

Fig. 4: The process of validating CF Ds in two types: C1 and C2.For clarity, ∆r contains only 3 tuples, and two candidate FDs are validated in each type. 

support the original FDs, and most FDs still remain valid after updates. As a result, C1 accounts for a high proportion of the total candidate FDs. Therefore, improving the validation efficiency of C1 has become key to optimizing the overall efficiency. To efficiently validate the candidates in C1 over r ∪ ∆r,EAIFD adopts a two-step validation strategy. The first step, 

M HT -based validation , compares the local hash table built on ∆r (denoted as M HT ∆) with the pre-computed M HT 

on r to rapidly identify conflicts and efficiently prune the validation space. Any candidates that cannot be resolved in 

MHT-based validation step proceed to the second step, table-scan validation , which loads the relevant data blocks from 

r and applies IGHV for validation. Figure 4 illustrates how EAIFD processes candidate FDs in C1.

Step 1: M HT -based validation. For a candidate FD X →

A, EAIFD first builds a local multi-attribute mapping hash table on ∆r, denoted as M HT ∆(X → A) (Line 3). Since 

∣∆r∣ is small, the memory usage for M HT ∆(X → A) is manageable. After constructing M HT ∆, M HT -based vali-dation compares the mapping items in M HT ∆(X → A) with 

M HT (X → A) built on r (Line 5). During the comparison, the validation result for a candidate FD X → A must be one of the following three cases.  

> ●

Valid. If M HT ∆(X → A) ⊆ M HT (X → A), all mappings in M HT ∆ already exist in M HT , meaning 

∆r does not introduce new LHS to RHS value mappings. 

X → A naturally holds on r ∪ ∆r and is updated to F,and is removed from C1 without further validation.  

> ●

Invalid. If an item (f (tj [X]) , t j [A]) in M HT ∆(X →

A) and an item (f (ti[X]) , t i[A]) in M HT (X → A)

satisfy f (ti[X]) = f (tj [X]) but ti[A] ≠ tj [A], then tuples tj ∈ ∆r and ti ∈ r form a violated tuple pair. This pair shares the same hash key but has different RHS values, thus X → A is invalid on r∪∆r and removed from 

C1. The comparison of (ti, t j ) yields a new difference set, which is added to ∆dif f and used to update the relevant sub-hypergraphs in H before the next iteration.  

> ●

Uncertain. During the comparison between M HT ∆(X →

A) and M HT (X → A), any mapping item (h, a ) in 

M HT ∆(X → A) that also exists in M HT is removed to reduce redundancy. If M HT ∆(X → A) ≠ ∅ after comparison, it indicates that M HT ∆(X → A) contains mapping items that do not appear in M HT (X → A).Such items occur for two reasons: (i) they exist in the initial dataset r but their occurrence frequency does not meet the frequency threshold θ, and thus are not recorded in M HT ; or (ii) they represent new LHS to RHS value mappings introduced by ∆r. In this case, the validity of 

X → A remains uncertain and triggers the table-scan validation .

Step 2: table-scan validation. After the MHT-based valida-tion , only candidate FDs classified as Uncertain require further table-scan validation, which employs IGHV on the selectively loaded data blocks from the initial dataset. Specifically, the table-scan validation selectively loads cor-responding data blocks from r based on LHS attribute values in ∆r. We only need to detect potential conflicts between a tuple from ∆r and a tuple from r to check candidate FDs. Conflicts may occur when tuples in r contain the same LHS attribute values as tuples in ∆r, generating same hash keys while having different RHS values. Since tuples within blocks of r that do not share the sort attribute value with any tuple in ∆r cannot produce same hash keys with ∆r tuples, no conflicts can be detected from them. Scanning such blocks would only incur redundant I/O and computational costs. Since 

∣∆r∣ is typically small, the number of blocks to be loaded is also limited, making this strategy highly effective in improving validation efficiency while ensuring correctness. 

Table-scan validation first builds local M HT ∆ for each candidate FD on ∆r (Line 3). Then, the candidate FDs are grouped using the same grouping strategy as employed in the IGHV of the one-time discovery process (Line 7). After grouping, a sort attribute B is selected for each group (Line 9).When validating the candidate FDs in this group, EAIFD only loads data blocks from r sorted by B and whose values of B also exist in ∆r. Formally, the set of data blocks to be loaded is defined as: r′ = {ri ∣ ti ∈

ri ∧ ti[B] ∈ {tj [B] ∣ tj ∈ ∆r, f (tj [X]) ∈ K∆}} where 

K∆ = KeySet (M HT ∆(X → A)) (Line 10). The method then validates this group candidates on blocks in r′ instead of the full r, sequentially performing batch validation on each block. After obtaining relevant data block set r′, EAIFD sequen-tially loads each data block and performs batch validation on the current block. For each candidate FD X → A in the group, it first builds a local temporary M HT (X → A) to record the mappings from LHS hash key value to RHS value based on the current data block tuples; then, it compares the local temporary M HT (X → A) and M HT ∆(X → A) (Line 13). 

X → A is valid only if no conflict is found across all blocks in r′. Conversely, if there exist tj ∈ ∆r and ti ∈ r satisfying 

f (ti[X]) = f (tj [X]) but ti[A] ≠ tj [A], the X → A is 8

Algorithm 3: Validate Candidate FDs (VCF)                                                                                                           

> Input: Candidate set Cand validation mode mode
> Output: V alid F Ds ,∆dif f
> // When mode equals "C1_M ODE ": C=C1
> // When mode equals "C2_M ODE ": C=C2
> 1V alid F Ds ,∆dif f initialize to empty sets
> 2for each X→A∈Cdo
> 3Build M HT ∆(X→A)on ∆r
> 4if mode equals “ C1_M ODE ”then
> 5Check M HT ∆(X→A)and M HT (X→A)and collect result
> 6Update V alid F Ds ,C,∆dif f by result
> 7G←Group (C)
> 8for each g∈Gdo
> 9Select a common attribute Bof all LHSs of g
> 10 for each block ri∈r′do
> 11 for each X→A∈gdo
> 12 Build M HT (X→A)on ri
> 13 Check M HT ∆(X→A)and M HT (X→A)
> and collect result
> 14 Update V alid F Ds ,∆dif f by compare result
> 15 return V alid F Ds ,∆dif f

judged invalid, and the difference set of (ti, t j ) is added to 

∆dif f for hypergraph updating in the next iteration. After each block validation, mapping items in the local temporary M HT 

with frequency exceeding θ are recorded and updated to the global complete M HT , and then local temporary M HT is cleared to reduce memory usage. After processing all blocks in r′, the valid FDs and the new difference set are recorded (Line 14). The procedure repeats for all groups (Lines 8-14). After processing all groups, the validation concludes by returning the final V alid F Ds and ∆dif f (Line 15). Thus, through M HT -based comparison between M HT ∆

and M HT , followed by selective table-scan validation using IGHV on loaded data blocks, EAIFD efficiently validates C1

and identifies minimal FDs that remain valid. 

3) Validating candidate FDs in C2 with IGHV: For can-didate FDs in C2, which hold on r but are non-minimal, no corresponding mapping items exist in M HT since it only stores value mappings for minimal and non-trivial FDs. EAIFD validates them using the same procedure as the table-scan validation described in Section IV-B2 for the uncertain candidate FDs in C1, loading data blocks from r containing attribute values present in ∆r. This approach ensures correct-ness while avoiding redundant computation. The process is illustrated in Fig. 4. After validating C1 and C2, EAIFD updates FDs that hold on r ∪ ∆r into F and returns a difference set ∆dif f from the violated tuple pairs. This ∆dif f updates the relevant hypergraphs in H for the next iteration. 

C. Algorithm analysis 

To establish the theoretical soundness and efficiency of EAIFD algorithm, this part first provides a formal proof of correctness and then a detailed complexity analysis. The former guarantees the accuracy of the algorithm, while the latter quantifies its performance advantages. 

1) Correctness Proof: EAIFD involves candidate genera-tion and validation in both one-time discovery process and 

incremental update mechanism , with overall correctness rely-ing on two steps. For candidate generation, its correctness is guaranteed in three aspects: (i) completeness, (ii) minimality, and (iii) non-triviality. 

Completeness. For each RHS attribute A, the one-time discovery process builds a sub-hypergraph HA and iteratively updates it with new hyperedges derived from violated tuple pairs until no new hyperedges can be generated. As proved by Lemma IV.1, Corollaries IV.1 and IV.2, when no new hyperedges are generated, HA has same minimal hitting sets as the global hypergraph constructed from all minimal difference sets in r. Hence, HA guarantees complete candidate FD generation. MMCS enumerates all minimal hitting sets of 

HA, each corresponding to a valid minimal FD X → A per Definition III.4. Completeness in the incremental update mechanism relies on the same principle of iterative hypergraph refinement used in the one-time discovery process . This mechanism starts with 

HA, upon which completeness in one-time discovery process 

is proven, and updates it iteratively by adding two types of new difference sets: those from pairwise comparisons within ∆r,and those generated by violated tuple pairs between ∆r and r.Once no new hyperedges appear, Lemma IV.1 and Corollar-ies IV.1–IV.2 ensure that MMCS enumerates the same minimal hitting sets as those derived from all minimal difference sets over r ∪ ∆r, thereby guaranteeing completeness in candidate FD generation in the incremental update mechanism .

Lemma IV.1. For any two hypergraphs H and H′, H ≺ H′ if and only if HS (H) ⊇ HS (H′). [24] 

Corollary IV.1. A minimal hitting set X for the partial hypergraph PA is a global minimal FD (i.e., X ∈ HS (DA))if and only if the FD X → A holds on the dataset. [24] 

Corollary IV.2. When the algorithm iteration produces no new difference sets (i.e., new hyperedges), the minimal hitting sets HS (PA) of the current hypergraph PA constitute all minimal FDs, i.e., HS (PA) = HS (DA). [24] 

Minimality. As stated in Property III.1, MMCS guarantees the minimality of candidate FDs in both the one-time discov-ery process and the incremental update mechanism , since it enumerates only minimal hitting sets. Each hitting set X on 

HA directly corresponds to a minimal FD X → A.

Non-triviality. To ensure non-triviality, one-time discovery process constructs sub-hypergraph HA using difference sets generated from tuple pairs different on A, while excluding 

A. One-time discovery process and the incremental update mechanism update HA with new difference sets generated from tuple pairs different on A, also excluding A. MMCS then enumerates hitting sets on HA. According to Definitions III.3 and III.4, since A ∉ X and X determines A, the FD X → A

is non-trivial. The second essential step ensuring correctness of EAIFD is candidate validation, whose validation strategies in both phases accurately determine the validity of candidates and identify all violated tuple pairs necessary for hypergraph refinement. 9

One-time discovery process employs IGHV to validate candidate FDs. IGHV first groups the candidates and selects a common sort attribute for each group. It then sequentially loads data blocks sorted by this attribute and performs vali-dation within each block. Under the hash function f , tuples with identical LHS values share the same hash value, while those with different LHS values have distinct hashes, ensuring that tuples with equal hashes are located in the same block after sorting. Consequently, validation only needs to examine tuple pairs within the current block for possible violations, which localizes comparisons and greatly reduces cost while preserving correctness. If any block contains violated tuple pairs, the FD is deemed invalid; IGHV generates difference sets to update the hypergraphs. If no conflicts are found across all blocks, the FD is considered valid. 

Incremental update mechanism ensures validation correct-ness by a two-step validation strategy. Candidate FDs are divided into C1 and C2 and validated through distinct methods. Candidates in C1 are rapidly validated by the MHT-based validation through comparing items in M HT ∆ with the items in M HT . Since M HT stores the high-frequency hash LHS value to RHS value mapping items of candidate FDs from 

r and M HT ∆ stores all hash LHS value to RHS value mapping items from ∆r, the comparison between them can accurately detect violated tuple pairs between ∆r and r. The remaining candidate FDs in C1 that cannot be validated by 

MHT-based validation , along with C2, are validated using 

table-scan validation , which performs IGHV through selective loading of data blocks. After grouping the candidate FDs and selecting a common sort attribute, this method does not load all data blocks of r. Instead, it selectively loads only those blocks containing tuples that share the same value as the tuples in ∆r on the common sort attribute. Only tuples within these blocks may share the same LHS hash values as the incremental data, thus requiring validation of their RHS attributes to detect violated tuple pairs. Since tuples in other blocks have different sort values and thus distinct LHS hashes, they are safely skipped. This method guarantees correctness while substantially reducing I/O and computation cost. Therefore, based on the demonstrated correctness of both candidate generation and validation steps, the EAIFD algo-rithm is proven to be correct. 

2) Complexity Analysis: This part comprehensively ana-lyzes the time complexity of EAIFD, covering both one-time discovery process and incremental update mechanism , and concludes with the analysis of space complexity. We consider a dataset r with n tuples and m attributes, and the incremental data ∆r with n∆ tuples. 

Time complexity. We begin with the one-time discovery process . Its preprocessing involves sorting r by each attribute for subsequent validation, taking O(m × n log n) time. Then, tuples in sample data s are compared pairwise to compute the difference sets with complexity O(∣ s∣2). This avoids the 

O(m × n2) complexity of DHSFD requiring all pairwise tuple comparisons and enables rapid initialization of EAIFD. Following preprocessing, one-time discovery process itera-tively performs candidate generation and validation for each RHS attribute A. Within each iteration, candidate generation relies on MMCS algorithm. As proved in [25], the time com-plexity for a single search step of MMCS on sub-hypergraph 

HA = (R, D Ar ) is O(∣ HA∣) , where ∣HA∣ = ∑D∈DAr ∣D∣ denotes the total size of hyperedges. Discovering one minimal hitting set typically requires multiple MMCS search steps, with most search steps used only for state updates or pruning. Generating 

k minimal hitting sets in an iteration takes time complexity of 

O(k × ∣HA∣ × α), where α is the average number of search steps to discover one minimal hitting set. In the same iteration, candidate validation employs IGHV to validate the k candidate FDs generated by MMCS. In the worst case, each candidate FD needs to be validated across all data blocks with complexity O(k × n), while candidates are pruned immediately upon conflict. Let b be the average number of blocks validated per candidate FD and ∣V ∣ the distinct values of the common sort attribute in r for the group. With approximate block size n  

> ∣V∣

, the validation cost per candidate FD is O(b × n  

> ∣V∣

), yielding average complexity O(k × b × n  

> ∣V∣

)

for k candidates. Combining the candidate generation and validation, the time complexity for one iteration is O(k ×(∣ HA∣×α +b× n  

> ∣V∣

)) . The total cost of one-time discovery process can be obtained by multiplying this one iteration cost by the number of iterations, which is dependent on the underlying data. Next, we analyze the time complexity of incremental update mechanism . This mechanism first computes new difference sets from pairwise comparisons of tuples in ∆r, with time complexity O(n2∆). It then iteratively generates candidate FDs using MMCS and validates them. Since the complexity of MMCS is discussed previously, our focus here is on validation. Candidate FDs are divided into C1 and C2, and a two-step validation strategy is applied. We then formally characterize the time complexity of both validation steps for k candidates. For candidate FDs in C1, MHT-based validation constructs a local M HT ∆ for each X → A in ∆r at a cost of O(k ×n∆).Comparing M HT ∆ with M HT requires traversing M HT ∆

items and querying M HT , which takes O(1) per candidate FD due to hash table lookups, yielding O(k) total for k

candidates. The overall complexity for validating k candidates in C1 is O(k × (n∆ + 1)) , which is highly efficient since n∆

is typically small. 

Table-scan validation handles uncertain candidates in C1

and all candidate FDs in C2. Assuming a total of k can-didates, a local M HT ∆ is first built for each FD on ∆r,costing O(k × n∆). Candidates are grouped by common LHS attributes, and each group is validated sequentially. Unlike the 

one-time discovery process that loads all blocks, table-scan validation selectively loads only blocks containing attribute values in ∆r. Let V∆ and V denote the distinct values of the common sort attribute in ∆r and r, respectively. The loaded blocks contain roughly n × ∣V∆∣∣V ∣ tuples. Candidates are pruned immediately upon invalidation. Let b′ be the average number of blocks validated per candidate. The total cost of per group containing k 

> ∣G∣

candidates is O ( k 

> ∣G∣

× b′ × n  

> ∣V∣

), giving a total complexity of O (k × (n∆ + b′ × n  

> ∣V∣

)) . Compared to 

O(k × b × n  

> ∣V∣

) complexity of validating k candidates using IGHV in the one-time discovery process , table-scan validation 10 

is more efficient since n∆ ≪ n and b′ ≪ b.

Space complexity. EAIFD uses M HT as its core aux-iliary structure to accelerate candidate FD validation while controlling memory. Without filtering, each FD could store up to n hash entries, yielding a worst-case space complexity of O(∣ F∣ × n). To reduce memory, EAIFD applies a high-frequency mapping preservation with threshold θ ∈ (0, 1),caching only hash keys h satisfying count (h) 

> n

≥ θ. Let p denote the cached items per FD; since p × (θ × n) ≤ n, we have p ≤ 1 

> θ

,giving an upper bound of O( 1 

> θ

) per FD. Consequently, the overall space complexity is O( ∣F∣ 

> θ

), independent of dataset size 

n and scalable for large datasets, enabling EAIFD to maintain controllable memory usage even for large-scale datasets. In summary, EAIFD reduces preprocessing to O(m ×

n log n), avoiding the O(m × n2) cost of pairwise compar-isons and enabling rapid initialization. Its optimized valida-tion achieves near-linear time in both the one-time discovery process and incremental update mechanism . Along with the 

high-frequency mapping items preservation strategy , space complexity is bounded by O( ∣F∣ 

> θ

), allowing EAIFD to main-tain correctness and scale efficiently on incrementally updated datasets. V. P ERFORMANCE EVALUATION 

This section evaluates the performance of EAIFD on a DELL OptiPlex Tower Plus 7010 desktop PC equipped with an Intel(R) Core(TM) i7-13700 CPU (16 cores, 3.00 GHz), 32 GB of RAM, running 64-bit Windows 11. All experiments are implemented in Java (JDK 8). 

Datasets. We evaluate EAIFD on 20 different real datasets to assess its practical performance. Table I summarizes their properties, which differ in tuple number, attribute number, and data distribution, enabling a comprehensive performance analysis. All real datasets are publicly available 3.

NULL Semantics. In relational databases, NULL represents missing or inapplicable values, which are common in real-world datasets. Different NULL-handling strategies impact the validity of related FDs. We adopt the NULL-equals-NULL semantics, treating all missing values as equal, which is the common default in FD discovery [27], [16]. 

Competitors. The static FD discovery algorithm FD-HITS [11] and two incremental FD discovery algorithms, DynFD [17] and DHSFD [18], are selected for comparison 4.FDHITS outperforms other static algorithms [11]. DynFD (an attribute-based incremental algorithm) maintains positive and negative covers to generate and validate candidate FDs via PLIs, while DHSFD (a tuple-based incremental algo-rithm) formulates FD discovery as a hitting set enumeration problem over hypergraphs of difference sets with a dynamic enumeration strategy. Comparing these three algorithms with EAIFD provides a comprehensive evaluation of its incremental effectiveness. 

Parameters. EAIFD performance is influenced by two parameters: the sampling ratio ε and the mapping frequency     

> 3Datasets are available at https://hpi.de/naumann/projects/repeatability/ data-profiling/fds.html and https://www.kaggle.com/datasets.
> 4All codes are obtained from https://hpi.de/naumann/projects/repeatability/ data-profiling/fds.html and https://github.com/RangerShaw/DHSFD.

threshold θ. Given a dataset with n records, the total num-ber of record pairs is p = (n

> 2

). EAIFD uniformly samples approximately pε record pairs ( 0 < ε < 1) without replacement to build initial partial hypergraphs. Prior studies indicate that setting ε = 0.3 achieves a good balance between coverage and efficiency [24], [11]. The mapping frequency threshold θ

determines how many items are stored in the M HT , thus controlling pruning strength and memory use. Experiments show θ = 80% provides an effective balance. Thus, we adopt 

ε = 0.3 and θ = 80% as default settings. 

Other settings. From each dataset, we randomly select 80% 

of tuples as r and generate ∆r from the remaining 20% as in DHSFD [18]. All times are in seconds unless specially stated; TL indicates a run exceeding the limit of 5 hours. Results are averaged over three executions, with the machine restarted each time. 

Experimental Structure. We conduct comprehensive ex-periments to evaluate the efficiency, scalability, and unique features of EAIFD. Experiment 1 on real-life datasets demon-strates the performance advantages of EAIFD. Experiment 

2, 3 and 4 analyze scalability across tuple number, attribute number, and incremental data size. Experiment 5 compares EAIFD against the current state-of-the-art static algorithm in incremental updates. Experiments 6, 7 and 8 respectively demonstrate the key advantages of EAIFD: rapid initialization, a memory-light M HT with high-frequency mapping items preservation strategy , and the effect of θ across a wide range. 

Exp 1: Overall performance comparison. Experiment 1

evaluates EAIFD against current incremental FD discovery algorithms on 20 real-world datasets, with ∣∆r∣∣r∣ ranging from 

10% to 30% , ε = 0.3, and θ = 80% . As reported in Table I. EAIFD is consistently faster than DynFD and DHSFD across all insertion proportions. Against DynFD, EAIFD achieves speedups of up to two orders of magnitude on several datasets (e.g., Plista). The advantages are particularly significant on larger datasets like Flights, Census, Fd-Reduced-30, and CAB, where DynFD consistently exceeds the time limit (TL). This confirms the scalability limitations of DynFD due to its layer-by-layer candidate generation and PLI-based validation. In contrast, EAIFD transforms candidate generation as a hitting set enu-meration problem on hypergraphs and employs a two-step validation strategy, achieving a higher scalability. Although both DHSFD and EAIFD can handle large-scale datasets, EAIFD runs nearly an order of magnitude faster than DHSFD on several datasets (e.g., Nursery). The significant speedup validates the effectiveness of two-step validation strategy in EAIFD. MHT-based validation rapidly processes the majority of candidates ( C1) using efficient O(1) hash lookups. Subsequently, table-scan validation selectively loads data blocks and validates the remaining FDs in batch. This strategy substantially improves overall efficiency. 

Exp 2: Scalability with tuple number ∣r∣. Experiment 2

evaluates the row scalability of EAIFD, DynFD, and DHSFD on the CAB and Pitches datasets by varying ∣r∣, as shown in Fig. 5(a) and Fig. 5(d). We keep ∣∆r∣∣r∣ = 20% and fix ∣R∣

(54 for CAB, 40 for Pitches). Specifically, ∣r∣ ranges from 11 

TABLE I: The experimental results on real-life datasets. Runtime in seconds for EAIFD compared with two incremental algorithms DynFD and DHSFD. The fastest runtimes are highlighted in bold. TL indicates that the algorithm exceeded the time limit.                                                                                                                                                                                                                                                                            

> Dataset Properties Incremental (∣ ∆r∣=10% ∣r∣) Incremental (∣ ∆r∣=20% ∣r∣) Incremental (∣ ∆r∣=30% ∣r∣)
> Dataset ∣r∣∣R∣∣F∣DynFD DHSFD EAIFD DynFD DHSFD EAIFD DynFD DHSFD EAIFD Bridges 108 13 142 0.2 0.0013 0.001 0.23 0.004 0.003 0.28 0.008 0.005
> Balance 625 510.13 0.03 0.006 0.15 0.06 0.007 0.21 0.071 0.009
> Abalone 4177 9137 0.1 0.03 0.008 0.21 0.07 0.011 0.31 0.094 0.024
> Iris 147 540.166 0.03 0.009 0.21 0.037 0.009 0.2 0.049 0.012
> Nursery 13000 910.8 0.17 0.015 1.1 0.21 0.018 1.6 0.235 0.027
> NCV 1000 19 758 0.3 0.04 0.02 0.5 0.09 0.05 0.54 0.11 0.07
> Hepatitis 155 20 8250 1.2 0.25 0.07 2.5 0.3 0.09 2.8 0.384 0.121
> Claim 20000 11 12 0.42 1.8 0.27 0.47 2.5 0.45 0.7 3.5 0.507
> Letter 20000 17 61 14 1.5 0.53 22.1 1.9 0.77 28 2.6 1.21
> Plista 1001 63 173409 143.4 4.6 0.5 147.5 7.5 1.3 145.1 12.8 3.2
> Horse 300 29 128726 30.1 2.1 0.9 33 4.1 1.8 34.5 5.5 3.906
> Bioentry 184292 919 69 5.6 2.6 83 14.8 8.4 103 34.1 17.9
> Tax 1000000 15 263 346 52 29 387 93 57 494 176 87
> Ditag_feature 3960124 13 58 TL 358 44 TL 1439 287 TL 5204 743
> Fd-Reduced-30 250000 30 89571 TL 144 76 TL 367 174 TL 583 297
> Flight 250000 31 117367 TL 190 83 TL 410 198 TL 630 410
> Pitches 250000 40 608928 TL 310 132 TL 600 420 TL 920 780
> CAB 67300 54 3353531 TL 330 197 TL 700 580 TL 1350 990
> Census 196000 42 41861 TL 341 206 TL 622 461 TL 1077 845
> Lineitem 6000000 16 3984 TL 1308 968 TL 3314 2767 TL 7123 6594

10 ,000 to 14 ,000 (∣∆r∣ from 2,000 to 2,800 ) for CAB, and from 10 ,000 to 50 ,000 (∣∆r∣ from 2,000 to 10 ,000 ) for Pitches. Runs exceeding 5 hours are omitted. DynFD fails to complete within the time limit on both datasets because its PLI-based validation cost grows rapidly with ∣r∣, resulting in poor row scalability. In contrast, DHSFD and EAIFD scale more gently with increasing ∣r∣, showing stable performance. Notably, EAIFD consistently outperforms DHSFD. The row scalability of DHSFD is limited by the increasing costs associated with updating PLI, computing new difference sets, and refining hypergraphs as ∣r∣ increases. By updating the hypergraph through difference sets derived from tuple pairs in ∆r, EAIFD avoids redundant full-pair comparisons involving r. Then, two-step validation does not scan the initial dataset but leverages M HT and loads relevant data blocks for validation, ensuring high row scalability and stable performance as ∣r∣ increases. 

Exp 3: Scalability with attribute number ∣R∣. Experi-ment 3 evaluates the column scalability of EAIFD, DynFD, and DHSFD on the CAB and Pitches datasets by varying ∣R∣,with ∣r∣ and ∣∆r∣ fixed ( 10 ,000 and 2,000 for CAB; 50 ,000 

and 10 ,000 for Pitches). Fig. 5(b) and Fig. 5(e) show results when varying ∣R∣ from 34 –54 for CAB and 20 –40 for Pitches, respectively. DynFD exhibits poor column scalability on both datasets. On CAB, its runtime rises sharply with ∣R∣ (713 s-8,831 s) and fails to finish within the limit at ∣R∣ = 54 . On Pitches, DynFD failed to complete within the time limit across all tested attribute numbers ( ∣R∣ from 20 to 40 ). This severe performance degradation stems from its reliance on PLI-based validation, where increasing attribute numbers lead to a rapid growth in both candidate number and set intersection operations. As observed in Fig. 5(b) and (e), both tuple-based algo-rithms, DHSFD and EAIFD, indicate significantly better col-umn scalability than column-based DynFD, exhibiting moder-ate runtime growth as ∣R∣ increases. EAIFD consistently out-performs DHSFD, showing lower runtime and growth trend. The performance of DHSFD is still affected by the increasing complexity of updating PLI, difference sets and hypergraphs as more attributes are involved. In contrast, EAIFD achieves a superior column scalability due to two key choices. For one thing, EAIFD prevents excessive candidates growth by modeling FD generation as hitting set enumeration on partial hypergraph and generating new candidates selectively. For another, its two-step validation strategy replaces costly PLI-based set intersections by M HT structure for rapid validation, and then validating the remaining candidate FDs in batches over relevant data blocks, ensuring high column scalability. 

Exp 4: Scalability with incremental size ∣∆r∣. Experiment 

4 evaluates the incremental scalability of EAIFD, DynFD, and DHSFD on CAB ( ∣r∣ = 10 ,000 , ∣R∣ = 54 , ∣∆r∣ = 2,000 –10 ,000 , 

> ∣∆r∣
> r

= 20% − 100% ) and Pitches ( ∣r∣ = 50 ,000 , ∣R∣ = 40 , ∣∆r∣ =

10 ,000 –22 ,500 , ∣∆r∣ 

> r

= 20% − 45% ) as shown in Fig. 5(c) and Fig. 5(f). On both datasets, DynFD fails to complete within the time limit, confirming its poor scalability, as its PLI-based validation across r ∪ ∆r becomes prohibitive. The runtimes of DHSFD and EAIFD both grow noticeably as the incremental ratio ∣∆r∣ 

> r

increases, incurred by higher costs in difference set computation, hypergraph refinement and validation operation. However, EAIFD consistently outperforms DHSFD and ex-hibits a lower growth rate in runtime. The key advantage for EAIFD stems directly from two aspects. First, EAIFD avoids enumerating all cross-set pairs and updates difference sets within ∆r, while DHSFD computes difference sets between 

∆r and r. Second, a two-step validation strategy enables efficient candidate validation. Even when ∣∆r∣ 

> r

increases, MHT-based validation in EAIFD handles most C1 candidates effi-ciently via rapid conflict detection. The subsequent table-scan validation only loads relevant data blocks for the remaining candidates in batch, reducing the unnecessary I/O cost. 12 10 11 12 13 14 Number |r| of tuples (K)                                           

> 0
> 20
> 40
> 60
> 80 Time (sec)
> (a) CAB: varying |r|
> EAIFD DHSFD
> 34 38 42 46 50 54 Number |R|of attributes
> 10
> 100
> 1000
> 10000 Time (sec)
> (b) CAB: varying |R|
> EAIFD DHSFD DynFD
> 20% 40% 60% 80% 100% Percentage |r|of inserts
> 0
> 20
> 40
> 60
> 80
> 100
> 120
> 140 Time (sec)
> (c) CAB: varying |r|
> EAIFD DHSFD
> 10 20 30 40 50 Number |r|of tuples (K)
> 0
> 10
> 20
> 30
> 40
> 50 Time (sec)
> (d) Pitches: varying |r|
> EAIFD DHSFD
> 20 25 30 35 40 Number |R|of attributes
> 0
> 10
> 20
> 30
> 40
> 50 Time (sec)
> (e) Pitches: varying |R|
> EAIFD DHSFD
> 20% 25% 30% 35% 40% 45% Percentage |r|of inserts
> 0
> 50
> 100
> 150
> 200
> 250
> 300 Time (sec)
> (f) Pitches: varying |r|
> EAIFD DHSFD

Fig. 5: Time performance of EAIFD compared with DynFD, and DHSFD on the CAB and Pitches. Subfigures (a, b, c) correspond to CAB with varying tuple number ∣r∣, attribute number ∣R∣, and incremental data size ∣∆r∣, respectively, while (d, e, f) show the same for Pitches. 0.00% 2% 5% 7% 10% 12% 15% Percentage ratio of | r||r|            

> 0
> 250
> 500
> 750
> 1000
> 1250
> 1500
> 1750 Time (sec)
> (a) Lineitem: varying |r||r|
> EAIFD FDHITS
> 0.00% 2% 5% 7% 10% 12% 15% Percentage ratio of |r||r|
> 0
> 20
> 40
> 60
> 80 Time (sec)
> (b) Ditag_feature: varying |r||r|
> EAIFD FDHITS

Fig. 6: Time performance of EAIFD compared with FDHITS across different incremental ratios ∣∆r∣ 

> r

. Subfigures (a) and (b) correspond to Lineitem and Ditag_feature, respectively. 

Exp 5: EAIFD against static FD discovery algorithm. 

Experiment 5 evaluates the performance of EAIFD and FD-HITS (a state-of-the-art static algorithm) on Lineitem ( ∣r∣ =

6, 000 , 000 , ∣R∣ = 16 ) and Ditag_feature ( ∣r∣ = 3, 960 , 124 , ∣R∣ =

13 ). For FDHITS, sep variant is selected over joint variant for two reasons. First, the original paper [11] demonstrates that 

sep and joint variants have highly competitive performance, while the former outperforms the latter on many real-world datasets. Second, the performance of joint variant is highly sensitive to sampled data characteristics, while seq variant provides more stable and predictable performance. We fix ∣r∣

and ∣R∣, and vary incremental ratios ∣∆r∣

r across a wide range (0.01% , 0.05% , 0.1% , 0.5% , 1% , 5% , 10% , 15% ) to identify a performance break-even point between EAIFD and FDHITS. The results in Figure 6(a) and (b) show two distinct trends. The runtime of FDHITS is high but grows modestly across all incremental ratios, since its runtime is determined by the total dataset size ( ∣r∣ + ∣∆r∣). In contrast, EAIFD shows significant efficiency advantages in small increments (Lineitem: ∣∆r∣

r ≤

5% ; Ditag_feature: ∣∆r∣

r ≤ 10% ), achieving much lower run-time. As the incremental ratio increases, the runtime of EAIFD grows rapidly and eventually exceeds that of FDHITS. This is because the runtime of EAIFD is primarily determined by the incremental data size ( ∣∆r∣). Nevertheless, this experiment confirms the practical value of EAIFD: in typical applications, the incremental update ∆r typically constitutes a small portion of the large and growing data set r. Thus, EAIFD consistently stays on the favorable side of the performance break-even point, making it more suitable for real-world scenarios. TABLE II: Preprocessing of three algorithms on different datasets. 

Dataset Properties Preprocessing Time DataSet ∣r∣ ∣R∣ ∣F ∣ DynFD DHSFD EAIFD 

Iris 147 5 4 0.06s 0.03s 0.017s 

Bridges 108 13 142 0.15h 1.1s 0.77s 

Hepatitis 155 20 8250 0.45h 1.6s 0.88s 

Horse 300 29 128726 5.1h 2.5s 0.63s 

Balance 625 5 1 0.02s 0.62s 0.03s NCV 1000 19 758 67.4s 4.6s 0.7s 

Plista 1001 63 173409 5.5h 6.7s 2.8s 

Claim 20000 11 12 128s 123s 62s 

Letter 20000 17 61 164s 145s 92s 

CAB 67300 54 3353531 6.5h 1.34h 0.77h 

Bioentry 184292 9 19 0.25h 0.42h 0.12h 

Flight 250000 31 117367 6.6h 3.45h 0.34h 

Pitches 250000 40 608928 7.9h 5.6h 0.56h 

Ditag_Feature 3960124 13 58 7.2h 7.4h 0.67h 

Exp 6: The preprocessing time of algorithms. Experi-ment 6 evaluates the one-time preprocessing cost of EAIFD, DynFD, and DHSFD on real-world datasets. The startup mech-anisms of the three algorithms differ significantly. DynFD computes positive and negative covers from the initial dataset. DHSFD compares all tuple pairs to build the hypergraph. In contrast, EAIFD first sorts the dataset by attributes and performs pairwise comparisons only on sampled data. Experimental results in Table II show that EAIFD achieves shortest preprocessing times, often over an order of magnitude 13 

faster on large datasets. DynFD is slow on large datasets with many FDs, while DHSFD can take several hours on long datasets with many tuples, making startup virtually impractical on very large datasets. EAIFD avoids building auxiliary struc-tures such as PLIs or positive and negative covers. Instead, it sorts each attribute ( O(∣ R∣ × ∣r∣ log ∣r∣) ) and performs pairwise comparisons only on a small sampled dataset ( O(∣ s∣2)). The limited sample size keeps preprocessing efficient, enabling rapid initialization in practice. 

Exp 7: The memory usage of auxiliary structures. 

Experiment 7 evaluates the memory usage of PLIs and M HT 

(with θ = 80% ) of r. As PLIs are the common validation structures, they provide a key benchmark. To analyze the factors influencing memory usage, we record ∣r∣, ∣R∣, ∣F∣, and memory size of datasets. Results are summarized in Table III. Experimental results show that PLI memory usage grows rapidly with increasing ∣r∣ and ∣R∣, due to maintaining com-plete attribute partition indices. In contrast, M HT employs a 

high-frequency mapping items preservation strategy , caching only key value mapping items of valid FDs exceeding a frequency threshold θ. Thus each FD stores at most O( 1 

> θ

)

mapping items, making the overall space complexity O( ∣F∣ 

> θ

),which is independent of dataset size. Consequently, on datasets with few FDs, such as Claim and Bioentry, M HT consumes substantially less memory than PLIs, and its memory usage remains controllable even for large-scale datasets. On complex datasets or with many FDs, the memory usage of M HT may exceed that of PLIs, which also aligns perfectly with its O( ∣F∣ 

> θ

) space complexity. Nevertheless, this is our time-memory tradeoff design, where EAIFD accepts moderate memory for a substantial performance speedup. TABLE III: Auxiliary structures on different datasets.                                                                                          

> Dataset Properties Auxiliary Structures DataSet ∣r∣∣R∣∣F∣Data PLIs MHT ( θ=80% )
> Iris 147 545KB 1.7KB 0.04KB
> Bridges 108 13 142 6KB 3KB 1.4KB
> Hepatitis 155 20 8250 6.1KB 3.6KB 8.4KB Horse 300 29 128726 18KB 6.3KB 1.2MB Balance 625 517KB 2.3KB 0
> NCV 1000 19 758 151KB 76KB 10.3KB
> Plista 1001 63 173409 496KB 312KB 1.7MB Claim 20000 11 12 2.6MB 1.5MB 5KB
> Letter 20000 17 61 696KB 221KB 15KB
> CAB 67300 54 3353531 11.4MB 8.9MB 98MB Bioentry 184292 919 24MB 16.7MB 6KB
> Flight 250000 31 117367 19.7KB 11KB 1.1MB Pitches 250000 40 608928 50MB 27.3MB 19MB
> Ditag_Feature 3960124 13 58 348MB 202MB 14KB

Exp 8: The effect of frequency threshold θ. Experiment 8 evaluates the scalability of EAIFD under different mapping frequency thresholds θ for retaining high-frequency items in the M HT . The experiment uses four datasets: Plista, Pitches, CAB, and Flight, with an incremental ratio of ∣∆r∣∣r∣ = 10% .Table IV illustrates the performance and memory consump-tion of the M HT for r when the frequency threshold θ

increases from 70% to 90% . When θ = 70% , EAIFD achieves the highest efficiency with the largest M HT memory usage, since more high-frequency mapping items are retained in the initial M HT . This allows a large portion of validations to be TABLE IV: Runtime and memory usage of M HT under different mapping frequency θ on four datasets.                                                      

> Dataset Runtime (sec) Memory of M HT (MB)
> 70% 75% 80% 85% 90% 70% 75% 80% 85% 90% Plista 0.09 0.32 0.50 2.91 4.51 1.90 1.75 1.70 1.43 1.27 Flight 42 68 83 174 406 3.27 1.93 1.10 0.82 0.56 Pitches 72 95 132 357 630 54.7 31.6 19.0 13.6 10.8 CAB 78 144 197 522 927 272.6 134.2 98.7 77.3 45.5

efficiently performed via hash lookups with M HT ∆, without revalidating on the initial data blocks. This reflects a trade-off between space consumption and time performance. As θ

increases, fewer mapping items satisfy the threshold, leading to reduced memory usage of the M HT . Since fewer candidates can be verified by comparing items in the M HT , more data blocks need to be loaded and validated, thereby increasing the runtime. The results clearly demonstrate that θ is a highly sensitive parameter for the time–memory trade-off. Considering both 

M HT memory usage and runtime, the results confirm that θ =

80% is a well-balanced parameter choice. Overall, the high-frequency mapping item preservation strategy enables EAIFD to balance storage efficiency and computational performance. 

Summary of Experimental Analysis. The experimental results demonstrate that EAIFD achieves superior perfor-mance in incremental FD discovery. By reformulating FD discovery as a partial hypergraph hitting set enumeration and employing M HT to preserve historical computations com-bined with efficient validation methods, EAIFD outperforms existing static and incremental algorithms. It maintains stable performance across varying tuple, attribute, and incremental data scales, while reducing preprocessing time. Moreover, the 

high-frequency mapping items preservation strategy of M HT 

balances memory and computational efficiency, and the core frequency threshold θ shows acceptable performance across a wide range. Overall, EAIFD provides an efficient, scalable, and practical solution for incremental FD discovery. VI. C ONCLUSION 

This paper proposes EAIFD, an efficient algorithm for FD discovery in relational databases under continuous incremental updates. EAIFD overcomes redundant cost caused by re-execution in static algorithms and the performance bottlenecks of existing incremental methods. It introduces two core com-ponents: (1) modeling incremental FD discovery as hitting set enumeration over partial hypergraphs, (2) employing efficient two-step incremental validation strategy. This design achieves high performance with low memory consumption. Extensive experiments demonstrate that EAIFD consistently outperforms existing incremental FD discovery (achieving up to an order-of-magnitude speedup on datasets such as Plista) and shows significant performance compared with static algorithms when the incremental data ratio is small. Its efficiency advantage stems from its core operations, which depend primarily on the incremental data size ∣∆r∣ rather than the full dataset size ∣r∣. This characteristic further enhances the efficiency and scalability in long-term applications where ∣∆r∣

> r14

decreases over time. In terms of memory, EAIFD achieves a low memory consumption independent of ∣r∣ by utilizing 

M HT with the high-frequency mapping items preservation strategy . Furthermore, EAIFD enables fast initialization with preprocessing complexity O(∣ R∣ × ∣r∣ log ∣r∣) .While EAIFD achieves remarkable progress in incremental FD discovery, several directions remain for future work, such as deletions and modifications for fully dynamic FD discovery, and explore parallel or distributed implementations to improve scalability on massive datasets. REFERENCES [1] A. Silberschatz, H. F. Korth, and S. Sudarshan, Database System Concepts, Seventh Edition . McGraw-Hill Book Company, 2020. [2] T. Papenbrock and F. Naumann, “Data-driven schema normalization,” in 

Proc. 20th Int. Conf. Extending Database Technol. , 2017, pp. 342–353. [3] Z. Wei and S. Link, “Embedded functional dependencies and data-completeness tailored database design,” ACM Trans. Database Syst. ,vol. 46, no. 2, pp. 7:1–7:46, 2021. [4] J. Kossmann, T. Papenbrock, and F. Naumann, “Data dependencies for query optimization: a survey,” VLDB J. , vol. 31, no. 1, pp. 1–22, 2022. [5] P. Bohannon, W. Fan, F. Geerts, X. Jia, and A. Kementsietsidis, “Conditional functional dependencies for data cleaning,” in Proc. Int. Conf. Data Eng. , 2007, pp. 746–755. [6] A. Doan, A. Y. Halevy, and Z. G. Ives, Principles of Data Integration .Morgan Kaufmann, 2012. [7] Y. Huhtala, J. Kärkkäinen, P. Porkka, and H. Toivonen, “TANE: an efficient algorithm for discovering functional and approximate depen-dencies,” Comput. J. , vol. 42, no. 2, pp. 100–111, 1999. [8] N. Novelli and R. Cicchetti, “Functional and embedded dependency inference: a data mining point of view,” Inf. Syst. , vol. 26, no. 7, pp. 477–506, 2001. [9] Z. Abedjan, P. Schulze, and F. Naumann, “DFD: efficient functional dependency discovery,” in Proc. ACM Int. Conf. Inf. Knowl. Manag. ,2014, pp. 949–958. [10] T. Papenbrock and F. Naumann, “A hybrid approach to functional dependency discovery,” in Proc. 2016 ACM Int. Conf. Manag. Data ,2016, pp. 821–833. [11] T. Bleifuß, T. Papenbrock, T. Bläsius, M. Schirneck, and F. Naumann, “Discovering functional dependencies through hitting set enumeration,” 

Proc. ACM Manag.Data , vol. 2, no. 1, pp. 43:1–43:24, 2024. [12] J. Liu, F. Ye, J. Li, and J. Wang, “On discovery of functional depen-dencies from data,” Data Knowl. Eng. , vol. 86, pp. 146–159, 2013. [13] X. Wan, X. Han, J. Wang, and J. Li, “Efficient discovery of functional dependencies on massive data,” IEEE Trans. Knowl Data Eng. , vol. 36, no. 1, pp. 107–121, 2024. [14] C. M. Wyss, C. Giannella, and E. L. Robertson, “Fastfds: A heuristic-driven, depth-first algorithm for mining functional dependencies from relation instances - extended abstract,” in Proc. Int. Conf. Data Ware-hous. Knowl. Discov. , 2001, pp. 101–110. [15] H. Yao and H. J. Hamilton, “Mining functional dependencies from data,” 

Data Min. Knowl. Discov. , vol. 16, no. 2, pp. 197–219, 2008. [16] Z. Wei and S. Link, “Discovery and ranking of functional dependencies,” in Proc. Int. Conf. Data Eng. , 2019, pp. 1526–1537. [17] P. Schirmer, T. Papenbrock, S. Kruse, F. Naumann, D. Hempfing, T. Mayer, and D. Neuschäfer-Rube, “Dynfd: Functional dependency discovery in dynamic datasets,” in Proc. Int. Conf. Extending Database Technol. , 2019, pp. 253–264. [18] R. Xiao, Y. Yuan, Z. Tan, S. Ma, and W. Wang, “Dynamic functional dependency discovery with dynamic hitting set enumeration,” in Proc. Int. Conf. Data Eng. , 2022, pp. 286–298. [19] P. A. Flach and I. Savnik, “Database dependency discovery: A machine learning approach,” AI Commun. , vol. 12, no. 3, pp. 139–160, 1999. [20] S. Lopes, J. Petit, and L. Lakhal, “Efficient discovery of functional dependencies and armstrong relations,” in Proc. Int. Conf. Extending Database Technol. , ser. Lect. Notes Comput. Sci., vol. 1777, 2000, pp. 350–364. [21] T. Papenbrock, J. Ehrlich, J. Marten, T. Neubert, J. Rudolph, M. Schön-berg, J. Zwiener, and F. Naumann, “Functional dependency discovery: An experimental evaluation of seven algorithms,” Proc. VLDB Endow. ,vol. 8, no. 10, pp. 1082–1093, 2015. [22] H. Garcia-Molina, J. D. Ullman, and J. Widom, Database systems - the complete book (2. ed.) . Pearson Education, 2009. [23] H. Mannila and K. Räihä, “Algorithms for inferring functional depen-dencies from relations,” Data Knowl. Eng. , vol. 12, no. 1, pp. 83–99, 1994. [24] J. Birnick, T. Bläsius, T. Friedrich, F. Naumann, T. Papenbrock, and M. Schirneck, “Hitting set enumeration with partial information for unique column combination discovery,” Proc. VLDB Endow. , vol. 13, no. 11, pp. 2270–2283, 2020. [25] K. Murakami and T. Uno, “Efficient algorithms for dualizing large-scale hypergraphs,” Discret. Appl. Math. , vol. 170, pp. 83–94, 2014. [26] R. Agrawal, T. Imielinski, and A. N. Swami, “Mining association rules between sets of items in large databases,” in Proc. 1993 ACM Int. Conf. Manag. Data , 1993, pp. 207–216. [27] L. Berti-Équille, H. Harmouch, F. Naumann, N. Novelli, and S. Thiru-muruganathan, “Discovery of genuine functional dependencies from relational data with missing values,” Proc. VLDB Endow. , vol. 11, no. 8, pp. 880–892, 2018.