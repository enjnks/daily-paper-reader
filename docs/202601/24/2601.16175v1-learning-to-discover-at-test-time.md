# Learning to Discover at Test Time
# 在测试时学习发现

**Authors**: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun \
**Date**: 2026-01-22 \
**PDF**: https://arxiv.org/pdf/2601.16175v1 \
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 8.0 \
**Evidence**: 利用AI发现科学问题的新SOTA \
**TLDR**: 提出TTT-Discover方法，通过测试时强化学习为科学问题自动发现更优的解决方案。 \

---

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

## 摘要
如何利用人工智能为科学问题发现新的最先进（SOTA）成果？以往关于测试时

---

## 论文详细总结（自动生成）

这是一份关于论文《Learning to Discover at Test Time》的结构化深入总结：

### 1. 论文的核心问题与整体含义
*   **研究动机**：科学发现（Scientific Discovery）要求产生超越现有知识的新思想。传统的“测试时缩放”（Test-time Scaling）方法（如 AlphaEvolve）通常通过提示词（Prompting）驱动冻结的 LLM 进行搜索。然而，这种方式限制了模型内化新思想的能力。
*   **核心问题**：如何让 LLM 在面对特定的、分布外（OOD）的科学难题时，通过“边做边学”来突破现有的技术极限（SOTA）？
*   **整体含义**：论文提出了一种名为 **TTT-Discover** 的方法，将“测试时训练”（Test-Time Training, TTT）与强化学习（RL）结合。其核心逻辑是：针对单一测试问题进行持续学习，目标不是泛化，而是为该特定问题找到一个最优解。

### 2. 论文提出的方法论
*   **核心思想**：在测试阶段，针对单个问题对 LLM 进行在线强化学习。该方法不追求平均奖励的最大化，而是追求**最大奖励（Maximum Reward）**的突破。
*   **关键技术细节**：
    *   **熵目标函数（Entropic Objective）**：不同于标准 RL 优化期望奖励，TTT-Discover 使用指数加权的熵目标 $J_\beta$，通过自适应调节 $\beta$ 参数，使模型高度关注那些产生极高奖励（即潜在发现）的动作。
    *   **改进的 PUCT 搜索**：借鉴 AlphaZero 的 PUCT 算法进行状态重用（State Reuse）。不同之处在于，其 $Q$ 值计算基于子节点中的**最大奖励**而非平均值，从而鼓励探索具有高潜力的路径。
    *   **状态与动作重用**：维护一个缓冲区存储过去的尝试，通过搜索启发式算法选择初始状态，从而有效延长了解决复杂问题的“有效步长”。
*   **算法流程**：
    1.  从缓冲区采样初始状态和上下文（PUCT）。
    2.  LLM 生成动作（代码或思考过程）。
    3.  环境执行代码并返回连续奖励（如运行速度、数学界限）。
    4.  使用熵目标函数更新模型权重（LoRA 微调）。
    5.  重复上述过程，直到发现超越 SOTA 的解。

### 3. 实验设计
*   **实验场景与数据集**：
    *   **数学**：Erdős 最小重叠问题、自相关不等式（AC1/AC2）、圆填充问题。
    *   **GPU 内核工程**：GPUMode 竞赛任务（TriMul 三角矩阵乘法优化、DeepSeek MLA 解码优化）。
    *   **算法设计**：AtCoder 启发式竞赛（AHC039 几何、AHC058 调度）。
    *   **生物学**：单细胞 RNA 测序数据去噪（OpenProblems 评估）。
*   **Benchmark 与对比方法**：
    *   **基准**：人类专家最高分、现有 AI 最高分（如 AlphaEvolve, ThetaEvolve, ALE-Agent）。
    *   **对比方法**：Best-of-N（同等采样预算下的纯搜索）、OpenEvolve（进化算法）、Naive RL（标准强化学习）。

### 4. 资源与算力
*   **模型**：主要使用开源模型 `gpt-oss-120b`，部分对比实验使用了 `Qwen3-8B`。
*   **算力成本**：通过 Thinking Machines 的 **Tinker API** 运行。
*   **训练细节**：每个问题进行 **50 个训练步骤**，每步生成 **512 个样本**（Rollouts）。
*   **经济成本**：每个问题的测试时训练成本约为 **几百美元**（文中提到约 $500）。
*   **硬件环境**：评估和验证在 H100、A100、B200 及 AMD MI300X 等 GPU 上完成。

### 5. 实验数量与充分性
*   **实验规模**：论文涵盖了四个截然不同的科学与工程领域，并在每个领域都选择了具有代表性的硬核挑战。
*   **消融实验**：作者对“熵目标函数”、“PUCT 优先级”以及“TTT 本身”进行了详细消融，证明了每个组件对最终发现 SOTA 的贡献。
*   **客观性与公平性**：
    *   作者声明报告了“尝试过的每一个问题”，避免了选择性报告。
    *   所有结果均使用开源模型达成，对比的却是使用闭源前沿模型（如 Gemini 2.0 Pro）的先前方法。
    *   结果经过了相关领域的专家（如数学教授、GPU 专家、生物学教授）的评审。

### 6. 论文的主要结论与发现
*   **全面超越 SOTA**：TTT-Discover 在几乎所有尝试的任务中都刷新了纪录。
    *   **数学**：改进了 Erdős 最小重叠问题的上界和 AC1 不等式的上界。
    *   **内核工程**：生成的 TriMul 内核比人类专家最优解快达 2 倍。
    *   **算法**：在 AtCoder 竞赛中达到了足以获得第一名的分数。
*   **开源模型的潜力**：证明了通过有效的测试时学习，中等规模的开源模型可以击败依赖强大闭源模型的搜索方法。
*   **学习优于搜索**：实验显示，单纯增加采样量（Best-of-N）的边际效应递减很快，而通过 TTT 让模型“内化”经验是实现科学突破的关键。

### 7. 优点
*   **范式创新**：将测试时训练从简单的“泛化增强”提升到了“科学发现”的高度。
*   **高效的目标设计**：自适应 $\beta$ 的熵目标函数解决了科学发现中“极值优化”的痛点。
*   **高可复现性**：使用开源模型和公开代码，且成本在可接受范围内，打破了前沿科学发现对闭源巨型模型的依赖。
*   **跨领域验证**：从纯数到硬件底层优化，展现了极强的通用性。

### 8. 不足与局限
*   **奖励函数依赖**：目前仅适用于具有**连续且可验证奖励**的问题。对于奖励稀疏、二元（对/错）或无法自动验证的领域（如人文社科或某些实验科学），该方法尚无法直接应用。
*   **上下文限制**：受限于 LLM 的上下文窗口（如 32k），在处理极长代码或复杂推理链时可能存在瓶颈。
*   **计算开销**：虽然比训练大模型便宜，但相比于直接推理，每个问题数百美元的成本对于大规模简单任务来说依然昂贵。
*   **生物学验证**：如专家评审所述，虽然在 Benchmark 指标上占优，但其发现的算法在实际生物学意义上的有效性仍需进一步实验验证。

（完）
