# Learning to Discover at Test Time
# 学习在测试时进行发现

**Authors**: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun \
**Date**: 2026-01-22 \
**PDF**: https://arxiv.org/pdf/2601.16175v1 \
**Tags**: <span class="tag-label tag-blue">精读区</span> <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 8.0 \
**Evidence**: 通过测试时搜索和强化学习自动发现最先进的解决方案 \
**TLDR**: 提出 TTT-Discover，利用测试时强化学习和搜索来发现科学问题的最优解。

---

## 速览
**TLDR**：提出 TTT-Discover 方法，通过在测试时对 LLM 进行强化学习训练，在数学、GPU 优化、算法和生物等多个领域刷新了 SOTA。
**Motivation**：现有的测试时搜索方法多基于冻结模型，难以针对特定科学问题的特性进行深度自我进化和优化。
**Method**：采用测试时训练（TTT-Discover），利用强化学习使模型在解决特定问题时持续学习，并设计了优先探索高潜力方案的搜索机制。
**Result**：在 Erdős 数学问题、GPU 内核加速（最高提速 2 倍）、AtCoder 竞赛及单细胞分析中均取得领先，且仅需数百美元成本。
**Conclusion**：该研究表明，通过针对特定问题的测试时训练，开源模型在科学发现任务上能够超越闭源的前沿模型。

---

## Abstract
How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős' minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.

## 摘要
如何利用人工智能为科学问题发现新的最先进水平（SOTA）？先前的测试时缩放（test-time scaling）工作（如 AlphaEvolve）通过提示冻结的大语言模型（LLM）进行搜索。我们在测试时执行强化学习，使 LLM 能够继续训练，但现在使用的是针对特定测试问题的经验。这种形式的持续学习非常特殊，因为其目标是产生一个极佳的解决方案，而不是平均意义上的许多良好方案，并且旨在解决当前这一特定问题，而非泛化到其他问题。因此，我们的学习目标和搜索子程序旨在优先考虑最有希望的解决方案。我们将此方法称为“测试时训练以发现”（TTT-Discover）。遵循先前的工作，我们专注于具有连续奖励的问题。我们报告了在数学、GPU 内核工程、算法设计和生物学领域尝试过的每一个问题的结果。TTT-Discover 在几乎所有问题上都创下了新的最先进水平：(i) 埃尔德什（Erdős）最小重叠问题和自相关不等式；(ii) GPUMode 内核竞赛（比现有技术快达 2 倍）；(iii) 过去的 AtCoder 算法竞赛；以及 (iv) 单细胞分析中的去噪问题。我们的解决方案经过了专家或组织者的审查。与之前需要封闭前沿模型的最优结果不同，我们的所有结果都是使用开源模型 OpenAI gpt-oss-120b 实现的，并且可以使用我们公开的代码进行复现。我们的测试时训练运行是使用 Thinking Machines 的 API Tinker 执行的，每个问题的成本仅为几百美元。

---

## 论文详细总结（自动生成）

这是一份关于论文《Learning to Discover at Test Time》的结构化深入分析报告：

### 1. 核心问题与研究动机
*   **核心问题**：如何利用 AI 突破人类现有知识边界，发现科学与工程问题的全新最先进（SOTA）解决方案？
*   **研究动机**：
    *   **搜索 vs. 学习**：现有的测试时缩放（Test-time Scaling）方法（如 AlphaEvolve）主要依赖于在冻结的 LLM 上进行进化搜索。作者认为，人类解决难题不仅靠尝试（搜索），更靠从失败中总结经验（学习）。
    *   **分布外挑战**：科学发现本质上是“分布外”任务。冻结的模型难以内化新发现的思路，而“测试时训练”（TTT）允许模型针对特定问题进行参数更新。
    *   **发现的特殊性**：科学发现的目标是找到“一个极佳解”（Max），而非“平均性能”（Mean），这与传统强化学习（RL）的目标函数存在本质区别。

### 2. 方法论（TTT-Discover）
TTT-Discover 的核心思想是在测试阶段针对单个问题运行在线强化学习，其关键技术细节包括：
*   **核心流程**：算法 1 描述了一个循环：从缓冲区采样初始状态（reuse） $\rightarrow$ 模型生成动作（rollout） $\rightarrow$ 环境反馈奖励 $\rightarrow$ 更新模型权重（train）。
*   **熵目标函数（Entropic Objective）**：为了优先考虑最有希望的“发现”，作者采用了指数加权的奖励目标 $J_\beta$。当 $\beta \to \infty$ 时，该目标趋向于最大化最大奖励（Max），而非期望奖励。
*   **自适应 $\beta$ 调节**：为了解决训练稳定性问题，作者引入了自适应 $\beta(s)$，通过约束诱导策略与原策略之间的 KL 散度来动态调整权重集中度。
*   **PUCT 状态重用机制**：借鉴 AlphaZero 的启发式搜索，设计了基于奖励排名（Prior）和访问次数（Exploration）的初始状态选择规则，且 Q 值取子节点的最大奖励而非平均值。
*   **参数高效微调**：使用 LoRA（Rank 32）对模型进行轻量化更新，确保在测试时训练的效率。

### 3. 实验设计与 Benchmark
论文在四个极具挑战性的领域进行了验证：
*   **数学**：Erdős 最小重叠问题、自相关不等式（AC1/AC2）、圆堆积问题。对比对象包括人类数学家记录、AlphaEvolve (V1/V2)、ThetaEvolve。
*   **GPU 内核工程**：GPUMode 竞赛中的 TriMul（三角矩阵乘法）和 DeepSeek MLA 算子优化。对比对象为人类专家提交的最优内核。
*   **算法设计**：AtCoder 启发式竞赛（AHC039, AHC058）。对比对象为 ALE-Agent、ShinkaEvolve 及人类竞赛榜单。
*   **生物学**：单细胞 RNA 测序数据去噪（OpenProblems 任务）。对比对象为 MAGIC、ALRA 等专业生物信息学算法。

### 4. 资源与算力
*   **基础模型**：开源模型 `gpt-oss-120b`（基于 Qwen 架构）及 `Qwen3-8B`。
*   **算力平台**：使用 Thinking Machines 的 **Tinker API** 进行训练。
*   **成本与时长**：
    *   每个问题的训练成本约为 **500 美元**。
    *   训练设置：50 个 Step，每个 Step 生成 512 个 rollout（总计 25,600 个样本）。
    *   硬件环境：评估过程使用了 NVIDIA H100、H200、A100 以及 AMD MI300X 等多种 GPU 以验证内核性能。

### 5. 实验数量与充分性
*   **实验覆盖**：论文报告了在所有尝试过的问题上的结果，未进行“樱桃拾取”（Cherry-picking）。
*   **消融实验**：针对“熵目标函数”、“自适应 $\beta$”、“PUCT 重用”以及“有无 TTT”进行了详细对比。结果显示，仅靠搜索（Best-of-N）或朴素 RL 均无法达到 TTT-Discover 的效果。
*   **客观性**：
    *   引入了多位外部专家（数学教授、GPU 架构师、生物学家）进行盲审或实名评审。
    *   所有代码和发现的数学构造、内核代码均已开源，可供复现。
    *   对比基准统一了采样预算（25,600 次），确保了公平性。

### 6. 主要结论与发现
*   **全面突破 SOTA**：在 Erdős 最小重叠问题上发现了新的上界（0.380876）；在 GPU 内核优化中比人类专家快 15%-50%；在 AtCoder 竞赛中达到了全球第一的水平。
*   **开源超越闭源**：通过测试时训练，120B 的开源模型在科学发现任务上击败了依赖 GPT-4o 或 Gemini 1.5 Pro 的现有方法。
*   **学习优于搜索**：实验证明，随着计算预算增加，通过 RL 更新模型参数（学习）比单纯增加采样次数（搜索）具有更好的扩展性。

### 7. 优点与亮点
*   **范式创新**：将“测试时训练”从简单的泛化增强扩展到了“科学发现”领域。
*   **目标函数适配**：敏锐地捕捉到了“发现”任务中 Max 优于 Mean 的特性，并给出了稳定的数学实现。
*   **跨领域通用性**：一套超参数（学习率、LoRA Rank 等）几乎无需调整即可在数学、代码、生物等多个异构领域生效。
*   **落地价值**：生成的 GPU 内核具有极高的实用价值，直接提升了 AlphaFold3 等模型的运行效率。

### 8. 不足与局限
*   **奖励函数依赖**：该方法目前仅适用于具有**连续且可验证奖励**的问题。对于奖励稀疏（如只有对/错）或不可验证（如人文创作）的领域，其有效性尚未验证。
*   **上下文限制**：受限于模型 32k 的上下文窗口，长代码或复杂推理过程可能导致截断，限制了更复杂算法的生成。
*   **生物学验证**：虽然在 Benchmark 指标上领先，但单细胞去噪的生物学意义（如是否引入了伪影）仍需进一步的湿实验验证。
*   **成本门槛**：虽然比闭源模型便宜，但每个问题 500 美元的成本对于大规模部署仍有一定压力。

（完）
