# Martingale Foresight Sampling: A Principled Approach to Inference-Time LLM Decoding
# 鞅前瞻采样：一种规范的大语言模型推理时解码方法

**Authors**: Huayu Li, ZhengXiao He, Siyuan Tian, Jinghao Wen, Ao Li \
**Date**: 2026-01-21 \
**PDF**: https://arxiv.org/pdf/2601.15482v1 \
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-green">LNS</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 6.0 \
**Evidence**: 理论基础扎实的搜索与剪枝算法 \
**TLDR**: 引入鞅预见采样，通过理论化的搜索空间剪枝改进大模型解码。 \

---

## Abstract
Standard autoregressive decoding in large language models (LLMs) is inherently short-sighted, often failing to find globally optimal reasoning paths due to its token-by-token generation process. While inference-time strategies like foresight sampling attempt to mitigate this by simulating future steps, they typically rely on ad-hoc heuristics for valuing paths and pruning the search space. This paper introduces Martingale Foresight Sampling (MFS), a principled framework that reformulates LLM decoding as a problem of identifying an optimal stochastic process. By modeling the quality of a reasoning path as a stochastic process, we leverage Martingale theory to design a theoretically-grounded algorithm. Our approach replaces heuristic mechanisms with principles from probability theory: step valuation is derived from the Doob Decomposition Theorem to measure a path's predictable advantage, path selection uses Optional Stopping Theory for principled pruning of suboptimal candidates, and an adaptive stopping rule based on the Martingale Convergence Theorem terminates exploration once a path's quality has provably converged. Experiments on six reasoning benchmarks demonstrate that MFS surpasses state-of-the-art methods in accuracy while significantly improving computational efficiency. Code will be released at https://github.com/miraclehetech/EACL2026-Martingale-Foresight-Sampling.

## 摘要
大型语言模型（LLM）中标准的自回归解码

---

## 速览摘要（自动生成）

**问题**：传统大模型自回归解码存在短视问题，且现有的前瞻采样
