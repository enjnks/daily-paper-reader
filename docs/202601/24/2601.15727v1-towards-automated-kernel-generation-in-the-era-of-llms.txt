Title: Towards Automated Kernel Generation in the Era of LLMs

URL Source: https://arxiv.org/pdf/2601.15727v1

Published Time: Fri, 23 Jan 2026 01:29:41 GMT

Number of Pages: 9

Markdown Content:
# Towards Automated Kernel Generation in the Era of LLMs 

## Yang Yu 1 , Peiyu Zang 1,2 , Chi Hsu Tsai 1,3 , Haiming Wu 1,4 , Yixin Shen 1,5 ,

## Jialing Zhang 1,6 , Haoyu Wang 1,7 , Zhiyou Xiao 1,3 , Jingze Shi 8 , Yuyu Luo 8 ,

## Wentao Zhang 3 , Chunlei Men 1 , Guang Liu 1 and Yonghua Lin 11Beijing Academy of Artificial Intelligence 

> 2

## Beijing Normal University 

> 3

## Peking University 

> 4

## Beijing Institute of Technology 

> 5

## Cornell University 

> 6

## Beijing Jiaotong University 

> 7

## Renmin University of China 

> 8

## Hong Kong University of Science and Technology (Guangzhou) 

## Abstract 

The performance of modern AI systems is funda-mentally constrained by the quality of their un-derlying kernels, which translate high-level algo-rithmic semantics into low-level hardware oper-ations. Achieving near-optimal kernels requires expert-level understanding of hardware architec-tures and programming models, making kernel en-gineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating ker-nel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic sys-tems further enable scalable optimization by cast-ing kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing ap-proaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that under-pin learning and evaluation in this domain. More-over, key open challenges and future research di-rections are further outlined, aiming to establish a comprehensive reference for the next genera-tion of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation. 

## 1 Introduction 

The rapid scaling of large language models (LLMs) has placed efficient hardware utilization at the core of modern AI systems [Kaplan et al. , 2020]. To meet these demands, spe-cialized accelerators such as GPUs and NPUs have become the backbone of large-scale training and inference [Choquette 

et al. , 2021; Liao et al. , 2021]. At the core of these platforms are kernels that implement fundamental operations, includ-ing matrix multiplication and attention, which dominate ex-ecution time in LLM workloads. As a result, the end-to-end performance, efficiency, and cost of LLM systems are largely determined by kernel efficiency rather than hardware peak ca-pability. Despite their foundational role, the development of effi-cient kernels remains a formidable engineering challenge. Achieving near-peak hardware utilization requires deep ex-pertise in both algorithmic design and hardware-specific in-tricacies. Furthermore, kernel optimization is inherently non-scalable: implementations are often tightly coupled to par-ticular hardware architectures and workload characteristics, which hinders their reuse and generalization across different GPU generations or hardware vendors [Wu, 2023]. In response to these challenges, LLMs and LLM-based agents offer a transformative paradigm for kernel generation. By training on vast repositories of code and documentation, LLMs effectively compress expert-level “world knowledge” regarding hardware specifications, enabling them to bridge the semantic gap between high-level algorithms and low-level implementation details. Beyond static code generation, LLM-based agents excel in navigating the irregular optimiza-tion landscape through iterative refinement. This closed-loop approach not only drastically reduces the engineering but also generalizes across workloads and hardware configurations, toward a future of scalable, automated kernel discovery. As a result, LLMs and LLM-based agents are emerging as com-pelling foundations for the next generation of kernel genera-tion and optimization frameworks. Although the integration of LLMs and LLM-based agents into kernel generation marks a rapidly advancing frontier in AI systems research, the absence of a systematic survey has resulted in a fragmented research landscape. This survey ad-

> arXiv:2601.15727v1 [cs.LG] 22 Jan 2026

LLM4Kernel  Datasets 

> KernelLLM

Benchmarks   

> AutoTriton CUDA -L1
> TritonRL
> QiMeng -Kernel
> Kevin
> Nvidia
> Workflow
> CUDA -LLM GEAK
> FM Agent Astra
> EvoEngineer
> STARK
> KernelBand
> ParEval
> KernelBench
> MultiKernelBenc h
> TritonBench
> CUDAEval
> BackendBench
> ROCm -Bench
> Robust -kbench

Agent4Kernel   

> bits andbytes
> FlashInfer
> KB -samples
> KernelBook
> ConCuR
> 2025 -02
> 2025 -06
> 2025 -09
> 2025 -12
> KernelCoder
> SwizzlePerf
> KernelGen
> QiMeng -Attention
> CUDA -L2
> AI CUDA Engineer
> FlagGe ms
> FlagAttention
> Liger -Kernel
> cuTile
> AscendKernelGen DiffAgent MaxCode
> AKG
> FlashInfer
> -Bench

Figure 1: Illustration of the growth trend in the field of LLM-driven kernel generation. We organize these research works chronologically and categorically based on their publication dates and the domains they belong to. 

dresses this gap by presenting a unified overview of the field, clarifying foundational concepts, and highlighting emergent methodologies and trends. A key contribution is our consoli-dated resource infrastructure, featuring a structured compila-tion of training-ready kernel datasets and a literature collec-tion tailored for retrieval-augmented generation (RAG), de-signed to facilitate data-driven research in this specialized kernel-generation domain. Moving beyond a synthesis of existing methodologies, we also spotlight critical open chal-lenges and propose promising research directions, aiming to establish a foundational reference for the next generation of innovation in LLM-driven kernel generation. 

## 2 Background 

LLM and LLM-based Autonomous Agents. The foun-dation of modern LLMs is the Transformer architec-ture [Vaswani et al. , 2017], which functions as the probabilis-tic predictor trained via the next token prediction objective. Given a sequence of tokens x = ( x1, . . . , x T ), the model maximizes the joint probability: 

P (x) = 

> T

Y

> t=1

P (xt | x1, . . . , x t−1; θ).

This objective enables the model to internalize world knowl-edge and reasoning patterns implicitly during pretraining. While LLMs serve as the cognitive engine for reasoning and decision-making, autonomous agents extend this capa-bility by integrating additional system components such as planning, memory, and tool-use mechanisms [Wang et al. ,2024]. These components enable agents to decompose com-plex tasks, retain and retrieve long-term context, and inter-act with external environments. In this framework, the LLM functions as the “brain”, orchestrating actions through rea-soning strategies. And agents utilize tools such as compilers or interpreters to perform actions beyond the model’s internal knowledge. 

Kernel Programming and Code Generation. A kernel is the fundamental unit of GPU execution that concretizes high-level algorithmic semantics into hardware-level parallel oper-ations. Since CUDA made kernels explicitly programmable, GPUs have evolved into general-purpose computing plat-forms, supported by highly optimized libraries such as CUT-LASS [Thakkar et al. , 2017]. Nevertheless, writing high-performance custom kernels remains challenging, as it re-quires expert knowledge of hardware-specific optimization strategies. While higher-level abstractions—such as Triton and tile-based compiler frameworks—significantly improve programmability, achieving competitive performance still de-mands substantial domain expertise and often fails to trans-fer across heterogeneous accelerator platforms, underscoring the persistent gap between programmability and performance portability. In parallel, large language models have significantly ad-vanced code generation, evolving from simple completion to managing complex software engineering workflows. How-ever, kernel generation fundamentally differs from general-purpose code synthesis. While conventional code generation focuses on functional correctness, kernel generation must sat-isfy strict efficiency constraints and adapt to hardware exe-cution characteristics. As a result, kernel generation aligns more closely with performance-oriented program synthesis and compiler optimization than with standard software devel-opment, necessitating specialized generation methods beyond generic LLM-based code generation. 3 LLM for Kernels Generation 

Building on advances in LLM-driven code generation, re-cent work has increasingly applied LLMs to the generation of high-performance kernels. To highlight the methodological patterns that have emerged across this landscape, the follow-ing sections review two dominant families of post-training techniques used to specialize LLMs for kernel generation: su-pervised fine-tuning and reinforcement learning .

3.1 Supervised Fine-Tuning 

Supervised fine-tuning (SFT) has become a central method-ology for enabling LLMs to synthesize high-quality kernels, relying on paired datasets that capture both high-level compu-tational intent and low-level kernel implementation patterns. One influential line of work shows that the structure and clar-ity of model reasoning can strongly affect kernel correctness and performance. ConCuR [Kong et al. , 2025] demonstrates this by constructing a curated dataset in which training sam-ples are selected based on the conciseness of their reason-ing processes, the speedup they achieve, and the diversity of their computational tasks. Fine-tuning on such data leads to KernelCoder, a model capable of generating CUDA kernels with state-of-the-art reliability and efficiency. Another direc-tion builds paired training corpora through compiler align-ment, where kernel implementations are automatically gener-ated to mirror high-level operators. KernelLLM [Fisches et al. , 2025] adopts this strategy by using the Triton compiler to produce aligned PyTorch–Triton examples and by applying instruction tuning with structured prompts that explicitly en-code the mapping between computation and kernel structure. Together, these approaches show that well-designed super-vised datasets can effectively specialize LLMs for robust and high-performance GPU kernel synthesis. 

3.2 Reinforcement Learning 

Reinforcement learning enhances kernel generation via it-erative feedback. Kevin [Baronio et al. , 2025] models kernel generation as a multi-turn optimization using cross-turn reward attribution for long-horizon credit assignment. QiMeng-Kernel [Zhu et al. , 2025] further structures opti-mization by applying RL hierarchically to macro-thinking strategies rather than low-level implementation. Recent ap-proaches focus on robust reward mechanisms and verifiable evaluation. AutoTriton [Li et al. , 2025d] addresses reward sparsity by combining structural assessments of generated kernels with execution-based runtime rewards, while Tri-tonRL [Woo et al. , 2025] extends this line of work through hierarchical reward decomposition and explicit verification of code outputs and intermediate reasoning traces. CUDA-L1 introduces contrastive RL with an LLM-as-a-judge for dense feedback, and is refined by CUDA-L2 [Su et al. , 2025] to sur-pass cuBLAS performance. Finally, AscendKernelGen [Cao 

et al. , 2026] expands the preference learning paradigm to Ascend NPUs, combining CoT-based SFT with preference learning. 

## 4 LLM Agent for Kernels Generation 

Relying on foundational LLMs alone typically reduces kernel development to a static, one-pass inference process. In con-trast, LLM-based agents introduce autonomy and feedback into the optimization loop by enabling planning, tool use, and evaluation of intermediate results. This closed-loop, self-improving paradigm allows agent-based approaches to scale kernel optimization across diverse workloads and hardware platforms, while sustaining long-horizon, fatigue-free explo-ration. Concretely, we categorized recent agent-driven ad-vancements into four structural dimensions: learning mecha-nisms , external memory management , hardware profiling in-tegration , and multi-agent orchestration .

4.1 Learning Mechanisms 

The first dimension of advancement concerns search strate-gies. Initial approaches view kernel generation as iterative refinement. Caesar (in [Ouyang et al. , 2025]) utilizes sim-ple feedback loops to refine kernels, while Inference-Time Scaling [Chen et al. , 2025b] demonstrates that scaling test-time compute and reflection significantly boost kernel quality. To manage complexity, PEAK [Tariq et al. , 2025] employs a stepwise, modular iterative refinement strategy, and “Mini-mal Executable Programs” is proposed [Chu et al. , 2025] to enable efficient, isolated iteration without building costly full-scale applications. DiffAgent [Zhu et al. , 2026] adopts itera-tive refinement to accelerate diffusion models, TritonX [Ham-mond et al. , 2025] uses iterative refinement within a state ma-chine to cover kernels of complete PyTorch ATen backends, and KernelGen [BAAI, 2025] leverages test-time scaling and reflection techniques to enable kernel generation for multi-chip backends. MaxCode [Ou et al. , 2026] further unifies existing iterative search methods under a max-reward rein-forcement learning framework, combined with a natural lan-guage critique model converting raw execution feedback into diagnostic insights. To escape local optima, recent frameworks adopt population-based evolution. Lange et al. [Lange et al. , 2025b] optimize translation CUDA via mutation and crossover. FM Agent [Li et al. , 2025a] includes an evolutionary stage with the principles of diversity preservation, adaptive evolution, and multi-population dynamics. Advanced population dy-namics are also introduced in EvoEngineer [Guo et al. , 2025], which decouples traversal techniques from population man-agement. GPU Kernel Scientist [Andrews and Witteveen, 2025] employs a multi-stage evolutionary workflow to ad-dress the challenge of optimizing HIP kernels for the AMD accelerators. And cuPilot [Chen et al. , 2025a] guides evolu-tion through high-level semantic strategies. 

4.2 External Memory Management 

Complex kernel optimization often requires domain-specific knowledge, such as CUDA APIs and hardware instruction sets that may be hallucinated or forgotten by the LLM. Agents in this category augment generation with external memory. The AI CUDA Engineer [Lange et al. , 2025a] leverages a vector database of high-quality kernel examples to ground the LLM’s generation, ensuring syntactic correctness and ad-herence to best practices in low-level programming. Ker-nelEvolve [Liao et al. , 2025] further advances the external knowledge management paradigm by integrating a sophisti-cated hardware-specific knowledge base specifically tailored for heterogeneous AI accelerators. Beyond retrieving un-structured textual context, recent work has explored utilizing structured representations as external memory to guide model inference. Work such as ReGraphT [Gong et al. , 2025] pro-poses a novel framework that treats a reasoning graph as a domain-specific external memory for CUDA code optimiza-tion. In this approach, the logical transitions between op-timization states of large language models are externalized into a static, navigable graph structure for the small language model to retrieve. 

4.3 Hardware Profiling Integration 

The third dimension addresses the hardware-agnostic nature of standard LLMs by configuring the agent’s persona profile with hardware specifications, and iteratively reasoning over performance profiling feedback. QiMent-TensorOp [Zhang et al. , 2025a] triggers LLMs to analyze and distill low-level hardware documentation ac-cording to user input into the generation prompt, while QiMeng-GEMM [Zhou et al. , 2025b] generates General Ma-trix Multiplication (GEMM) with the meta-prompt, which of-fers universal templates for various general optimization tech-niques and platform-specific optimization details. QiMeng-Attention [Zhou et al. , 2025a] considers target GPU architec-ture and instruction set to convert the high-level thinking lan-guage into low-level CUDA code, and implements the high-performance FlashAttention on different GPUs. SwizzlePerf [Lei et al. , 2025] explicitly tackles the swizzling problem, which explicitly injects precise architectural specifications into the prompt context and restricts the search space specifi-caslly to swizzling patterns that focuses solely on maximizing the L2 cache hit rate. Complementing this, agents leverage dynamic feedback. CUDA-LLM [Chen et al. , 2025c] incorporates detailed tar-get GPU specifications (e.g., warp size, cache size) into the agent’s prompt. Simultaneously, compilation logs and run-time performance metrics are also aggregated to guide the optimization process. TritonForge [Li et al. , 2025b] uti-lizes profiling-guided feedback loops to iteratively analyze and identify performance bottlenecks. PRAGMA [Lei et al. ,2025] uses a specialized profiling module to parse low-level quantitative metrics into the interpretable natural language suggestion. KERNELBAND [Ran et al. , 2025] clusters run-time behavior of potential kernels to reduce the exploration space and utilizes profiling data as context to guide the opti-mization strategies selection. 

4.4 Multi-Agent Orchestration 

Recognizing that kernel development inherently involves het-erogeneous skills ranging from algorithmic planning to low-level coding and debugging, recent works increasingly adopt multi-agent designs that explicitly decompose these respon-sibilities into coordinated roles. STARK [Dong et al. , 2025] structures generation into Plan-Code-Debug phases to emulate human workflows, while AKG [Du et al. , 2025] leverages similar modularity to achieve cross-platform synthesis. Astra [Wei et al. , 2025] specializes this approach for production-grade SGLang ker-nels, focusing on tuning-focused agents. CudaForge [Zhang 

et al. , 2025b] employs a Coder-Judge loop driven by hardware-level feedback, whereas KForge [Sereda et al. ,2025] adapts this dual-agent model to new platforms using only single-shot example supervision. Addressing scale, Ker-nelFalcon [Team and Contributors, 2024] employs a multi-agent system to tackle the challenge of GPU kernel genera-tion of full machine learning architectures, where the system specifically addresses hierarchical task decomposition and delegation through coordinated manager and worker agents. Conversely, GEAK [Wang et al. , 2025] targets AMD GPUs, integrating generation and reflection within a Triton-based workflow. 

## 5 Datasets for LLM-Based Kernel Generation 

The efficacy of Large Language Models (LLMs) in high-performance kernel generation relies critically on the avail-ability of domain-specific data. Unlike general software en-gineering, kernel generation requires models to internalize hardware intrinsics, parallel execution semantics, and mem-ory hierarchy constraints. In this section, we survey the data landscape and organize resources into two categories: (1) 

Training Corpora , covering both structured datasets and raw kernel repositories; (2) Knowledge Bases , which we identify as essential for grounding RAG systems. The training data consists of targeted, structure-aware cu-ration and unstructured repositories. Structured datasets rep-resent the highest-value signal for instruction tuning, as they explicitly pair intent with optimization. Open-source reposi-tories contain the vast majority of domain knowledge, where optimized kernel code can be extracted and cleaned from open-source operator and kernel libraries, integrated frame-work or system, and domain-specific languages’ tutorials and reference implementations. Beyond executable code, domain knowledge base also plays a critical role in LLM-driven ker-nel generation. Such knowledge can be distilled into pre-training corpora to enrich model understanding, or integrated as external knowledge bases to support agent-based systems, where the corpora is always provided in authoritative docu-mentation and guides, as well as in community indices or tu-torials. A comprehensive index is provided in Table 1. Note that the dates listed in the table correspond to the initial re-lease, where these libraries are under active development. 

## 6 Benchmark 

This chapter focuses on the systematic benchmarking of ker-nel generation, and provides a structured overview of repre-sentative evaluation benchmarks, including both evaluation metrics and benchmark datasets, to lay a solid foundation for subsequent method comparison and performance analysis. 

6.1 Metrics 

Several factors should be considered when evaluating the per-formance of the operator implementation: correctness, effi-ciency, and so on. To build a comprehensive evaluation, ex-isting benchmarks generally adopt execution-based unit tests, where the generated kernels will be compared with the stan-dard implementations of CUDA/PyTorch. Given the instabil-ity of operator generation, each testing task usually involves Data Resource Description Access 

I. Structured Datasets (Hugging Face & Benchmarks) 

02/2024 The Stack v2 [Lozhkov et al. , 2024] Unsupervised CUDA/Triton Corpus [Data] 06/2024 HPC-Instruct [HPC-AI Tech, 2024] Instructions for CUDA/MPI/OpenMP [Data] 05/2025 KernelBook [Paliskara and Saroufim, 2025] Torch-Triton Aligned Corpus [Data] 02/2025 KernelBench samples Kernel Code Snapshots and Profiling Data [Data] 

II. Code-Centric Corpora (GitHub Repositories) 

Layer 1: High-Performance Operator Libraries 

12/2017 CUTLASS CUDA C++ Template Library for Matrix Ops [Code] 05/2022 FlashAttention [Dao et al. , 2022] Fast and Memory-Efficient Exact Attention [Code] 11/2023 FlagAttention [FlagOpen Team, 2023] Memory Efficient Attention Operators in Triton [Code] 02/2024 AoTriton [AMD, 2024] AOT-compiled Triton kernels for AMD ROCm [Code] 11/2021 xFormers [Lefaudeux et al. , 2022] Hackable and Optimized Transformer Blocks [Code] 08/2024 Liger-Kernel [LinkedIn, 2024] Efficient Triton Kernels for LLM Training [Code] 04/2024 FlagGems [FlagOpen Team, 2024] Triton-based Operator Library for LLMs [Code] 09/2022 Bitsandbytes [Dettmers et al. , 2022] K-bit Quantization Kernels for LLMs [Code] 09/2024 Gemlite [Dropbox, Inc., ] Low-Bit Matrix Multiplication Triton Kernels [Code] 01/2025 FlashInfer [Ye et al. , 2025] Kernel Library for Efficient LLM Serving [Code] 05/2021 FBGEMM [Khudia et al. , 2021] Low-Precision Matrix Multiplication [Code] 09/2022 Transformer Engine [NVIDIA, 2022] Acceleration Library for Transformer Models [Code] 

Layer 2: Framework & System Integration 

10/2016 PyTorch (ATen) Foundational Tensor Library for C++ and Python [Code] 06/2023 vLLM High-Efficient Serving Engine [Code] 12/2023 SGLang Structured Generation Language for LLMs [Code] 03/2023 llama.cpp LLM Inference in C/C++ [Code] 08/2023 TensorRT-LLM TensorRT Toolbox for LLM Inference [Code] 10/2019 DeepSpeed System for Large Scale Model Training [Code] 

Layer 3: Domain-Specific Languages 

07/2019 Triton Open-Source GPU Programming Language [Code] 04/2024 TileLang Tile-based Optimization Language [Code] 12/2025 cuTile NVIDIA’s DSL for Tile-centric Programming [Link] 

III. Knowledge Bases & Educational Resources 

Documentation & Guides 

06/2007 CUDA Guide CUDA C++ Programming Guide [Docs] 06/2007 PTX ISA PTX ISA Reference [Docs] 05/2020 Tuning Guides NVIDIA Architecture Tuning Guides [Docs] 

Community Indices & Tutorials 

01/2024 GPU-MODE Resource Stream & KernelBook [List] 01/2024 Triton Index Community Index for Triton Optimization [List] 06/2016 Awesome-CUDA Community Curated List for CUDA [List] 12/2023 Awesome-GPU Awesome GPU Engineering List [List] 05/2023 LeetCUDA CUDA Programming Exercises [Code] 01/2023 Triton-Puzzles Puzzles for Learning Triton [Code] 01/2011 Colfax Research Technical Hub Dedicated to HPC and AI [Link] 09/2018 Nsight Compute Kernel Profiling Guide [Docs] 

> Table 1: A structured overview of training corpora and kernel knowledge bases. Note that the dates in the table correspond to the initial release; the libraries themselves continue to undergo active development.

multiple evaluations across k random samples among n times of generation. 

Correctness primarily includes two aspects based on dif-ficulty: (1) successful compilation and (2) consistency with the reference in multiple input-output comparisons. Among various metrics used in code generation, pass@ k is widely chosen, which calculates the probability that at least one cor-rect implementation is generated in k trials. The standard estimator is defined as: pass@ k ≜ E



1 −

n − ck



/

nk

 

, (1) where the expectation is taken over kernel tasks and prompts, 

c is the number of correct kernel implementations. Name Time Metrics Hardware Description 

ParEval 01/2024 C S  

> E
> NA

420 expert-selected tasks across 12 algorithmic domains for benchmarking general parallel code generation. KernelBench 02/2025 C f N 250 PyTorch-to-CUDA kernel generation tasks, curated from popular GitHub repositories and official PyTorch operators, for evaluating AI/DL kernel generation. TritonBench 02/2025 C S S  

> E*
> N

TritonBench evaluates Triton kernel generation via two sub-sets: 184 high-level kernels sourced from popular GitHub projects (TritonBench-G) and 166 fusion tasks derived from diverse PyTorch operators with different frequencies of usage (TritonBench-T). MultiKernel-Bench 07/2025 C S H N G 285-task benchmark across 14 operator categories for multi-platform DL kernel synthesis. TritonBench-revised & ROCm Benchmark 07/2025 C S A An AMD GPU-centric evaluation dataset comprising 30 expert-verified ROCm kernels and an adapted version of TritonBench-G, specifically optimized for AMD GPU perfor-mance benchmarking. Robust-kbench 09/2025 C S N A robustness-focused benchmark featuring 9 specialized deep learning task categories, derived by refining and extending Ker-nelBench. BackendBench 09/2025 C S N A rigorous evaluation framework that enforces PyTorch’s offi-cial core library standards on 271 operators. Current use cases primarily leverage NVIDIA CUDA and Triton, yet the archi-tecture remains backend-agnostic. CUDAEval 10/2025 C S N Leveraging 313 curated tasks from the the Stack v2 to bench-mark the efficacy of reasoning transfer in CUDA code opti-mization. FlashInfer-Bench 01/2026 C f N Provide a unified schema describing kernel definitions, work-loads, implementations, and evaluations, including eight repre-sentative kernel types used in LLM inference.  

> *

Efficiency here is defined as the ratio of the operator’s measured throughput to the theoretical maximum performance.                    

> Table 2: Benchmark datasets for kernel generation and optimization. Metrics: CCorrectness, SSpeedup, EEfficiency, ff ast p,P
> Perf, SSimilarity. Hardware Platforms: NNVIDIA GPUs, HHUAWEI NPUs, GGoogle TPUs, AAMD GPUs.

Efficiency is another principal goal that kernel evaluation focuses on. Speedup@ k measures how much faster a gener-ated implementation is compared with the baselines by cal-culating speedup@ k ≜ E



> n

X

> j=1

 j − 1

k − 1



T base 



/

 nk



Tj

 ,

(2) where Tj is the running time of the j-th generated implemen-tation while T base is the time consumed by the baseline. Note that the implementations are sorted by their performance, i.e., 

T1 corresponds to the slowest and Tn to the fastest. In addition, Efficiency@ k refers to how effectively the generated operators utilize computation resources during ex-ecution, and Compatibility is considered when evaluating op-erator generation techniques across different hardware plat-forms or languages. Combined metrics are also used to eval-uate multiple aspects of performance. For example, Perf @K

measures how close the best result from K generated kernels is to a human expert performance. The f ast p jointly eval-uates the functional correctness and runtime performance of generated kernels. Similarity uses 4 items (n-gram, weighted n-gram, syntax and dataflow) to measure the similarity be-tween the generated code and the reference code. 

6.2 Benchmark Datesets 

As summarized in Table 2, kernel benchmarks are evolving from simple, single-platform evaluations toward comprehen-sive, real-world and generalized operator evaluation. We ob-serve the following three key trends. 

Metrics. Moving beyond basic correctness and raw speedup (ParEval [Nichols et al. , 2024], KernelBench), re-cent suites adopt composite objectives. Examples include ef-ficiency metrics in TritonBench [Li et al. , 2025c] and robust-ness assessments in Robust-kbench. 

Hardware. Evaluation is expanding beyond NVIDIA ex-clusivity. Compared to early benchmarks such as ParEval and KernelBench exclusively targeting NVIDIA GPUs, Mul-tiKernelBench [Wen et al. , 2025] integrates HUAWEI NPUs and Google TPUs, while TritonBench-revised targets AMD GPUs. Additionally, NPUEval [Kalade and Schelle, 2025] specifically targets power-sensitive kernels of neural process-ing units. Content. Workloads are shifting from generic algorithms to production-grade traces. KernelBench and TritonBench em-phasize real-world PyTorch-to-CUDA or Triton kernel gen-eration curated from popular GitHub repositories and The Stack v2. FlashInfer-Bench [Xing et al. , 2026] standard-izes 1,600 real-world LLM serving workloads, and Backend-Bench [Saroufim et al. , 2025] targets complex edge cases. 

## 7 Challenges and Opportunities 

While the integration of LLMs and agents has shown strong potential for automating kernel generation, the field remains at an early stage of development. Bridging the gap between promising prototypes and production-grade systems requires addressing a set of interrelated challenges. This section ex-amines these challenges and highlights emerging research di-rections spanning data, agents, infrastructure, evaluation, and human–AI collaboration, which are likely to shape the next generation of AI-driven kernel generation and optimization systems. 

Data Scarcity and Synthetic Scaling. Progress toward production-grade performance remains fundamentally con-strained by data scarcity. High-performance kernels exhibit a pronounced long-tail distribution and are sparsely repre-sented in existing code corpora, where most available datasets still lack deep, hardware-aware domain knowledge, and exist-ing corpora predominantly capture only final optimized ker-nels but omit optimization trajectories. Promising directions to access these limitations include systematic kernel dataset construction, large-scale synthetic data generation, and the collection of execution-driven optimization processes. Such data can support a wide range of learning paradigms, includ-ing pretraining, supervised fine-tuning, and reinforcement learning, and may be crucial for enabling meaningful scaling behavior in kernel generation systems. 

Agentic Reasoning and Engineering Standards . Cur-rent agent-based kernel optimization relies on predefined, workflow-driven paradigms, often failing long-horizon tasks due to redundant exploration and context exhaustion. To tran-scend these limitations, we propose three critical advance-ments: (1) enhancing autonomy by shifting from handcrafted workflows to self-directed planning and dynamic memory; (2) enabling principled reasoning by integrating dispersed heuristics across documentation and experts into structured knowledge bases; and (3) ensuring reliability through rigor-ous engineering standards, including formal verification and strict specifications. Collectively, addressing these challenges is critical for transforming agentic kernel optimization from exploratory automation into a robust, engineering-grade ca-pability. 

Scalable Infrastructure for Synthesis and Training. 

Scalable infrastructure remains a bottleneck due to the se-vere latency mismatch between rapid model inference and costly kernel compilation. This disparity hinders the high-throughput feedback loops essential for reinforcement learn-ing and synthetic data generation. Addressing this challenge calls for infrastructure that cleanly decouples model reason-ing from environment execution via standardized, distributed “gym-like” environments, while supporting distributed and asynchronous execution at scale. Ultimately, advances in scalable infrastructure are critical for transforming kernel synthesis and data sampling from low-throughput experimen-tation into a systematic, data-driven learning process. 

Evaluation Robustness and Generalization. A key open challenge in AI-driven kernel generation is the lack of ro-bust and comprehensive evaluation. A critical deficit in AI-driven kernel generation is the lack of robust evaluation. Ex-isting benchmarks are often confined to fixed input shapes and forward-pass primitives within the NVIDIA ecosystem, failing to reflect the diversity of real-world workloads. Ad-dressing these gaps requires evaluation protocols that jointly assess robustness and generalization across shapes, opera-tors, and ecosystems, providing a more reliable foundation for measuring progress in kernel generation research. 

Human-AI Collaboration for Kernel Generation. Be-yond fully automated approaches, human–AI collaboration represents an important and complementary paradigm for kernel generation. An open research question is how to sys-tematically combine agentic exploration with human exper-tise to expand the design space and improve controllability in performance-critical settings. To operationalize this, we identify two critical requirements: (1) Explainability, where agents provide interpretable rationales for optimization de-cisions (e.g., tiling) to facilitate expert verification; and (2) Mixed-initiative interaction, a paradigm where humans spec-ify high-level constraints while agents execute implementa-tion and tuning. Establishing this principled division of labor is essential to balance controllability with the scalability of automation. 

## 8 Conclusion 

This survey highlights the transformative potential of large language models and agentic workflows for automating high-performance kernel generation, synthesizing recent advances in supervised fine-tuning, reinforcement learning, and multi-agent orchestration, together with progress in kernel-centric dataset and benchmark development. Looking ahead, fu-ture work should move beyond rigid workflows toward self-evolving agentic reasoning with strong hardware generaliza-tion. Such a shift is essential not only to alleviate the burden of manual kernel engineering, but also to unlock substantial productivity gains in the face of rapidly scaling AI infrastruc-ture 

## References 

[AMD, 2024] AMD. Aotriton: Pre-compiled triton kernels for rocm. https://github.com/ROCm/aotriton, 2024. [Andrews and Witteveen, 2025] Martin Andrews and Sam Witteveen. Gpu kernel scientist: An llm-driven frame-work for iterative kernel optimization. arXiv preprint arXiv:2506.20807 , 2025. [BAAI, 2025] BAAI. KernelGen. https://github.com/ flagos-ai/KernelGen, 2025. [Baronio et al. , 2025] Carlo Baronio, Pietro Marsella, et al. Kevin: Multi-turn rl for generating cuda kernels. 2025. [Cao et al. , 2026] Xinzi Cao, Jianyang Zhai, et al. As-cendkernelgen: A systematic study of llm-based kernel generation for neural processing units. arXiv preprint arXiv:2601.07160 , 2026. [Chen et al. , 2025a] Jinwu Chen, Qidie Wu, et al. cupilot: A strategy-coordinated multi-agent framework for cuda ker-nel evolution. arXiv preprint arXiv:2512.16465 , 2025. [Chen et al. , 2025b] Terry Chen, Bing Xu, et al. Automating gpu kernel generation with deepseek-r1 and inference time scaling. NVIDIA Developer Blog, 2025. [Chen et al. , 2025c] Wentao Chen, Jiace Zhu, et al. Cuda-llm: Llms can write efficient cuda kernels. arXiv preprint arXiv:2506.09092 , 2025. [Choquette et al. , 2021] Jack Choquette, Wishwesh Gandhi, et al. Nvidia a100 tensor core gpu: Performance and inno-vation. IEEE Micro , 41(2):29–35, 2021. [Chu et al. , 2025] Ruifan Chu, Anbang Wang, et al. Gpu kernel optimization beyond full builds: An llm frame-work with minimal executable programs. arXiv preprint arXiv:2512.22147 , 2025. [Dao et al. , 2022] Tri Dao, Dan Fu, et al. Flashatten-tion: Fast and memory-efficient exact attention with io-awareness. NeurIPS , 35:16344–16359, 2022. [Dettmers et al. , 2022] Tim Dettmers, Mike Lewis, et al. Llm.int8(): 8-bit matrix multiplication for transformers at scale. NeurIPS , 2022. [Dong et al. , 2025] Juncheng Dong, Yang Yang, et al. Stark: Strategic team of agents for refining kernels. arXiv preprint arXiv:2510.16996 , 2025. [Dropbox, Inc., ] Dropbox, Inc. Gemlite: A lightweight machine learning framework for efficient model serving. https://github.com/dropbox/gemlite. [Du et al. , 2025] Jinye Du, Quan Yuan, et al. Akg kernel agent: A multi-agent framework for cross-platform kernel synthesis, 2025. [Fisches et al. , 2025] Zacharias V. Fisches, Sahan Paliskara, et al. Kernelllm: Making kernel development more acces-sible, 6 2025. [FlagOpen Team, 2023] FlagOpen Team. Flagattention: A collection of memory efficient attention operators im-plemented in the triton language. https://github.com/ flagos-ai/FlagAttention, 2023. Accessed: 2025-12-30. [FlagOpen Team, 2024] FlagOpen Team. Flagopen/flaggems: Flaggems is an operator library for large language models implemented in the triton language., 2024. [Gong et al. , 2025] Junfeng Gong, Zhiyi Wei, et al. From large to small: Transferring cuda optimization expertise via reasoning graph. arXiv preprint arXiv:2510.19873 ,2025. [Guo et al. , 2025] Ping Guo, Chenyu Zhu, et al. Evo-engineer: Mastering automated cuda kernel code evo-lution with large language models. arXiv preprint arXiv:2510.03760 , 2025. [Hammond et al. , 2025] Alec M Hammond, Aram Markosyan, et al. Agentic operator generation for ml asics. arXiv preprint arXiv:2512.10977 , 2025. [HPC-AI Tech, 2024] HPC-AI Tech. hpc-instruct: A dataset for hpc instruction tuning. https://huggingface.co/datasets/ hpcgroup/hpc-instruct, 2024. Hugging Face Dataset. [Kalade and Schelle, 2025] Sarunas Kalade and Graham Schelle. Npueval: Optimizing npu kernels with llms and open source compilers. arXiv preprint arXiv:2507.14403 ,2025. [Kaplan et al. , 2020] Jared Kaplan, Sam McCandlish, et al. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020. [Khudia et al. , 2021] Daya Khudia, Jianyu Huang, et al. Fbgemm: Enabling high-performance low-precision deep learning inference. arXiv preprint arXiv:2101.05615 ,2021. [Kong et al. , 2025] Lingcheng Kong, Jiateng Wei, et al. Con-cur: Conciseness makes state-of-the-art kernel generation. 

CoRR , abs/2510.07356, 2025. [Lange et al. , 2025a] Robert Tjarko Lange, Aaditya Prasad, et al. The ai cuda engineer: Agentic cuda kernel discovery, optimization and composition. Technical report, Sakana AI, 2025. [Lange et al. , 2025b] Robert Tjarko Lange, Qi Sun, et al. To-wards robust agentic cuda kernel benchmarking, verifica-tion, and optimization. arXiv preprint arXiv:2509.14279 ,2025. [Lefaudeux et al. , 2022] Benjamin Lefaudeux, Francisco Massa, et al. xformers: A modular and hack-able transformer modelling library. https://github.com/ facebookresearch/xformers, 2022. [Lei et al. , 2025] Kelun Lei, Hailong Yang, et al. Pragma: A profiling-reasoned multi-agent framework for automatic kernel optimization. arXiv preprint arXiv:2511.06345 ,2025. [Li et al. , 2025a] Annan Li, Chufan Wu, et al. The fm agent. 

arXiv preprint arXiv:2510.26144 , 2025. [Li et al. , 2025b] Haonan Li, Keyu Man, et al. Tritonforge: Profiling-guided framework for automated triton kernel optimization. arXiv preprint arXiv:2512.09196 , 2025. [Li et al. , 2025c] Jianling Li, ShangZhan Li, et al. Triton-bench: Benchmarking large language model capabilities for generating triton operators. In ACL , pages 23053– 23066, 2025. [Li et al. , 2025d] Shangzhan Li, Zefan Wang, et al. Autotri-ton: Automatic triton programming with reinforcement learning in llms. arXiv preprint arXiv:2507.05687 , 2025. [Liao et al. , 2021] Heng Liao, Jiajin Tu, et al. Ascend: ascalable and unified architecture for ubiquitous deep neu-ral network computing: Industry track paper. In 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA) , pages 789–801. IEEE, 2021. [Liao et al. , 2025] Gang Liao, Hongsen Qin, et al. Kernele-volve: Scaling agentic kernel coding for heterogeneous ai accelerators at meta, 2025. [LinkedIn, 2024] LinkedIn. Liger-kernel: Efficient triton kernels for llm training. https://github.com/linkedin/ Liger-Kernel, 2024. [Lozhkov et al. , 2024] Anton Lozhkov, Raymond Li, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 , 2024. [Nichols et al. , 2024] Daniel Nichols, Joshua H. Davis, et al. Can large language models write parallel code? In HPDC ,HPDC ’24, page 281–294, New York, NY, USA, 2024. Association for Computing Machinery. [NVIDIA, 2022] NVIDIA. Transformer engine: An nvidia library for accelerating transformer training with fp8. https://github.com/NVIDIA/TransformerEngine, 2022. Open-source library for FP8-based Transformer training and inference. [Ou et al. , 2026] Jiefu Ou, Sapana Chaudhary, et al. Max-code: A max-reward reinforcement learning frame-work for automated code optimization. arXiv preprint arXiv:2601.05475 , 2026. [Ouyang et al. , 2025] Anne Ouyang, Simon Guo, et al. Ker-nelbench: Can LLMs write efficient GPU kernels? In 

ICML , 2025. [Paliskara and Saroufim, 2025] Sahan Paliskara and Mark Saroufim. Kernelbook, 5 2025. [Ran et al. , 2025] Dezhi Ran, Shuxiao Xie, et al. Kernel-band: Boosting llm-based kernel optimization with a hi-erarchical and hardware-aware multi-armed bandit. arXiv preprint arXiv:2511.18868 , 2025. [Saroufim et al. , 2025] Mark Saroufim, Jiannan Wang, et al. Backendbench: An evaluation suite for testing how well llms and humans can write pytorch backends, 2025. [Sereda et al. , 2025] Taras Sereda, Tom St John, et al. Kforge: Program synthesis for diverse ai hardware accel-erators. arXiv preprint arXiv:2511.13274 , 2025. [Su et al. , 2025] Songqiao Su, Xiaofei Sun, et al. Cuda-l2: Surpassing cublas performance for matrix multiplication through reinforcement learning, 2025. [Tariq et al. , 2025] Muhammad Usman Tariq, Abhinav Jangda, et al. Peak: A performance engineering ai-assistant for gpu kernels powered by natural language transformations. arXiv preprint arXiv:2512.19018 , 2025. [Team and Contributors, 2024] PyTorch Team and Contrib-utors. Kernelfalcon: Autonomous GPU kernel generation via deep agents, 2024. Accessed: 2026-01-02. [Thakkar et al. , 2017] Vijay Thakkar, Pradeep Ramani, et al. CUTLASS: CUDA templates for linear algebra subrou-tines, 2017. Version 3.x, accessed 2025. [Vaswani et al. , 2017] Ashish Vaswani, Noam Shazeer, et al. Attention is all you need. NeurIPS , 30, 2017. [Wang et al. , 2024] Lei Wang, Chen Ma, et al. A survey on large language model based autonomous agents. Frontiers of Computer Science , 18(6):186345, 2024. [Wang et al. , 2025] Jianghui Wang, Vinay Joshi, et al. Geak: Introducing triton kernel ai agent & evaluation bench-marks. arXiv preprint arXiv:2507.23194 , 2025. [Wei et al. , 2025] Anjiang Wei, Tianran Sun, et al. Astra: A multi-agent system for gpu kernel performance optimiza-tion. arXiv preprint arXiv:2509.07506 , 2025. [Wen et al. , 2025] Zhongzhen Wen, Yinghui Zhang, et al. Multikernelbench: A multi-platform benchmark for kernel generation. arXiv eprints, pp. arXiv–2507 , 2025. [Woo et al. , 2025] Jiin Woo, Shaowei Zhu, et al. Tritonrl: Training llms to think and code triton without cheating. 

arXiv preprint arXiv:2510.17891 , 2025. [Wu, 2023] Peng Wu. Pytorch 2.0: The journey to bring-ing compiler technologies to the core of pytorch (keynote). In Proceedings of the 21st ACM/IEEE International Sym-posium on Code Generation and Optimization , CGO ’23, page 1, New York, NY, USA, 2023. Association for Com-puting Machinery. [Xing et al. , 2026] Shanli Xing, Yiyan Zhai, et al. Flashinfer-bench: Building the virtuous cycle for ai-driven llm systems. arXiv preprint arXiv:2601.00227 ,2026. [Ye et al. , 2025] Zihao Ye, Lequn Chen, et al. Flashinfer: Ef-ficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005 , 2025. [Zhang et al. , 2025a] Xuzhi Zhang, Shaohui Peng, et al. Qimeng-tensorop: Automatically generating high-performance tensor operators with hardware primitives. 

arXiv preprint arXiv:2505.06302 , 2025. [Zhang et al. , 2025b] Zijian Zhang, Rong Wang, et al. Cudaforge: An agent framework with hardware feed-back for cuda kernel optimization. arXiv preprint arXiv:2511.01884 , 2025. https://arxiv.org/abs/2511. 01884. [Zhou et al. , 2025a] Qirui Zhou, Shaohui Peng, et al. QiMeng-attention: SOTA attention operator is generated by SOTA attention algorithm. In Findings of ACL , pages 8491–8505, Vienna, Austria, July 2025. Association for Computational Linguistics. [Zhou et al. , 2025b] Qirui Zhou, Yuanbo Wen, et al. Qimeng-gemm: Automatically generating high-performance matrix multiplication code by exploiting large language models. In AAAI , volume 39, pages 22982–22990, 2025. [Zhu et al. , 2025] Xinguo Zhu, Shaohui Peng, et al. Qimeng-kernel: Macro-thinking micro-coding paradigm for llm-based high-performance gpu kernel generation. arXiv preprint arXiv:2511.20100 , 2025. [Zhu et al. , 2026] Haowei Zhu, Puyuan Yang, et al. Diff-bench meets diffagent: End-to-end llm-driven diffu-sion acceleration code generation. arXiv preprint arXiv:2601.03178 , 2026.