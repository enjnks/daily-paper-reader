# Inference-Time Scaling of Verification: Self-Evolving Deep Research Agents via Test-Time Rubric-Guided Verification
# 推理时验证扩展：通过测试时评分标准引导的验证实现自进化深度研究智能体

**Authors**: Yuxuan Wan, Tianqing Fang, Zaitang Li, Yintong Huo, Wenxuan Wang, Haitao Mi, Dong Yu, Michael R. Lyu \
**Date**: 2026-01-22 \
**PDF**: https://arxiv.org/pdf/2601.15808v1 \
**Tags**: <span class="tag-label tag-green">速读区</span> <span class="tag-label tag-green">EOH</span> <span class="tag-label tag-green">EAA</span> \
**Score**: 7.0 \
**Evidence**: 通过迭代反馈和细化实现自我进化的智能体 \
**TLDR**: 一种通过迭代验证和准则引导反馈实现自我进化的智能体框架。

---

## 速览
**TLDR**：提出 DeepVerifier，通过基于细则的推理阶段验证实现深度研究智能体的自我进化。
**Motivation**：现有研究多关注训练后策略增强，缺乏在推理阶段通过自我验证和反馈来提升智能体性能的有效手段。
**Method**：基于自动构建的失败分类体系制定评分细则，开发 DeepVerifier 模块在推理时提供反馈并引导智能体迭代优化。
**Result**：在 GAIA 和 XBench-DeepResearch 任务中准确率提升 8%-11%，元评估 F1 分数显著优于现有基准。
**Conclusion**：推理阶段的验证缩放是提升研究智能体能力的有效范式，且开源数据集 DeepVerifier-4K 将助力该领域发展。

---

## Abstract
Recent advances in Deep Research Agents (DRAs) are transforming automated knowledge discovery and problem-solving. While the majority of existing efforts focus on enhancing policy capabilities via post-training, we propose an alternative paradigm: self-evolving the agent's ability by iteratively verifying the policy model's outputs, guided by meticulously crafted rubrics. This approach gives rise to the inference-time scaling of verification, wherein an agent self-improves by evaluating its generated answers to produce iterative feedback and refinements. We derive the rubrics based on an automatically constructed DRA Failure Taxonomy, which systematically classifies agent failures into five major categories and thirteen sub-categories. We present DeepVerifier, a rubrics-based outcome reward verifier that leverages the asymmetry of verification and outperforms vanilla agent-as-judge and LLM judge baselines by 12%-48% in meta-evaluation F1 score. To enable practical self-evolution, DeepVerifier integrates as a plug-and-play module during test-time inference. The verifier produces detailed rubric-based feedback, which is fed back to the agent for iterative bootstrapping, refining responses without additional training. This test-time scaling delivers 8%-11% accuracy gains on challenging subsets of GAIA and XBench-DeepResearch when powered by capable closed-source LLMs. Finally, to support open-source advancement, we release DeepVerifier-4K, a curated supervised fine-tuning dataset of 4,646 high-quality agent steps focused on DRA verification. These examples emphasize reflection and self-critique, enabling open models to develop robust verification capabilities.

## 摘要
深度研究智能体（DRA）的最新进展正在改变自动化的知识发现和问题解决。虽然现有的大多数工作集中在通过后训练增强策略能力，但我们提出了一种替代范式：在精心设计的评分标准（rubrics）引导下，通过迭代验证策略模型的输出来实现智能体能力的自进化。这种方法带来了验证的推理时扩展（inference-time scaling），即智能体通过评估其生成的答案来产生迭代反馈和改进，从而实现自我提升。我们基于自动构建的 DRA 失败分类法（DRA Failure Taxonomy）推导出评分标准，该分类法将智能体的失败系统地分为五个大类和十三个子类。我们提出了 DeepVerifier，这是一种基于评分标准的结果奖励验证器，它利用了验证的不对称性，在元评估 F1 分数上比原始的“智能体即评委”（agent-as-judge）和 LLM 评委基准高出 12%-48%。为了实现实际的自进化，DeepVerifier 在测试时推理期间作为一个即插即用模块集成。验证器生成基于评分标准的详细反馈，并将其反馈给智能体进行迭代引导（bootstrapping），在无需额外训练的情况下优化响应。在强大的闭源 LLM 支持下，这种测试时扩展在 GAIA 和 XBench-DeepResearch 的挑战性子集上实现了 8%-11% 的准确率提升。最后，为了支持开源发展，我们发布了 DeepVerifier-4K，这是一个包含 4,646 个高质量智能体步骤的精选监督微调数据集，专注于 DRA 验证。这些示例强调反思和自我批评，使开源模型能够开发出强大的验证能力。