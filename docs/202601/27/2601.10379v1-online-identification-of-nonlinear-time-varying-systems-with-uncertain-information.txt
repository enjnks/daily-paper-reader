Title: Online identification of nonlinear time-varying systems with uncertain information

URL Source: https://arxiv.org/pdf/2601.10379v1

Published Time: Fri, 16 Jan 2026 02:39:14 GMT

Number of Pages: 11

Markdown Content:
# Online identification of nonlinear time-varying systems with uncertain information ‚ãÜ

# He Ren a, Gaowei Yan* a, Hang Liu a, Lifeng Cao a, Zhijun Zhao b, Gang Dang a

> a

College of Electrical and Power Engineering, Taiyuan University of Technology, Taiyuan, 030024, China 

> b

Department of Automation, Taiyuan Institute of Technology, Taiyuan, 030024, China 

Abstract 

Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning. 

Key words: Nonlinear time-varying systems; Bayesian recursive symbolic learning; Uncertainty analysis. 

1 Introduction 

The rise of digital twins (DTs) as core enablers for the industrial metaverse and autonomous systems is driving a paradigm shift in the modeling, monitoring, and con-trol of complex cyber-physical systems [17,20]. A high-fidelity DT is more than a static virtual model; it is a dy-namic, evolving computational entity that continuously interacts with its physical counterpart via tight integra-tion of data, models, and connectivity [11]. The perfor-mance of this coupling depends on the virtual model‚Äôs ability to fulfill three key requirements: predictive accu-racy under uncertainty, interpretability of the underly-ing governing laws, and real-time adaptability to stream-

‚ãÜ This paper was not presented at any conference. * Corresponding author 

Email addresses: rh15848286658@outlook.com (He Ren), 

yangaowei@tyut.edu.cn (Gaowei Yan*), 

2023310084@link.tyut.edu.cn (Hang Liu), 

18636531698@163.com (Lifeng Cao), 

zhaozhijun1@tit.edu.cn (Zhijun Zhao), dzw627@126.com 

(Gang Dang). 

ing data [22]. Uncertainty in the physical world is a fundamental challenge in the construction and deployment of com-plex systems. In response, a substantial line of research has focused on real-time adaptability and predictive ac-curacy under uncertainty within a Bayesian framework. This approach includes techniques such as Bayesian compressive sensing‚Äîwhich leverages sparsity-inducing priors (e.g., Laplace [10], Horseshoe [5,13]) to enable uncertainty-aware signal recovery. It also encompasses recursive Bayesian inference methods that underpin adaptive filtering [7] and state estimation [18], together providing a principled methodology for online learning. Nevertheless, these advanced methods are predomi-nantly applied to state and parameter estimation within fixed model structures or to black-box function approx-imation [23,14]. As a result, although they excel in un-certainty quantification (thereby enhancing predictive accuracy) and computational efficiency (enabling real-time operation), they often sacrifice interpretability by failing to produce explicit symbolic descriptions of the underlying system dynamics [21,19]. 

Preprint submitted to XXXXX 16 January 2026 

> arXiv:2601.10379v1 [cs.RO] 15 Jan 2026

In contrast, state-of-the-art methods for deriving interpretable models, particularly those based on the Sparse Identification of Nonlinear Dynamics (SINDy) paradigm [8], excel at discovering parsimonious govern-ing equations from data. Variants like G-SINDy [15] and MGSINDy [16] have extended this framework to multi-input multi-output systems and incorporated noise decomposition, thus addressing aspects of predictive accuracy. Despite these strengths, their core limitation lies in the real-time domain: they operate within an of-fline, batch-processing paradigm that inherently relies on iteratively solving non-convex, often NP-hard, op-timization problems. This computational intractability creates a critical gap for DT applications that require online, continual adaptation [6]. Consequently, a core challenge persists: the lack of a unified framework that delivers interpretability, proba-bilistic predictive accuracy, and real-time efficiency. This work is motivated by two fundamental and intercon-nected scientific problems: 

The Semantic Gap between Interpretability and Uncertainty Quantification. A fundamental divide exists between interpretable symbolic modeling and probabilistic uncertainty quantification. While the former produces transparent models lacking uncertainty bounds, the latter provides predictive distributions at the expense of interpretability. Bridging this gap re-quires a new formalism that intrinsically unifies both capabilities. 

The Computational Gap between Batch and Streaming Inference. Translating high-fidelity sym-bolic discovery from offline batch processing to online streaming poses a fundamental challenge, necessitating a shift from iterative optimization to recursive estima-tion while preserving numerical stability and statistical efficiency. To this end, a novel Bayesian Regression-based Sym-bolic Learning (BRSL) framework is proposed. The con-tributions of this work are threefold: 

‚Ä¢ A unified probabilistic state-space model for online symbolic discovery that bridges deterministic sym-bolic regression and Bayesian inference. By incor-porating sparsity-inducing horseshoe priors, model-structure selection is cast as Bayesian inference, enabling simultaneous system identification and un-certainty quantification. 

‚Ä¢ An online recursive algorithm with a forgetting fac-tor, together with precise recursive conditions that en-sure posterior validity and well-posedness which also serve as a real-time data-utility monitor and enhance robustness. 

‚Ä¢ A rigorous convergence analysis establishes the con-vergence of parameter estimates under persistent ex-citation, thereby ensuring provable stability and per-formance. The remainder of this paper is organized as follows. Sec.2 provides the problem formulation. Sec.3 details the main algorithm and provides supporting proofs. Sec.4 validates the algorithm through two case studies. Fi-nally, Sec.5 concludes the paper. 

2 Problem formulation 

Consider a general nonlinear time-varying system 

yd,Œ± = f Œ≤ (xd,Œ± , Œ± ) , (1) where Œ± denotes continuous-time index; yd,Œ± ‚àà Rny de-notes the observation vector; xd,Œ± ‚àà Rnx denotes the state vector. For notational convenience, it is assumed that xd,Œ± encompasses both state variables and oper-ational input variables, and f Œ≤ represents a nonlinear mapping. Data sampled at discrete time instants Œ±t ‚àà [0 , ‚àû)(with t = 1 , 2, ... indexes the sampled data points) be de-noted as yd,Œ± t and xd,Œ± t . For simplicity, define yd,Œ± t =

yd,t and xd,Œ± t = xd,t . The dataset combining these ob-servations and states is then given by 

D =

Y d,t = [ yd, 1; yd, 2; . . . ; yd,t ],

Xd,t = [ xd, 1; xd, 2; . . . ; xd,t ]



, (2) where Y d,t ‚àà Rt√óny represents observation matrix, and 

Xd,t ‚àà Rt√ónx represents the state matrix, satisfying 

Y d,t = f Œ≤ (Xd,t , t ) . (3) In the symbolic learning framework, the structure of the governing equations is partially or completely un-known. Consequently, to learn interpretable dynamical equations, a common approach is to approximate the drift function by a sparse linear combination of prede-fined basis functions 

f Œ≤ (Xd,t , t ) = œà1(Xd,t )Œ≤1 + f œï(Xd,t , t )Œ≤2, (4) where f œï denotes the known drift function of the dynam-ics, such as the first principles; œà1 denotes the library of basis functions defined by symbol learning, which is typically constructed from polynomials 

œà1(Xd,t ) = 

h

1 Xd,t XP2 

> d,t

. . . 

i

, (5) 

XPi 

> d,t

represents all polynomials of i degree in Xd,t .In order to keep notational consistency, the dictionary function is defined as Œ®d =

h

œà1 f œï

i

‚àà Rt√ónp .2To sum up, the problem of interest is to describe the point prediction problem as a probabilistic predic-tion problem and develop a probabilistic form of re-currence framework. Under the aforementioned assump-tions, Eq.(3) can be transformed into a generalized lin-ear representation, i.e. 

Y d,t = Œ®d (Xd,t ) Œ≤, (6) where Œ≤d = [ Œ≤1

Œ≤2 ] represents the sparse coefficient matrix to be identified. 

3 Methodology 

This section introduces an algorithm termed BRSL for online identification of nonlinear models in high-noise environments. Furthermore, the recursive condi-tions and convergence properties of the BRSL algorithm are investigated. 

3.1 Bayesian Regression-Based Symbolic Learning 

To enhance robustness and parsimony, regression co-efficients are often assumed to be sparse in practice, meaning that only a subset of features exerts significant influence on the outcome, thereby simplifying the model structure and improving interpretability [2,9,4]. In this study, a sparsity-inducing horseshoe prior is employed over the parameters, Œ≤ ‚àº p(Œ≤) [3]. Without loss of gen-erality, the regression process is formulated as in Eq.(7): 

Y d,t = Œ®d(Xd,t )Œ≤d + Œ≤0, (7) Let vec( ¬∑) denote the vectorization operation of ma-trix, we have vec( Y d,t ) = vec( Œ®d(Xd,t )Œ≤d) + vec( Œ≤0). (8) Given Y t = vec( Y d,t ) ‚àà Rt¬∑ny √ó1 and Œ≤ = vec( Œ≤d) ‚àà

Rnp¬∑ny √ó1, then, 

Y t = Œ®(Xt)Œ≤ + vec( Œ≤0), (9) where Œ®(Xt) = Iny ‚äó Œ®d(Xd,t ) ‚àà Rt¬∑ny √ónp¬∑ny , and ‚äó

represents Kronecker product. From the probabilistic perspective, Eq.(9) can be ex-pressed as 

p(Y t |Œ≤ ) ‚àº N (Œ®(Xt)Œ≤, Œ£p ‚äó It). (10) Generally, Œ≤ is assumed to be sparse, and noise terms across the columns of Œ≤0 are assumed to be i.i.d., i.e vec( Œ≤0) ‚àº N (0 , Œ£p ‚äó It), where Œ£p = diag (œÉ2 

> i

), i =1, 2, . . . , n y and œÉ2 

> i

denotes the measurement noise vari-ance corresponding to the i-th output. The horseshoe prior assumes that each component Œ≤

of Œ≤ is conditionally independent, with its distribution given by a mixture of Gaussians 

p(Œ≤i,j |Œªi,j , œÑ ) ‚àº N (0 , Œª 2 

> i,j

œÑ 2),p(Œªi,j ) ‚àº Half Cuachy (0 , 1) ,p(œÑ ) ‚àº Half Cuachy (0 , 1) ,

(11) where Œ≤i,j denotes the element at the ( i, j )-th position of matrix Œ≤d; Œªi,j is the local shrinkage parameter for each coefficient Œ≤i,j ; œÑ is the global shrinkage parameter. 

Remark 1 Reliable uncertainty quantification remains a critical challenge in high-dimensional inference. How-ever, the estimates derived from Lasso regression fail to provide meaningful distributional uncertainty for param-eter estimates. The horseshoe prior demonstrates perfor-mance comparable to that of Lasso regression in sparse representation within high-dimensional spaces, while its heavy-tailed properties ensure that strong signals retain high posterior probability. More importantly, this design enables the mapping of deterministic point prediction problems into probabilistic forecasting frameworks. For further details, refer to [1]. 

As in conventional Bayesian inference, the objec-tive is to infer the posterior distribution of parameters 

Œ≤, given the observed and input sequences. Incorpo-rating the horseshoe prior assumptions and letting 

Œ∏ = diag (Œª2 

> i,j

)œÑ 2, we obtain the following posterior 

p(Œ≤ |Œ®(Xt), Y t )

‚àù N (Œ®(Xt)Œ≤, Œ£p ‚äó It) ¬∑ N (0, Œ∏)

‚àù

> t

Y

> j=1

N (yj ; Œ®(xj )Œ≤, Œ£p) ¬∑ N (0, Œ∏),

(12) where Œ®(xi) = Iny ‚äóŒ®d(xd,i ) and yi = vec( yd,i ). Since the conjugate prior of the Gaussian distribution is also a Gaussian distribution, the resulting posterior distribu-tion remains Gaussian, 

> t

Y

> j=1

N (yj ; Œ®(xj )Œ≤, Œ£p)=

> t

Y

> j=1

1(2 œÄ)ny /2|Œ£p|1/2 exp( ‚àí 12 (yj ‚àí Œ®(xj )Œ≤)T

¬∑ Œ£‚àí1 

> p

(yj ‚àí Œ®(xj )Œ≤)) = 1(2 œÄ)tn y /2|Œ£p|t/ 2 exp( ‚àí 12

> t

X

> j=1

(yj ‚àí Œ®(xj )Œ≤)T

¬∑ Œ£‚àí1 

> p

(yj ‚àí Œ®(xj )Œ≤)) .

(13) 3Substituting Eq.(13) into Eq.(12) leads to the follow-ing posterior distribution for the parameters Œ≤:

p(Œ≤ |Œ®(Xt), Y t )

‚àù exp( ‚àí 12 (Y t ‚àí Œ®(Xt)Œ≤)T (Œ£‚àí1 

> p

‚äó It)

¬∑ (Y t ‚àí Œ®(Xt)Œ≤) ‚àí 12 Œ≤T Œ∏‚àí1Œ≤).

(14) Assuming that the final result obtained by Bayesian inference follows the distribution N (Œºw, Œûw), we can then write 

(Œºw = ŒûwŒ®T (Xt)( Œ£‚àí1 

> p

‚äó It)Y t,

Œû‚àí1 

> w

= Œ®T (Xt)( Œ£‚àí1 

> p

‚äó It)Œ®(Xt) + Œ∏‚àí1. (15) Further, since Œ®(Xt) = Iny ‚äó Œ®d(Xd,t ), Eq.(15) can be rewriting as 

(Œºw = Œûwvec( Œ®Td (Xd,t )Y d,t Œ£‚àí1 

> p

),

Œû‚àí1 

> w

= Œ£‚àí1 

> p

‚äó Œ®Td (Xd,t )Œ®d(Xd,t ) + Œ∏‚àí1. (16) The above procedure constitutes the offline training phase. To address the requirement for online parameter updates in DT systems, the entire methodology must be extended to incorporate a recursive online update strat-egy. Within the framework of this study, the generalized expression for online updating is given by 

p(Œ≤t+1 |Œ®(Xt+1 ), Y t+1 )= N (Œºw,t +1 , Œûw,t +1 )= f [N (Œºw,t , Œûw,t ), Œ®(xt+1 ), y t+1 ].

(17) Considering Eq.(12)-(14), 

p(Œ≤t+1 |Œ®(Xt+1 ), Y t+1 )

‚àù

> t

Y

> j=1

N (yj ; Œ®(xj )Œ≤, Œ£p) ¬∑ N (0, Œ∏)

¬∑ N (yt+1 ; Œ®(xt+1 )Œ≤, Œ£p)

‚àù N (Œºw,t , Œûw,t ) ¬∑ N (yt+1 ; Œ®(xt+1 )Œ≤, œÉ 2)

‚àù N (Œºw,t +1 , Œûw,t +1 ).

(18) where Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

Œºw,t +1 = Œûw,t +1 [Œû‚àí1

> w,t

Œºw,t 

+ vec( Œ®Td (xd,t +1 )yd,t +1 Œ£‚àí1 

> p

)] ,

Œû‚àí1 

> w,t +1

= Œû‚àí1 

> w,t

+ Œ£‚àí1

> p

‚äó (Œ®Td (xd,t +1 )Œ®d(xd,t +1 )) .

(19) In recursive processes, it is typically desirable for new data to exert greater influence on the parameter posteriors, while the contribution of historical data gradually diminishes over time. Consequently, a forget-ting factor must be introduced into Eq.(19) to control the weighting of historical data. Unlike conventional recursive algorithms, this method operates within a Bayesian inference framework where simply multiply-ing coefficients cannot achieve historical data forgetting for posterior inference. In this framework, the forget-ting factor œÖ functions analogously to a sliding window length. The corresponding target posterior distribu-tion p(Œ≤t+1 Œ®(X(t‚àíœÖ+1: t+1) ), Y (t‚àíœÖ+1: t+1) ) is given by Eq.(20). Here the subscript ( t ‚àí œÖ + 1 : t + 1) denotes data from time ( t ‚àí œÖ + 1) to ( t + 1). The symbol ‚Äú/‚Äù represents division in probability computation. 

p(Œ≤t+1 Œ®(X(t‚àíœÖ+1: t+1) ), Y (t‚àíœÖ+1: t+1) )

‚àù

> t+1

Y

> j=1

N (yj ; Œ®(xj )Œ≤t, Œ£p) ¬∑ N (0, Œ∏)

/

> t‚àíœÖ

Y

> k=1

N (yk; Œ®(xk)Œ≤t, Œ£p)

‚àù N (Œºw,t , Œûw,t ) ¬∑ N (yt+1 ; Œ®(xt+1 )Œ≤t, Œ£p)

/

> t‚àíœÖ

Y

> k=1

N (yk; Œ®(xk)Œ≤t, Œ£p)

‚àù N œÖ (ŒºœÖw,t +1 , ŒûœÖw,t +1 ),

(20) where Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

ŒºœÖw,t +1 = ŒûœÖw,t +1 {Œû‚àí1

> w,t

Œºw,t 

+ vec[( Œ®Td (xd,t +1 )yd,t +1 

‚àí Œ®Td (Xd,t ‚àíœÖ )Y d,t ‚àíœÖ )Œ£‚àí1 

> p

]},

(ŒûœÖw,t +1 )‚àí1 = Œû‚àí1 

> w,t

+ Œ£‚àí1

> p

‚äó [Œ®Td (xd,t +1 )Œ®d(xd,t +1 )

‚àí Œ®Td (Xd,t ‚àíœÖ )Œ®d(Xd,t ‚àíœÖ )] .

(21) 

Remark 2 The division operation ‚Äú/‚Äù is not a primi-tive operation in probabilistic theory, implying that prob-ability spaces are not closed under the division operation. The following subsection will discuss the establishment conditions for ‚Äú/‚Äù and further provide recursive rules and parameter design guidelines. 

Indeed, Eq.(21) does not constitute a standard recur-sive formulation. This is because the terms Œû‚àí1

> w,t

, Œºw,t ,

Xd,t ‚àíœÖ and Y d,t ‚àíœÖ must be recomputed from time zero during each update, contradicting the core objective of recursive efficiency. For a system with an update period of Œπ, Eq.(20) is extended as 

p(Œ≤t+Œπ Œ®(X(t‚àíœÖ+Œπ:t+Œπ)), Y (t‚àíœÖ+Œπ:t+Œπ) )

‚àù N œÖ (ŒºœÖw,t , ŒûœÖw,t ) ¬∑

> t+Œπ

Y

> k=t+1

N (yk; Œ®(xk)Œ≤t, Œ£p)

/

> t‚àíœÖ+Œπ‚àí1

Y

> k=t‚àíœÖ

N (yk; Œ®(xk)Œ≤t, Œ£p)

‚àù N œÖ (ŒºœÖw,t +1 , ŒûœÖw,t +1 ).

(22) 4Thus, Eq.(21) is updated as 

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

ŒºœÖw,t +1 = ŒûœÖw,t +1 {(ŒûœÖw,t )‚àí1ŒºœÖw,t 

+ vec[( Œ®Td (Xd, (t+1: t+Œπ))Y d, (t+1: t+Œπ)

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+Œπ‚àí1) )

¬∑ Y d, (t‚àíœÖ:t‚àíœÖ+Œπ‚àí1) )Œ£‚àí1 

> p

]},

(ŒûœÖw,t +1 )‚àí1 = ( ŒûœÖw,t )‚àí1

+ Œ£‚àí1 

> p

‚äó [Œ®Td (Xd, (t+1: t+Œπ))

¬∑ Œ®d(Xd, (t+1: t+Œπ))

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+Œπ‚àí1) )

¬∑ Œ®d(Xd, (t‚àíœÖ:t‚àíœÖ+Œπ‚àí1) )] .

(23) The central innovation of the proposed BRSL frame-work lies in its integration of a recursive structure with traditional probabilistic methods, notably through a novel probabilistic recursive scheme that incorporates a forgetting factor. However, unlike traditional point-estimation methods, the practical implementation of Bayesian recursion is subject to specific constraints. The following subsection details these recursive conditions. 

3.2 Recursive condition of BRSL 

Let the initial time be t0, window width be œÖ, number of new samples per update be Œπ, and historical informa-tion forgotten per update be k.

Theorem 1 Given Gaussian distributions N  Œº1, œÉ 21

,

N  Œº2, œÉ 22

, N  Œº3, œÉ 23

 satisfying 

N  Œº1, œÉ 21

 ‚àù N  Œº2, œÉ 22

 N  Œº3, œÉ 23

 (24) 

(i.e., N  Œº1, œÉ 21

 is the normalized product of N  Œº2, œÉ 22



and N  Œº3, œÉ 23

). If the precision condition holds 

œÉ21 < œÉ 22 , (25) 

there exists a unique Gaussian distribution N  Œº3, œÉ 23

,with parameters given by 

Ô£±Ô£¥Ô£≤Ô£¥Ô£≥

œÉ‚àí23 = œÉ‚àí21 ‚àí œÉ‚àí22 ,Œº3 = Œº1œÉ‚àí21 ‚àí Œº2œÉ‚àí22

œÉ‚àí21 ‚àí œÉ‚àí22

. (26) 

Define ‚Äú/‚Äù as N  Œº3, œÉ 23

 = N  Œº1, œÉ 21

 /N  Œº2, œÉ 22

.

Theorem 2 Let A ‚àà Rn√óm, B ‚àà Rp√óm, and C ‚àà Rl√óm

be arbitrary matrices, with n = p + l. If A and B satisfy 

A =

"BC

#

, (27) 

AT A ‚àí BT B is necessarily positive semi-definite. 

Since N (Y (t0:t0+œÖ); Œ®  X(t0:t0+œÖ)

 Œ≤(t0:t0+œÖ), Œ£p‚äó

I(t0:t0+œÖ)), we first address the initialization at t0 = 1, the following result is obtained: 

N



Y (1: œÖ); Œ®  X(1: œÖ)

 Œ≤(1: œÖ), Œ£p ‚äó I(1: œÖ)



=

> k

Y

> j=1

N  yj ; Œ® (xj ) Œ≤j , Œ£p



¬∑

> œÖ

Y

> p=k+1

N  yp; Œ® (xp) Œ≤p, Œ£p

N (0, Œ∏) .

(28) To achieve unbiased recovery of œÖQ

> p=k+1

N (yp; Œ®(xp)Œ≤p,

Œ£p)N (0, Œ∏) according to Theorem 1, the following con-ditions must hold simultaneously, i.e. 

Œ£‚àí1 

> p

‚äó [Œ®Td (Xd, (1: œÖ))Œ®d(Xd, (1: œÖ))

‚àí Œ®Td (Xd, (1: k))Œ®d(Xd, (1: k))] + Œ∏‚àí1 ‚âª 0. (29) Because the matrix Œ∏, defined by the horseshoe prior, is positive definite, the following condition must hold 

Œ®Td (Xd, (1: œÖ))Œ®d(Xd, (1: œÖ))

‚àí Œ®Td (Xd, (1: k))Œ®d(Xd, (1: k)) ‚âª 0. (30) By using Theorem 2, it is straightforward to imple-ment, we have 

N (Y (k+1: œÖ); Œ®  X(k+1: œÖ)

 Œ≤(k+1: œÖ)

, Œ£p ‚äó I(k+1: œÖ))= N (Y (1: œÖ); Œ®  X(1: œÖ)

 Œ≤(1: œÖ), Œ£p ‚äó I(1: œÖ))

/N



Y (1: k); Œ®  X(1: k)

 Œ≤(1: k), Œ£p ‚äó I(1: k)



,

and 

N (Y (k+1: œÖ+Œπ); Œ®  X(k+1: œÖ+Œπ)

 Œ≤(k+1: œÖ+Œπ)

, Œ£p ‚äó I(k+1: œÖ+Œπ))= N (Y (k+1: œÖ); Œ®  X(k+1: œÖ)

 Œ≤(k+1: œÖ)

, Œ£p ‚äó I(k+1: œÖ))N (Y (œÖ+1: œÖ+Œπ); Œ®  X(œÖ+1: œÖ+Œπ)



Œ≤(œÖ+1: œÖ+Œπ), Œ£p ‚äó I(œÖ+1: œÖ+Œπ)).

(31) Combining the preceding equations, we obtain the fol-lowing constraint, which is equivalent to Eq.(30), 

N (Y (k+1: œÖ+Œπ); Œ®  X(k+1: œÖ+Œπ)

 Œ≤(k+1: œÖ+Œπ)

, Œ£p ‚äó I(k+1: œÖ+Œπ))= N (Y (1: œÖ); Œ®  X(1: œÖ)

 Œ≤(1: œÖ), Œ£p ‚äó I(1: œÖ))

/N (Y (1: k); Œ®  X(1: k)

 Œ≤(1: k), Œ£p ‚äó I(1: k))

¬∑ N (Y (œÖ+1: œÖ+Œπ); Œ®  X(œÖ+1: œÖ+Œπ)

 Œ≤(œÖ+1: œÖ+Œπ)

, Œ£p ‚äó I(œÖ+1: œÖ+Œπ)),

(32) 5i.e. 

Œ®Td

 Xd, (œÖ+1: œÖ+Œπ)

 Œ®d

 Xd, (œÖ+1: œÖ+Œπ)



‚àí Œ®Td

 Xd, (1: k)

 Œ®d

 Xd, (1: k)

 ‚âª 0. (33) Precisely, the equivalent constraint holds universally at any time t. Consider the case when t0 > œÖ :

N (Y (t0‚àíœÖ+1: t0); Œ®  X(t0‚àíœÖ+1: t0)



¬∑ Œ≤(t0‚àíœÖ+1: t0), Œ£p ‚äó I(t0‚àíœÖ+1: t0))=

> t0‚àíœÖ+k

Y

> j=t0‚àíœÖ+1

N  yj ; Œ® (xj ) Œ≤j , Œ£p



¬∑

> t0

Y

> q=t0‚àíœÖ+k+1

N  yq ; Œ® (xq ) Œ≤q , Œ£p

.

(34) By applying Theorem 2, the invertibility of the opera-tions involved in the preceding expression is mathemat-ically ensured. Following the same reasoning as in the initial case, a recursive condition is derived that holds uniformly over the entire time domain, as shown below: 

Œ®T (X(t0+1: t0+Œπ))Œ®(X(t0+1: t0+Œπ))‚àí

Œ®T (X(t0‚àíœÖ+1: t0‚àíœÖ+k))Œ®(X(t0‚àíœÖ+1: t0‚àíœÖ+k))

‚âª 0.

(35) Therefore, Eq.(23) should be revised as 

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

ŒºœÖw,t +1 = ŒûœÖw,t +1 {(ŒûœÖw,t )‚àí1ŒºœÖw,t 

+ vec[( Œ®Td (Xd, (t+1: t+Œπ))Y d, (t+1: t+Œπ)

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Y d, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Œ£‚àí1 

> p

]},

(ŒûœÖw,t +1 )‚àí1 = ( ŒûœÖw,t )‚àí1

+ Œ£‚àí1 

> p

‚äó [Œ®Td (Xd, (t+1: t+Œπ))

¬∑ Œ®d(Xd, (t+1: t+Œπ))

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Œ®d(Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )] .

(36) 

Remark 3 The variance matrix ŒûœÖw,t in Eq. (36) must remain strictly positive definite (i.e., ŒûœÖw,t ‚âª 0) at all times during the recursive process. This requirement is guaranteed by the recursive condition specified in Eq. (35) .Furthermore, defining Œ≥ as the evaluation matrix, it has 

Œ≥ = diag (Œ∫i) ‚âª 0, (37) 

where Œ∫i denotes the i-th eigenvalue of 

Œ®Td (Xd, (t0+1: t0+Œπ))Œ®d(Xd, (t0+1: t0+Œπ))

‚àíŒ®Td (Xd, (t0‚àíœÖ+1: t0‚àíœÖ+k))Œ®d(Xd, (t0‚àíœÖ+1: t0‚àíœÖ+k)).The recursive condition can be interpreted as follows: the informational contribution of new data must consis-tently exceed that of old data. This implies that upon the arrival of new data, the computation of Œ∫i enables online assessment of data utility. Specifically: 

‚Ä¢ If ‚àÉŒ∫i = 0 , the incoming data is redundant with his-torical data; 

‚Ä¢ If ‚àÉŒ∫i < 0, the new data may contain significant noise that could compromise model stability. This mechanism effectively supports real-time data quality screening during the recursive update process. 

Indeed, during recursive updates, parameters Œπ and k

are inherently distinct: Œπ can be adaptively determined, while k is typically fixed as a hyperparameter(Typically, (œÖ+Œπ‚àík) is bigger than the number of unknown parame-ters, to enhance the long-term stability of the model, the value of k can be adjusted according to specific practi-cal requirements). This causes the effective window size 

œÖ to expand indefinitely. Artificially fixing œÖ resolves uncontrolled growth but introduces historical informa-tion remanence, leading to model divergence over time. Therefore, under a fixed window size œÖ, a forgetting fac-tor Œæ ‚àà (0 , 1) is incorporated for historical information, yielding the final recursive expression 

Ô£±Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥

ŒºœÖw,t +1 = ŒûœÖw,t +1 {Œæ(ŒûœÖw,t )‚àí1ŒºœÖw,t 

+ vec[( Œ®Td (Xd, (t+1: t+Œπ))Y d, (t+1: t+Œπ)

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Y d, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Œ£‚àí1 

> p

]},

(ŒûœÖw,t +1 )‚àí1 = Œæ(ŒûœÖw,t )‚àí1

+ Œ£‚àí1 

> p

‚äó [Œ®Td (Xd, (t+1: t+Œπ))

¬∑ Œ®d(Xd, (t+1: t+Œπ))

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Œ®d(Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )] .

(38) The recursive framework proposed in this paper is designed to enhance the model‚Äôs adaptability to new data by dynamically updating regression parameters, thereby accelerating convergence. A key feature of this approach is the ability to flexibly balance between incor-porating new information and retaining historical sta-bility through adjustable parameters ( œÖ, k, Œπ, Œæ). The re-cursive condition formally establishes a weighted infor-mation differential between new and old data, allowing the model to emphasize recent trends without compro-mising overall robustness. This enables a tunable trade-off between convergence speed and long-term stability, making the framework suitable for applications where responsive adaptation is valued. 63.3 Convergence 

Theorem 3 For a discrete-time system, let Œõ (t) ‚àà Rn

denotes the regression vector. If there exists constant 

Œ±1 > 0, and a positive integer N , such that for all time instants t ‚â• 0 and any unit vector vT ‚àà Rn, the follow-ing inequality holds: 

1

N 

> t+N‚àí1

X

> k=t



ŒõT (k) v

2

‚â• Œ±1, (39) 

then the signal Œõ (t) is said to satisfy the Persistent Ex-citation (PE) condition. 

Crucially, this PE condition is essentially equivalent to the conclusions established in the Sec.3.2. 

Lemma 1 Under the conditions of Theorem 3, there ex-ists a constant Œ±2 > 0 such that 

Œ±1Im ‚â∫

> ‚àí

1

N 

> t+N‚àí1

X

> k=t

ŒõT (k)Œõ (k) ‚â∫

> ‚àí

Œ±2Im, (40) 

then the signal Œõ (t) is said to satisfy the PE condition. 

Theorem 4 If A ‚àà Rn√ón is a symmetric positive defi-nite matrix, then there exists a unique triangular matrix 

Œõ ‚àà Rn√ón with strictly positive diagonal elements such that A = ŒõT Œõ. This factorization is termed the Cholesky decomposition. 

To reinforce the PE condition, this work requires the information matrix to be positive definite. Let 

ŒõT (t + 1) Œõ(t + 1) = 

Œ®Td (Xd, (t+1: t+Œπ))Œ®Td (Xd, (t+1: t+Œπ))

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) ).

(41) Assuming the observed data satisfies the PE condition and the observation noise Œµ(t) is i.i.d. with E [Œ≤0] = 0

and V ar 

h

Œ≤0Œ≤T

> 0

i

= Œ£p < ‚àû, the information matrix 

St+1 =  ŒûœÖw,t +1 

‚àí1 admits the recursive relation 

St+1 = ŒæSt + Œ£‚àí1 

> p

‚äó ŒõT (t + 1) Œõ(t + 1) = Œæt+1 S0 + Œ£‚àí1 

> p

‚äó

> t+1

X

> i=1

Œæt‚àíi+1 ŒõT (i)Œõ(i), (42) where S0 = Œ£‚àí1 

> p

Œ®Td (xd,t 0 ) Œ®d (xd,t 0 ) + Œ∏‚àí1, and Œõ is triangular matrix. Under the PE condition and with Œæ ‚àà (0 , 1), as 

t ‚Üí ‚àû , the eigenvalues Œª (St) of the matrix St satisfy 

Œªmax (Œ£ ‚àí1 

> p

)Œ±2/(1 ‚àí Œæ) ‚â• Œª (St) ‚â• Œªmin (Œ£ ‚àí1 

> p

)Œ±1/(1 ‚àí Œæ), which is uniformly positive definite. Thus ŒûœÖw,t +1 ex-hibits bounded convergence over time. Defining the parameter estimation error 

ÀúŒ≤ = ŒºœÖw,t ‚àí Œ≤‚àó, (43) where Œ≤‚àó = vec( Œ≤‚àó

> d

), Œ≤‚àó 

> d

represents the true values of the coefficients. Referring to Eq.8, the following can be derived: 

Œ®Td (Xd, (t+1: t+Œπ))Y d, (t+1: t+Œπ)

= Œ®Td (Xd, (t+1: t+Œπ))Œ®d(Xd, (t+1: t+Œπ))Œ≤‚àó

> d

+ Œ®Td (Xd, (t+1: t+Œπ))Œ≤0,new ,

and 

Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Y d, (t‚àíœÖ:t‚àíœÖ+k‚àí1) 

= Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )

¬∑ Œ®d(Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Œ≤‚àó

> d

+ Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Œ≤0,old ,

(44) where Œ≤0,new and Œ≤0,old represent the noise at different time instances. Then, 

Œ®Td (Xd, (t+1: t+Œπ))Y d, (t+1: t+Œπ)Œ£‚àí1

> p

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Y d, (t‚àíœÖ:t‚àíœÖ+k‚àí1) 

¬∑ Œ£‚àí1

> p

= ŒõT (t + 1) Œõ(t + 1) Œ≤‚àó

> d

Œ£‚àí1 

> p

+ Œµ,

(45) where 

Œµ = Œ®Td (Xd, (t+1: t+Œπ))Œ≤0,new Œ£‚àí1

> p

‚àí Œ®Td (Xd, (t‚àíœÖ:t‚àíœÖ+k‚àí1) )Œ≤0,old Œ£‚àí1 

> p

, (46) Since vec( ŒõT (t + 1) Œõ(t + 1) Œ≤‚àó

> d

Œ£‚àí1 

> p

+ Œµ)= ( Œ£‚àí1 

> p

‚äó ŒõT (t + 1) Œõ(t + 1)) Œ≤‚àó + vec( Œµ), (47) consequently, 

ÀúŒ≤t+1 = ŒºœÖw,t +1 ‚àí Œ≤‚àó

= ŒûœÖw,t +1 Œæ(ŒûœÖw,t )‚àí1(ŒºœÖw,t ‚àí Œ≤‚àó) + ŒûœÖw,t +1 vec( Œµ)= ŒûœÖw,t +1 Œæ(ŒûœÖw,t )‚àí1 ÀúŒ≤t + ŒûœÖw,t +1 vec( Œµ).

(48) Notice that E[vec( Œµ)] = 0, yield 

E[ ÀúŒ≤t+1 ] = E[ŒûœÖw,t +1 Œæ(ŒûœÖw,t )‚àí1 ÀúŒ≤t]= E[Œæt+1 ŒûœÖw,t +1 (Œû œÖw, 0)‚àí1 ÀúŒ≤0], (49) 7hence, lim  

> t‚Üí‚àû

E[ ÀúŒ≤t+1 ] = 0. (50) 

Remark 4 In online identification methods, the true pa-rameters are generally time-varying. However, due to the inherent constraints of process industries, abrupt changes in operating conditions are uncommon, i.e., the temporal variation of parameters is bounded: 

Œ≤‚àó 

> t

‚àí Œ≤‚àó 

> t+1

‚â§ Œ¥. (51) 

Under this assumption, Eq. (48) transforms into 

ÀúŒ≤t+1 = ŒûœÖw,t +1 Œæ(ŒûœÖw,t )‚àí1 ÀúŒ≤t

+ ( Œ≤‚àó 

> t

‚àí Œ≤‚àó

> t+1

) + ŒûœÖw,t +1 vec( Œµ). (52) 

Define 

œÅ(t, k ) = ŒûœÖw,t +1 Œæ(ŒûœÖw,t )‚àí1ŒûœÖw,t ‚àí1Œæ(ŒûœÖw,t ‚àí2)‚àí1

... ŒûœÖw,k +1 Œæ(ŒûœÖw,k )‚àí1

= Œæt‚àík+1 ŒûœÖw,t +1 (ŒûœÖw,k )‚àí1,

(53) 

we obtain 

ÀúŒ≤t+1 = œÅ(t, 0) ÀúŒ≤0 +

> t‚àí1

X

> k=0

œÅ(t, k + 1) Ck + Ct, (54) 

where Ck = ( Œ≤‚àó 

> t

‚àí Œ≤‚àó

> t+1

) + ŒûœÖw,t +1 vec( Œµ).Under the PE condition and assuming the input data is stationary, the matrix ŒûœÖw,t +1 (ŒûœÖw,t )‚àí1 is bounded [12]. According to the recursive stability condition, we have 

ŒûœÖw,t +1 (ŒûœÖw,k )‚àí1 ‚â§ H (since ŒûœÖw,t is bounded). There-fore, it can be inferred that 

lim  

> t‚Üí‚àû

E[‚à• ÀúŒ≤t+1 ‚à•] ‚â§ lim  

> t‚Üí‚àû

E[‚à•Œæt+1 ŒûœÖw,t +1 (ŒûœÖw, 0)‚àí1

¬∑ ÀúŒ≤0‚à• + Œ¥ 1 ‚àí (Œæ)t+1 

1 ‚àí Œæ H] = Œ¥ 11 ‚àí Œæ H. 

(55) 

4 Numerical simulations 

This section presents a numerical experiment to val-idate the accuracy and convergence of the proposed re-cursive scheme, along with a complex system case study to demonstrate the practical utility of the BRSL frame-work. 

4.1 Case study 1 

Symbol learning forms the core of the entire frame-work, where the accuracy of system identification and the sparsity of the regression parameters critically determine its downstream performance. The sparsity-inducing horseshoe prior applied within the symbol learning framework has been proven effective for system identification [3]. While Carvalho et al. extensively dis-cussed horseshoe priors in traditional supervised learn-ing settings‚Äîincluding linear regression, generalized linear models, and functional estimation‚Äîthis section validates their efficacy, and that of the broader BRSL framework, through a concise numerical case study. The dataset is generated from y ‚àº N (XŒ≤ , œÉ 2I), where the design matrix X ‚àà Rn√óm is drawn from a uniform distribution on ( ‚àí0.5, 0.5], and the regression coefficient vector Œ≤ is sparse. In this probabilistic model, the coefficients Œ≤ represent the mean parameters of the Gaussian distribution. This experiment evaluates the estimation accuracy of this m-dimensional mean vec-tor. We simulated 100 datasets with each Œ≤ containing approximately 30% non-zero parameters. The non-zero parameters were generated from a uniform distribution on (5 , 10], and additive Gaussian noise with zero mean and variance 0.1 was applied to the data. Fig.1 and Fig.2 show the distributions of the abso-lute estimation errors for the non-zero and zero-valued parameters, respectively. The results demonstrate that the horseshoe prior provides strong performance in sys-tem identification under noisy conditions. It achieves high precision in estimating the true values of non-zero parameters within systems containing multiple stochas-tic elements, while effectively suppressing the estimates of zero-valued parameters toward zero throughout the sparse recovery process. 1 3 5 7 9 11 13 15 Parameter number -2 024Absolute error 10 -1 

> Fig. 1. Absolute error statistics for non-zero parameters

The horseshoe prior employs a novel scale-mixture scheme of multivariate normal distributions, yielding estimation results that exhibit robustness against un-known sparsity patterns and significant outlier signals. 81 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 Parameter number 345Absolute error 10 -2 Fig. 2. Absolute error statistics for zero-valued parameters 5 6 7 8 9 10        

> 12345678910 11 12 13 14 15 16
> 8.7 5.8
> 6.5 9.6
> 5.8 9.9
> 9.8 5.7
> 7.3 6.5
> 9.4 5.5
> 8.0 9.4
> 8.9 5.3
> 7.5 8.9
> 8.6 9.7
> 9.5 8.7
> 5.3 8.1
> 8.8 9.6
> 8.8 8.3
> 9.4 7.2
> 5.7 5.8
> Coefficient Probability Density after BRSL

Fig. 3. The prediction of BRSL before and after the change of working conditions 

This characteristic is exquisitely aligned with the fun-damental requirements of sparse representation in sym-bol learning and the online probabilistic recursion of Bayesian symbolic learning. To validate the convergence and robustness of the re-cursive algorithm, an abrupt change in operating condi-tions was introduced at a random point during the simu-lation. Fig. 3 shows the resulting distribution of non-zero parameters before and after the change, where triangles and pentagrams represent the true post-change and pre-change coefficients, respectively. Fig. 4 illustrates the convergence behavior of the means and variances dur-ing the iterative process. After the change, the coeffi-cients do not converge gradually but exhibit a step-like decline, which is due to the recursive condition in the BRSL method that ensures absolute convergence. Both figures visually demonstrate the strong convergence ca-pability and robustness of the BRSL method. 500 1000 1500 2000 2500 3000 3500 4000 4500 Time Step -5 05Relative Error Operating Condition Change 

Fig. 4. The convergence of the means and variances in the recursive process 

4.2 Case study 2 

In this illustrative example, a state-space model of Lorenz is utilized to validate the performance of the BRSL. The model is described as 

Ô£±Ô£≤Ô£≥

Àôx1 = k1(x2 ‚àí x1) + v1

Àôx2 = x1(28 ‚àí x3) ‚àí x2 + v2

Àôx3 = x1x2 ‚àí k3x3 + v3

x0 =

Ô£ÆÔ£∞

‚àí8727 

Ô£πÔ£ª, (56) where v1, v2 and v3 represent system noise, assumed to follow a standard normal distribution, i.e., v ‚àº N (0, 1). The coefficients to be identified, k1 and k2, are defined as time-varying functions: 

k1 = 0 .5sin (0 .1t) + 10 ,k3 = arctan (0 .1t) + 3 . (57) The effectiveness of the proposed method is validated under time-varying parameter scenarios. During mod-eling, two cases are considered: one where the system model is fully known, and another where it is partially unknown (with x2 being a known state, while x1 and 

x3 are entirely unknown). For sparse identification, a li-brary of second-order polynomial basis functions of the state variables is constructed. Fig.5 and Fig.6 present the SHAP (SHapley Additive exPlanations) value analysis, which quantifies the con-tribution of each basis function to the system output. The results demonstrate that the BRSL method effec-tively drives the model to converge to the true complex system over time, confirming its strong identification ca-pability. Furthermore, Fig.7 and Fig.8 show the simu-lation results for state variables x1 and x3. Even under 90.5 1 1.5 2 2.5 3 3.5 4Time Step 10 4        

> x1
> x2
> x3
> x12
> x1x2
> x1x3
> x22
> x2x3
> x32
> Feature
> -200 -150 -100 -50 050 100 150 200

Fig. 5. SHAP analysis of feature contributions for x10.5 1 1.5 2 2.5 3 3.5 4Time Step 10 4      

> x1
> x2
> x3
> x12
> x1x2
> x1x3
> x22
> x2x3
> x32
> Feature
> -150 -100 -50 050 100 150

Fig. 6. SHAP analysis of feature contributions for x31.75 1.76 1.77 1.78 1.79 1.8 1.81 1.82 1.83 1.84 1.85 Time Step 10 4

> -200 -100 0100 200 300 Value Actual Predicted

Fig. 7. System state x1 trajectory: Actual vs. Pre-dicted(BRSL) 

noise disturbances, the BRSL method accurately tracks the system states, verifying its robustness and effective-ness in complex noisy environments. 1.75 1.76 1.77 1.78 1.79 1.8 1.81 1.82 1.83 1.84 1.85 Time Step 10 4

> -150 -100 -50 050 100 Value Actual Predicted

Fig. 8. System state x3 trajectory: Actual vs. Pre-dicted(BRSL) 

5 Conclusions 

In conclusion, this work unifies interpretability with uncertainty-aware online learning via a Bayesian sym-bolic regression framework, establishing a new paradigm for system identification that delivers full posterior dis-tributions over parsimonious governing equations. The proposed BRSL method provides a robust foundation for real-time symbolic learning through stable recursive conditions, a forgetting mechanism, and provable con-vergence guarantees. This approach enables dynamic, trustworthy models critical for applications such as DTs, bridging long-standing gaps between computational ef-ficiency, probabilistic rigor, and interpretability for de-ployment in real-world streaming data environments. Future work will extend the framework to distributed learning and non-stationary dynamics. 

Acknowledgements 

This work was supported by the Shanxi Province Gen-eral Program of Natural Science Research (20240302122 1055), Shanxi Province Major Special Program of Sci-ence and Technology (202201090301013), and Gemeng Group Technology Innovation Fund Project (2024-05, 2025-01). 

References 

[1] A. Bhadra, J. Datta, N. G. Polson, and B. Willard. Lasso meets horseshoe: A survey. Statistical Science , 34(3):405‚Äì 427, 2019. [2] S. L. Brunton, J. L. Proctor, and J. N. Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the National Academy of Sciences , 113(15):3932‚Äì3937, 2016. [3] C. M. Carvalho, N. G. Polson, and J. G. Scott. Handling sparsity via the horseshoe. In Proceedings of the 12th International Conference on Artificial Intelligence and Statistics (AISTATS) , pages 73‚Äì80, Clearwater Beach, FL, USA, 2009. 

10 [4] T. Chen, M. S. Andersen, L. Ljung, A. Chiuso, and G. Pillonetto. System identification via sparse multiple kernel-based regularization using sequential convex optimization techniques. IEEE Transactions on Automatic Control , 59(11):2933‚Äì2945, 2014. [5] K. Course and P. B. Nair. State estimation of aphysical system with unknown governing equations. Nature ,622(7982):261‚Äì267, Oct 2023. [6] L. Edington, N. Dervilis, A. B. Abdessalem, and D. Wagg. A time-evolving digital twin tool for engineering dynamics applications. Mechanical Systems and Signal Processing ,188:109971, 2023. [7] Z. Hua, J. Wu, and N. Li. An efficient second-order filtered characteristic method for navier‚Äìstokes equations with its adaptive optimization. Applied Mathematics Letters ,172:109723, 2026. [8] E. Kaiser, J. N. Kutz, and S. L. Brunton. Sparse identification of nonlinear dynamics for model predictive control in the low-data limit. Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences , 473(2206), 2017. [9] K. Li, J.-X. Peng, and G. W. Irwin. A fast nonlinear model identification method. IEEE Transactions on Automatic Control , 50(8):1211‚Äì1216, 2005. [10] H. Liao, Z.-R. Lu, L. Wang, J. Liu, and D. Yang. Data-driven modeling of bolted joints by iwan dictionary and laplace prior-enhanced sparse bayesian learning. International Journal of Non-Linear Mechanics , 175:105099, 2025. [11] W. Luo, T. Hu, Y. Ye, C. Zhang, and Y. Wei. Ahybrid predictive maintenance approach for cnc machine tool driven by digital twin. Robotics and Computer-Integrated Manufacturing , 65:101974, 2020. [12] B. Pasik-Duncan. [review of the book adaptive control (2nd ed.), by k. j. Àö astr¬® om and b. wittenmark]. IEEE Control Systems Magazine , 16(2):87, 1996. [13] G. Pillonetto and A. Yazdani. Sparse estimation in linear dynamic networks using the stable spline horseshoe prior. 

Automatica , 146:110666, 2022. [14] B. A. Surya. Maximum likelihood recursive state estimation: An incomplete-information based approach. Automatica ,168:111820, 2024. [15] J. Wang, J. Moreira, Y. Cao, and B. Gopaluni. Time-variant digital twin modeling through the kalman-generalized sparse identification of nonlinear dynamics. In 2022 American Control Conference (ACC) , pages 5217‚Äì5222, Atlanta, GA, USA, 2022. [16] J. Wang, J. Moreira, Y. Cao, and R. B. Gopaluni. Simultaneous digital twin identification and signal-noise decomposition through modified generalized sparse identification of nonlinear dynamics. Computers & Chemical Engineering , 177:108294, 2023. [17] Y. Wang, Z. Su, S. Guo, M. Dai, T. H. Luan, and Y. Liu. A survey on digital twins: Architecture, enabling technologies, security and privacy, and future prospects. IEEE Internet of Things Journal , 10(17):14965‚Äì14987, 2023. [18] X. Yang, W.-A. Zhang, and L. Yu. Fractional kalman filters. 

Automatica , 178:112383, 2025. [19] F. Yu, D. He, R. Jia, and Z. Mao. Structure identification of time delay polynomial hammerstein models. Automatica ,179:112386, 2025. [20] Q. Zhang, W. Long, R. Wang, Z. Cao, Z. Wang, Y. Yan, and Y. Wen. Caper: Dual-level physics-data fusion with modular metamodels for reliable generalization in predictive digital twins. Applied Energy , 398:126393, 2025. [21] Y. Zhang, C. Yu, and F. Fabiani. Identification of non-causal systems with random switching modes. Automatica ,182:112532, 2025. [22] J. Zhu, Y. Yang, M. Xi, S. Ji, L. Jia, and T. Hu. The next-generation digital twin: From advanced sensing towards artificial intelligence-assisted physical-virtual system. Journal of Industrial Information Integration ,48:100942, 2025. [23] L. Zou, Z. Wang, B. Shen, and H. Dong. Recursive state estimation in relay channels with enhanced security against eavesdropping: An innovative encryption‚Äìdecryption framework. Automatica , 174:112159, 2025. 

11