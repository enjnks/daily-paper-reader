Title: On Approaches to Building Surrogate ODE Models for Diffusion Bridges

URL Source: https://arxiv.org/pdf/2512.12671v1

Published Time: Tue, 16 Dec 2025 02:02:09 GMT

Number of Pages: 11

Markdown Content:
# ON APPROACHES TO BUILDING SURROGATE ODE M ODELS FOR 

# DIFFUSION BRIDGES 

A P REPRINT 

M.D. Khilchuk ∗

NSS Lab ITMO University Saint-Petersburg, 197101, Russia 

mdkhilchuk@itmo.ru 

V.V. Latypov 

NSS Lab ITMO University Saint-Petersburg, 197101, Russia 

donrumata@niuitmo.ru 

P.S. Kleshchev 

NSS Lab ITMO University Saint-Petersburg, 197101, Russia 

pskleshchev@itmo.ru 

A.A. Hvatov 

NSS Lab ITMO University Saint-Petersburg, 197101, Russia 

alex_hvatov@itmo.ru 

December 16, 2025 

# ABSTRACT 

Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment. 

Keywords Schrödinger Bridge · Surrogate modeling · SINDy · NeuralODE 

# 1 Introduction 

Generative modeling has become the dominant paradigm in modern machine learning, with diffusion-based approaches establishing state-of-the-art performance in capturing complex data distributions. These models have succeeded in domains ranging from image synthesis Nichol and Dhariwal [2021], Dhariwal and Nichol [2021], Rombach et al. [2022], Chen et al. [2024], Lomurno et al. [2024], Liu et al. [2023] to molecular design Liu et al. [2022], Prakash et al. [2025], Duan et al. [2025], Atik Ahamed et al. [2025]. However, this performance comes at a significant computational cost, primarily due to the slow, iterative nature of the sampling process Cao et al. [2025], Liu et al. [2025a], Issachar et al. [2025]. The foundational strength of standard diffusion models lies in their well-defined, discrete Markov chain of forward-noising and reverse-denoising steps. This process progressively corrupts data with Gaussian noise, then learns 

> ∗

Use footnote for providing further information about author (webpage, alternative address)— not for acknowledging funding agencies. 

> arXiv:2512.12671v1 [cs.LG] 14 Dec 2025

On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

a parameterized model to reverse the corruption, enabling sample generation from pure noise. While this paradigm is powerful and widely applicable, its sequential nature requires a large number of network evaluations to produce a single sample, which remains prohibitively slow for many real-world applications Cao et al. [2025], Xu et al. [2025], Chen and Deng [2025], Liu et al. [2025b]. The problem can be reformulated in continuous time using dynamic bridges, most notably the Schrödinger Bridge (SB) Shi et al. [2023], Liu et al. [2022], Gushchin et al. [2024]. This framework seeks a continuous stochastic process that directly maps a simple initial distribution (e.g., noise) to a complex target data distribution, effectively constructing a bridge between them. By moving from discrete steps to a continuous flow, these methods promise a significant reduction in the number of function evaluations required for sampling. However, this conceptual elegance comes with trade-offs: the problem statement becomes more constrained, and training algorithms such as Iterative Proportional Fitting (IPF) Shi et al. [2023], De Bortoli et al. [2024] can be complex and prone to scalability issues in high dimensions Shi et al. [2023], Bunne et al. [2023], Huang et al. [2024]. To mitigate these complexities, existing research has explored ways to simplify bridge models, for instance, by restricting the model class Kholkin et al. [2024], Song et al. [2020] or employing less precise but faster numerical integration schemes Salimans and Ho [2022], Peyre and Cuturi [2019]. While these strategies can improve speed, they often do so at the expense of model expressivity and the quality of the generated samples. This observation introduces a central tension: the primary motivation for adopting bridge methods is to accelerate sample generation, yet the resulting systems of stochastic differential equations (SDEs) remain difficult to integrate efficiently. An overparameterized deep neural network often represents the drift function in a theoretically optimal bridge. Consequently, most of the model’s complexity, interpretability burden, and computational cost reside in this learned network rather than in the bridge formalism itself. A further limitation is that the mathematical form of the bridge SDE is typically fixed by theory, even when a leaned parameterization could suffice. Hence, there is a clear motivation to design alternative bridge formulations that retain accuracy while offering lighter, task-adaptive dynamics. To address this tension, we propose using surrogate modeling. Our core idea is to learn simplified, data-driven approximations of the underlying dynamics as symbolic differential equations. These surrogates are intended to be (a) simpler and faster to simulate than full-scale neural SDEs and (b) more flexible because they are not strictly bound to a single theoretical form. Conceptually, surrogate models can be trained on trajectories generated by either classical discrete-time diffusion processes or their continuous-time bridge counterparts, providing a versatile pathway to compact symbolic representations. In addition, symbolic discovery methods allow the construction of compact models directly from first principles. We instantiate this idea with two algorithms. First, SINDy Flow Matching (SINDy-FM) leverages the Sparse Identifica-tion of Non-linear Dynamics (SINDy) framework to fit an interpretable, time-dependent dynamical model from either recorded discrete distribution transitions or an online data stream. Second, we reformulate the Schrödinger Bridge problem using Neural Ordinary Differential Equations (Neural ODEs), providing a flexible continuous-time parameteri-zation that we pretrain on physically meaningful trajectories. These symbolic and physics-informed approaches enable the incorporation of domain knowledge, yield highly interpretable models, require far fewer parameters, and can be integrated with high-order, symbol-aware solvers that exhibit faster convergence and superior numerical properties compared to standard SDE solvers. This methodology points to a new class of bridge models that retain the theoretical advantages of continuous-time dynamics while being parameterized by compact, efficient surrogates. By reducing the parameter count and liberating the model form, we pave the way for creating more diverse, self-adjusting bridges that can better capture the nuances of complex data distributions while remaining computationally tractable for practical deployment. In this paper, we provide background on diffusion and bridge models, present our SINDy-FM and Neural ODE-based methods, and demonstrate their efficacy and efficiency through experiments on Gaussian transport tasks and the MNIST dataset. 

# 2 Background 

2.1 Diffusion-based approach mechanism 

The field of generative modeling has witnessed significant advances through the development of diffusion-based approaches, including diffusion probabilistic models Sohl-Dickstein et al. [2015], noise-conditioned score networks Yang and Ermon [2019], and denoising diffusion probabilistic models Ho et al. [2020]. These methods share theoretical foundations while differing in specific implementations. The forward diffusion process considers a data point sampled from the true data distribution x0 ∼ q(x) and defines a Markov chain that progressively adds Gaussian noise over T steps, generating a sequence of noisy samples x1, . . . , xT . The process is governed by a variance schedule 

{βt ∈ (0 , 1) }_t = 1 T , where each transition follows: 2On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

q(xt|xt−1) = N (xt; p1 − βtxt−1, β tI), q(x1: T |x0) = 

> T

Y

> t=1

q(xt|xt−1) (1) As t increases, the sample x0 undergoes progressive degradation of its distinctive characteristics. In the asymptotic limit T → ∞ , xT converges to an isotropic Gaussian distribution. The forward process admits an efficient sampling mechanism via reparameterization. Defining αt = 1 − βt and ¯αt = Qti=1 αi, the state at any arbitrary time step t can be expressed as: 

xt = √αtxt−1 + √1 − αtϵt−1, ϵt−1, ϵ t−2, · · · ∼ N (0 , I )= √αtαt−1xt−2 + p1 − αtαt−1¯ϵt−2 = √¯αtx0 + √1 − ¯αtϵ (2) This yields the conditional distribution: 

q(xt|x0) = N (xt; √¯αtx0, (1 − ¯αt)I) (3) The merging of Gaussian distributions follows standard composition rules: for N (0 , σ 21 I) and N (0 , σ 22 I), the resultant distribution is N (0 , (σ21 + σ22 )I). The composite standard deviation is given by: 

p(1 − αt) + αt(1 − αt−1) = p1 − αtαt−1 (4) Typically, the variance schedule follows a monotonically increasing pattern β1 < β 2 < · · · < β T , consequently yielding ¯α1 > · · · > ¯αT . The reverse diffusion process reconstructs samples by reversing the diffusion trajectory, sampling from q(xt−1|xt) to recover data points from Gaussian noise inputs xT ∼ N (0 , I). Under the condition of sufficiently small βt, q(xt−1|xt) remains Gaussian. However, direct estimation of this reverse distribution is computationally intractable as it necessitates integration over the entire dataset. Consequently, a parametric model 

pθ must be learned to approximate these conditional probabilities. The reverse diffusion process is modeled as 

pθ (xt−1|xt) = N (xt−1; μθ (xt, t ), Σθ (xt, t )) . The learning objective involves training μθ to predict the target: 

ˆμt = 1

√αt



xt − 1 − αt

√1 − ¯αt

ϵt



(5) Leveraging the availability of xt during training, the reparameterization approach facilitates prediction of ϵt:

μθ (xt, t ) = 1

√αt



xt − 1 − αt

√1 − ¯αt

ϵθ (xt, t )



(6) 

xt−1 = N



xt−1; 1

√αt



xt − 1 − αt

√1 − ¯αt

ϵθ (xt, t )



, Σθ (xt, t )



(7) The training loss Lt minimizes the discrepancy from ˆμ:

Lt = Ex0,ϵ

 12∥Σθ (xt, t )∥22

∥ ˆμt(xt, x0) − μθ (xt, t )∥2



= Ex0,ϵ

"

12∥Σθ ∥22

1

√αt



xt − 1 − αt

√1 − ¯αt

ϵt



− 1

√αt



xt − 1 − αt

√1 − ¯αt

ϵθ (xt, t )

 2#

= Ex0,ϵ

 (1 − αt)2

2αt(1 − ¯αt)∥Σθ ∥22

∥ϵt − ϵθ (xt, t )∥2



= Ex0,ϵ

 (1 − αt)2

2αt(1 − ¯αt)∥Σθ ∥22

∥ϵt − ϵθ (√¯αtx0 + √1 − ¯αtϵt, t )∥2



(8) 3On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

2.2 Diffusion bridges 

The Schrödinger Bridge (SB) Shi et al. [2023], Peyre and Cuturi [2019]problem addresses the fundamental task of finding a stochastic dynamic mapping between two probability distributions π0 and πT by identifying a path measure 

PSB that minimizes the Kullback-Leibler divergence to a reference measure Q while satisfying marginal constraints: 

PSB = argmin 

> P

{KL( P | Q) : P0 = π0, PT = πT } .

When Q represents a Brownian motion, this formulation yields an entropy-regularized optimal transport solution. Traditional approaches like Iterative Proportional Fitting (IPF) alternate between projections based on marginal constraints but suffer from error accumulation and scalability issues in high dimensions. To overcome these limitations, we introduce Iterative Markovian Fitting (IMF), a novel methodology that alternately projects onto the space of Markov processes M and the reciprocal class R(Q):

P2n+1 = proj M(P2n), P2n+2 = proj R(Q)(P2n+1 ).

Unlike IPF, IMF preserves initial and terminal distributions at each iteration. Building on IMF, we propose Diffusion Schrödinger Bridge Matching (DSBM), a scalable algorithm that combines forward and backward Markovian projections with efficient bridge sampling. DSBM leverages simple regression losses akin to Bridge Matching and achieves superior performance in approximating SB solutions across various transport tasks, including high-dimensional domains and image translation, while mitigating bias accumulation via symmetric forward-backward updates. 

# 3 Proposed approaches 

3.1 SINDy Flow Matching (SINDy-FM) 

We propose SINDy Flow Matching (SINDy-FM) , a generative algorithm that constructs supervised samples of a target vector field from an interpolation between endpoint distributions and fits an interpretable time-dependent dynamical model through sparse identification of non-linear dynamics (SINDy) Brunton et al. [2016] by loss of flow matching style Lipman et al. [2023]. The method learns a velocity field vθ (x, t ) that, when integrated from t = 0 to 1, transports an initial sample x(0) ∼ p0 to a terminal state whose distribution approximates p1.Let p0 and p1 be probability measures on Rd. We seek a time-varying field vθ : Rd × [0 , 1] → Rd such that the ODE 

ddt x(t) = vθ

 x(t), t , x(0) ∼ p0, (9) induces a terminal distribution x(1) that matches p1.Given an interpolation γ : [0 , 1] × Rd × Rd → Rd that connects endpoints x0 ∼ p0 and x1 ∼ p1, we define supervision pairs by 

x(t) = γ(t; x0, x 1), ˙x(t) = ∂tγ(t; x0, x 1).

Sampling independent (x0, x 1) pairs and times t ∈ [0 , 1] (optionally m > 1 samples of t for each combination of 

(x0, x 1)) yields a dataset D = {(xi, t i, ˙xi)}Ni=1 of states, time stamps, and exact time derivatives. This bypasses numerical differentiation, which is typical for the identification of differential equation systems from raw data, and provides low-noise targets for regression of the field. We represent vθ as a sparse linear combination of p features in the form of symbolic expressions. Let Ξ( x, t ) ∈ Rp, and write vθ (x, t ) = W Ξ( x, t ) with coefficient matrix W ∈ Rd×p. We estimate W using sparse regression. Given D, parameters are obtained by minimizing the mean-squared discrepancy between predicted and supervised derivatives: 

min 

> W

1

N

> N

X

> i=1

W Ξ( xi, t i) − ˙xi 

> 22

+ (sparsity loss) . (10) The minimizer of this loss, according to the flow-matching approach, recovers the marginals pt of the interpolant. This is also the loss traditionally used in SINDy for approximating trajectories that in this setting would be formed by 

γ(˙ ,x 0, x 1) for the fixed endpoints. Whereas sampling more than one t per trajectory slightly reduces space coverage due to correlated samples, it empirically stabilizes training in some scenarios by allowing the model to identify dependencies within the trajectory. 4On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

Algorithm 1: SINDy Flow Matching ( SINDY -FM )

Input: Distributions p0, p 1; interpolation γ(t; x0, x 1); state library Φ, optional time library Ψ; integers N

(trajectories) and m (time points per trajectory). 

D ← ∅;

for i = 1 to N do 

Sample x0 ∼ p0, x1 ∼ p1;Draw and sort ti1, . . . , t im ∼ U[0 , 1] ;

for j = 1 to m do 

xij ← γ(tij ; x0, x 1);

˙xij ← ∂tγ(tij ; x0, x 1);Add (xij , t ij , ˙xij ) to D;

end end 

Compute features Ξ( x, t ) on the points (xi, t i) of the dataset; Fit SINDy on D with loss (10) to approximate ˙xi obtaining vθ (x, t ) = W Ξ( x, t );

Deployment: For any x0 ∼ p0, solve ˙x = vθ (x, t ), x(0) = x0, to t = 1 and return x(1) ;After fitting, transport is effected by integrating (9) for fresh x0 ∼ p0 from t = 0 to t = 1 . The collection of terminal states {x(1) } constitutes samples from the model-implied terminal distribution. The algorithm is presented in Figure 1. 

3.2 Diffusion Schrödinger Bridge Matching–NeuralODE (DSBM-NeuralODE) 

We first implement a Denoising Diffusion Probabilistic Model (DDPM) Nichol and Dhariwal [2021], training a U-Net to predict the noise added during the forward diffusion process. This process can be described as a forward diffusion process governed by the following SDE 

dXt = − 12 β(t)Xtdt + pβ(t)dBt (11) where Xt ∈ Rd is the system state at time t ∈ [0 , 1] , β(t) is a predefined schedule function with β(t) = βstart + ( βend −

βstart ) · t, and Bt is standard Brownian motion in Rd. The continuous process is discretized into N steps. For discrete time t ∈ { 0, 1, . . . , N }, the state evolves as: 

Xt = √¯αtX0 + √1 − ¯αtϵ (12) where ¯αt = Qts=1 (1 − βs) is the cumulative product and ϵ ∼ N (0 , Id) is standard Gaussian noise. The U-Net parameterized by θ learns to predict the noise ϵ:

LDDPM (θ) = Et∼U [0 ,N ],X0∼p0,ϵ∼N (0 ,I)

∥ϵ − ϵθ (Xt, t )∥22

 (13) where ϵθ (Xt, t ) is the U-Net prediction. After training, the generation uses the reverse SDE: 

Xt−1 = 1

√αt



Xt − 1 − αt

√1 − ¯αt

ϵθ (Xt, t )



+ σtz (14) where z ∼ N (0 , Id) and σ2 

> t

= βt.Let {X(i) 

> tk

}_k = 0 m for i = 1 , . . . , N denote trajectories sampled from this reference diffusion process. Each trajectory has shape (batch size , num steps + 1 , dim ), where the batch dimension indexes independent trajectories, the temporal axis enumerates discrete time points from t = 0 to t = 1 , and dim is the state dimension. We construct the training dataset from pairs of consecutive states: D = {(X(i) 

> tk

, X(i) 

> tk+1

) : i = 1 , . . . , N, k = 0 , . . . , m − 1}.The Schrödinger Bridge problem is defined as the solution to the minimization problem: PSB =arg min KL (P|Q) : P0 = π0, PT = πT , where Q is a reference measure on path space C = C([0 , T ], Rd), and KL 5On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

denotes the Kullback-Leibler divergence. In the dynamic formulation, the reference measure Q is typically given by a stochastic differential equation: dXt = ft(Xt)dt + σtdBt, with X0 ∼ π0, where (Bt) is a Wiener process and σt > 0

controls the diffusion strength. The DSBM Shi et al. [2023] approach introduces an iterative procedure that alternates between projections onto two fundamental sets: the space of Markov processes M and the reciprocal class R(Q) consisting of measures sharing the same bridge as the reference measure. The iterative sequence is defined by: 

P2n+1 = proj M(P2n), P2n+2 = proj R(Q)(P2n+1 )

with initialization satisfying P00 = π0 and P0 

> T

= πT . For a mixture of bridges Π = Π 0,T Q[0 , T ], the Markovian projection M∗ = proj M(Π) follows the SDE 

dX∗ 

> t

= ft(X∗ 

> t

) + v∗ 

> t

(X∗ 

> t

)dt + σt dBt,

where the optimal drift is given by 

v∗ 

> t

(xt) = σ2 

> t

EΠT |t

∇ log QT |t(XT | Xt) Xt = xt

 . (15) DSBM with Neural ODE presents a methodology for solving the Schrödinger Bridge problem by integrating continuous-time neural ordinary differential equations (Neural ODEs) with iterative Markovian fitting. Our approach leverages pre-training on diffusion trajectories to initialize the bridge dynamics. This DSBM-NeuralODE method parameterizes the drift in continuous time with Neural ODEs Chen et al. [2018]. For each direction d ∈ forward , backward , we define a neural network f dθ : R × Rn → Rn that takes time t and state x as input and outputs the drift vector. The drift function for direction d is given by vdθ (t, x) = f dθ (t, x), where f dθ is implemented as a feedforward neural network ODEFunc θ : Rd × [0 , 1] → Rd

dXt

dt = ODEFunc θ (Xt, t ) (16) For the forward network, we minimize: 

Lforward (θ) = E(Xt,Xt+∆ t)∼D 

h

Xt+∆ t −  Xt + f forward  

> θ

(t, Xt) · ∆t 2i

(17) For the backward network: 

Lbackward (ϕ) = E(Xt,Xt+∆ t)∼D 

h

Xt −  Xt+∆ t + f backward  

> ϕ

(t, Xt+∆ t) · (−∆t) 2i

(18) This pre-training initializes the networks to approximate the dynamics of the reference process, providing a warm start for the Schrödinger Bridge optimization. Given endpoint pairs (z0, z1) and t ∼ U (ϵ, 1 − ϵ), we generate training points through Brownian bridge interpolation: 

zt = tz1 + (1 − t)z0 + σpt(1 − t)ϵ (19) For the initial iteration we adopt the reference coupling strategy X1 = X0 + σZ with Z ∼ N (0 , I ). The training objective for direction d at iteration n is 

Ldn(θ) = E(X0,X1)∼Πn 

> 0,T

Et∼U [ϵ, 1−ϵ]EZ∼N (0 ,I )

h

vd 

> target

− f dθ (t, Xt) 2i

(20) where Xt is computed via Brownian bridge interpolation. We implement the Euler-Maruyama scheme as the sampling algorithm, discretizing the SDE dXt = f dθ (t, Xt)d t + σdBt.

# 4 Experimental Evaluation 

4.1 Experimental Settings 

We study two problems: (i) a family of Gaussian-to-Gaussian transport in moderate dimension and (ii) latent translation between class-conditional MNIST manifolds. We compare SINDy-FM and DSBM-NeuralODE with a traditional 6On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 0 20 40 60 

> Iteration
> 0.10
> 0.05
> 0.00
> 0.05

Mean    

> dim=5
> dim=20
> dim=50
> Reference
> 020 40 60
> Iteration
> 0.5
> 0.6
> 0.7
> 0.8
> 0.9

Covariance    

> dim=5
> dim=20
> dim=50
> Reference
> 020 40 60
> Iteration
> 0.75
> 1.00
> 1.25
> 1.50
> 1.75

Variance 

> dim=5
> dim=20
> dim=50
> Reference

Figure 1: DSBM-NeuralODE convergence in Gaussian experiment for mean=0.1 NN-based DSBM baseline. Unless specified otherwise, all endpoints live in five-dimensional Euclidean space for the Gaussian benchmarks and in the eight-dimensional latent space induced by a convolutional variational autoencoder for MNIST. The DSBM baseline in both the Gaussian and MNIST settings is trained on the same CPU using the identical latent pairs, with the drift parameterized by a feed-forward MLP (hidden width 64 for Gaussians, 128 for MNIST, depth 2) and a diffusion strength of σ = 1 .

4.1.1 Gaussian transport. 

We generate source and target marginals p0 = N (μ0, Σ0) and p1 = N (−μ0, Σ1) across five representative covariance scenarios: identity, diagonal, rotated, high_condition, and asymmetric (with differing eigenvalue scales for Σ0 and 

Σ1). The identity scenario is evaluated for multiple problem dimensions and distribution means. Covariance spectra are sampled log-uniformly within prescribed ranges and, in the rotated and asymmetric cases, conjugated by random orthogonal matrices. For each pair, we draw 5 × 10 4 straight-line flow-matching trajectories, each with two uniformly sampled time stamps – the supervision couples state samples with exact time derivatives along the linear path. We evaluate candidate fields by integrating from t = 0 to 1 and reporting the W2 Wasserstein distance, training time, inference time, and number of learnable parameters for the studied methods: SINDy-FM, DSBM-NeuralODE, and the neural baseline–DSBM Shi et al. [2023]. For SINDy-FM in gaussian case, we parameterize the drift as v(x, t ) = K(t), x +k(t) with K(t) ∈ Rd×d and k(t) ∈ Rd,motivated by the conditional Gaussian construction in Flow Matching where the canonical vector field is affine in x

Theorem 3 in Lipman et al. [2023]. An affine drift preserves Gaussianity along the flow (since ˙μt = K(t)μt + k(t) and 

˙Σt = K(t)Σ t + Σ tK(t)⊤), and for Gaussian endpoints the population FM minimizer is itself affine (under OT/Monge or independent pairings). In the Gaussian case, SINDy-FM parameterizes the drift as v(x, t ) = K(t)x + k(t) with K(t) ∈ Rd×d and k(t) ∈ Rd,motivated by the conditional Gaussian construction in flow matching where the canonical vector field is affine in x

(Theorem 3 in Lipman et al. [2023]). An affine drift preserves Gaussianity along the flow (because ˙μt = K(t)μt + k(t)

and ˙Σt = K(t)Σ t + Σ tK(t)⊤), and for Gaussian endpoints the population FM minimizer is itself affine under either OT/Monge or independent pairings. In practice, we expand the time dependence in a polynomial basis, 

K(t) = 

> R

X

> r=0

Ar tr , k(t) = 

> R

X

> r=0

br tr , (21) with learnable coefficients Ar ∈ Rd×d and br ∈ Rd. Such polynomial bases are a standard SINDy choice and are universal approximators on compact intervals, so the features follow the factorization Ξ( x, t ) = [Φ( x) ⊗ Ψ( t)] . In the identity scenario for dimensions 20 and 50 , we reduce the polynomial order in t to 1, preserving the true constant drift for that class. The polynomial order R chosen for experimentation is 2. We optimize with SR3 using threshold 0.10 ,

ν = 10 −2, and two time samples per trajectory, and we perform K = 20 integration steps at inference. For each method, a convergence test was performed to determine the optimal number of function evaluations. The example of convergence analysis is shown in Figure 1. 7On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

Table 1 shows varying mean scale from 0.1 to 10 and lifting the dimension from 5 to 50. The SINDy-FM library keeps 

W2 essentially flat. Crucially, per-sample inference remains in the microsecond range throughout. At the same time, the neural DSBM baseline requires two orders of magnitude more time and sees its transport error climb rapidly under the same conditioning. Table 1: Comparison of Three Methods Across Different Distribution Parameters Mean Dim Method W2 Train Time (s) Inference Time (s) Parameters 0.1 5Gaussian SINDy-FM 0.172 27.76 8 · 10 −6 25 DSBM-NeuralODE 0.131 2326.65 21.79 2.7 · 10 5

DSBM 0.103 90 0.08 4.9 · 10 3

0.1 20 Gaussian SINDy-FM 0.128 46.74 5 · 10 −6 20 DSBM-NeuralODE 0.170 2440.33 31.00 1.1 · 10 6

DSBM 0.220 260 0.08 6.9 · 10 3

0.1 50 Gaussian SINDy-FM 0.303 49.25 1.5 · 10 −5 50 DSBM-NeuralODE 0.219 2450.53 29.64 1.2 · 10 6

DSBM 0.450 570 0.19 3.0 · 10 4

1 5Gaussian SINDy-FM 0.167 27.66 8 · 10 −6 29 DSBM-NeuralODE 0.144 3098.43 32.76 2.7 · 10 5

DSBM 0.066 110 0.06 4.9 · 10 3

1 20 Gaussian SINDy-FM 0.128 49.61 5 · 10 −6 20 DSBM-NeuralODE 0.179 3157.54 45.52 1.1 · 10 6

DSBM 0.220 300 0.09 6.9 · 10 3

1 50 Gaussian SINDy-FM 0.303 50.38 1.5 · 10 −5 50 DSBM-NeuralODE 0.212 3374.63 48.39 1.2 · 10 6

DSBM 0.470 550 0.21 3.0 · 10 4

10 5Gaussian SINDy-FM 0.167 34.97 9 · 10 −6 27 DSBM-NeuralODE 0.175 5754.54 71.13 2.7 · 10 5

DSBM 0.110 280 0.06 4.9 · 10 3

10 20 Gaussian SINDy-FM 0.128 48.45 5 · 10 −6 20 DSBM-NeuralODE 0.215 5934.34 80.87 1.1 · 10 6

DSBM 0.260 520 0.14 2.2 · 10 4

10 50 Gaussian SINDy-FM 0.303 50.36 1.5 · 10 −5 50 DSBM-NeuralODE 0.247 6027.98 82.12 1.2 · 10 6

DSBM 0.550 750 0.41 9.2 · 10 4

Across the remaining covariance scenarios in Table 2, SINDy-FM again matches or beats the neural Schrödinger-bridge baselines yet remains vastly faster. Identity and diagonal experiments differ by only a few hundredths in W2 versus the best neural score. However, the symbolic drift integrates two to three orders of magnitude faster (microseconds instead of milliseconds). The asymmetric and high_condition benchmarks underscore the efficiency gap: SINDy-FM preserves 

W2 ≈ 0.25 without instability, whereas DSBM must run considerably longer to approach similar accuracy. Table 2: Comparison of Gaussian SINDy-FM and DBSM Methods Across Different Scenarios Scenario Method W2 Train Time/s Inference Time/s Parameters identity Gaussian SINDy-FM 0.167 29.18 8 · 10 −6 29 DBSM 0.12 150 0.1 4.9 · 10 3

diagonal Gaussian SINDy-FM 0.162 30.61 8 · 10 −6 71 DBSM 0.2/0.12 190 0.04 4.9 · 10 3

rotated Gaussian SINDy-FM 0.220 31.27 8 · 10 −6 76 DBSM 0.21/0.15 180 0.06 4.9 · 10 3

asymmetric Gaussian SINDy-FM 0.254 31.40 8 · 10 −6 81 DBSM 0.7/2.3 1100 0.05 4.9 · 10 3

high_condition Gaussian SINDy-FM 0.233 30.84 8 · 10 −6 85 DBSM 1.9/2.2 900 0.06 4.9 · 10 3

8On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

4.1.2 MNIST latent translation 

For MNIST, we translate digit-2 latent codes into digit-3 latents using a convolutional VAE encoder (trained to 99% 

reconstruction accuracy) and a lightweight classifier ( 99 .9% validation accuracy) to monitor digit purity. The flow-matching supervision consists of 1.2 × 10 5 latent pairs with three uniformly sampled time points per pair; training batches interleave SINDy-FM and the neural Schrödinger-bridge baseline (DSBM). The SINDy-FM translator employs the "rich” library obtained by the tensor product Φ( x) ⊗ Ψ( t), where Ψ( t) = {1, t, t 2} and Φ( x) includes all monomials in the latent coordinates up to degree two, so the learned drift is quadratic in x with polynomial time dependence. On MNIST (Figure 3), SINDy-FM enables control over the quality–complexity trade-off through the choice of sparsity penalization ν and threshold, while maintaining comparable generation quality. The denser variant demonstrates 92% digit-3 accuracy (as classified by a near-ideal CNN). It achieves a Fréchet Inception Distance of 83 and an Inception Score of 1.4 in the feature space of the MNIST-trained classifier. DSBM attains similar metrics but has slower inference and requires three orders of magnitude more parameters. Table 3: MNIST latent translation comparison Metric SINDy-FM DSBM thresh= 0.02 , ν = 0 .05 thresh= 0.05 , ν = 0 .1

Digit accuracy 0.919 ± 0.010 0.914 0.912 

FID 83 .48 ± 9.98 89 .38 ± 7.78 72 .17 

IS 1.431 ± 0.060 1.466 ± 0.056 1.47 

Train time/s 41 24 .68 450 

Inference time/s 9.34 · 10 −4 9.36 · 10 −4 8.0 · 10 −2

Active params 923 541 2.72 · 10 5

An example of translated 3s generated by SINDy-FM is shown in Figure 2. 

Figure 2: MNIST digit 3 generation via translation from 2s with SINDy-FM 

# 5 Discussion 

Our empirical findings indicate that surrogate modeling offers a feasible approach to harmonize the theoretical appeal of continuous-time diffusion bridges with the pragmatic requirements of efficient, interpretable sample generation. The introduced SINDy-FM method effectively obtained compact symbolic representations of the transport dynamics. In Gaussian transport scenarios, these surrogates achieved competitive Wasserstein distances while requiring significantly fewer parameters and enabling nearly instantaneous inference. This parsimony offers a notable advantage, yielding models that are not only rapid but also highly interpretable, as the sparse coefficients explicitly reveal the preeminent dynamical structure. For the more intricate MNIST latent translation task, the symbolic translator preserved robust performance in digit-classification accuracy and low latent-space FID, substantiating that the methodology can scale to semantically rich, non-linear manifolds. The principal strength of symbolic surrogates such as SINDy-FM resides in their data efficiency and transparency. When the underlying dynamics are well-represented by the selected feature library, these models can be precisely identified from relatively few samples. The resultant white-box models facilitate diagnostics, incorporate physical constraints, and exhibit robust extrapolation behavior. Nonetheless, this approach is intrinsically constrained by the expressivity of the pre-defined library. A suboptimal basis will limit the model’s accuracy, and sparsity-inducing regularization introduces a bias-variance trade-off that requires careful management. Conversely, the DSBM-NeuralODE method represents a distinct class of surrogate that balances flexibility with the continuous-time framework of the Schrödinger Bridge. By parameterizing the drift function with a Neural ODE, 9On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

this approach circumvents strict adherence to a fixed theoretical form, enabling the learning of more complex, non-linear dynamics that may be intractable for a simple symbolic library. Pre-training on diffusion trajectories provides a physically meaningful initialization, stabilizing subsequent SB optimization. While this method incurs higher computational costs during training and inference than SINDy-FM and yields a less interpretable black-box model, it serves as a potent and adaptable surrogate. It effectively bridges the gap between the rigid, theory-prescribed bridges and the necessity for adaptable models, particularly in scenarios where the optimal transport dynamics are intricate and not readily captured by a sparse linear combination of basic functions. 

# 6 Conclusion 

The investigation of surrogate models suggests a more diverse ecosystem of bridge models. Symbolic surrogates excel in scenarios with inherent structure and where interpretability and speed are of utmost importance. Neural surrogates, such as the Neural ODE variant, present a compelling alternative for capturing highly complex dynamics at a higher computational expense. The decision between them hinges on the specific requirements of the task—accuracy, speed, interpretability, and the known structure of the data. Future research will focus on expanding the symbolic toolkit with more complex differential structures and programmatic discovery, while refining neural surrogates for greater efficiency, ultimately paving the way for a new generation of self-adjusting, computationally tractable generative models. 

# References 

A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning , pages 8162–8171. PMLR, 2021. P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems , volume 34, pages 8780–8794, 2021. R. Rombach et al. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 10684–10695, 2022. R. Chen, Q. Mao, and Z. Cheng. Stable diffusion is a natural cross-modal decoder for layered ai-generated image compression. arXiv preprint arXiv:2412.12982 , 2024. E. Lomurno, M. D’Oria, and M. Matteucci. Stable diffusion dataset generation for downstream classification tasks. 

arXiv preprint arXiv:2405.02698 , 2024. X. Liu et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations , 2023. G. H. Liu et al. Deep generalized schrödinger bridge. In Advances in Neural Information Processing Systems , volume 35, pages 9374–9388, 2022. P. Prakash et al. Guided diffusion for the discovery of new superconductors. arXiv preprint arXiv:2509.25186 , 2025. C. Duan et al. The rise of generative ai for metal-organic framework design and synthesis. arXiv preprint arXiv:2508.13197 , 2025. M. Atik Ahamed, Q. Ye, and Q. Cheng. Molsnap: Snap-fast molecular generation with latent variational mean flow. 

arXiv e-prints , page arXiv:2508.05411, 2025. S. Cao, H. Chen, P. Chen, Y. Cheng, Y. Cui, X. Deng, Y. Dong, K. Gong, T. Gu, X. Gu, et al. Hunyuanimage 3.0 technical report. arXiv preprint arXiv:2509.23951 , 2025. J. Liu et al. A survey on cache methods in diffusion models: Toward efficient multi-modal generation. arXiv preprint arXiv:2510.19755 , 2025a. N. Issachar et al. Dype: Dynamic position extrapolation for ultra high resolution diffusion. arXiv preprint arXiv:2510.20766 , 2025. R. Xu et al. Geogen: A two-stage coarse-to-fine framework for fine-grained synthetic location-based social network trajectory generation. arXiv preprint arXiv:2510.07735 , 2025. Z. Chen and S. Deng. Flow marching for a generative pde foundation model. arXiv preprint arXiv:2509.18611 , 2025. H. Liu et al. Step-aware residual-guided diffusion for eeg spatial super-resolution. arXiv preprint arXiv:2510.19166 ,2025b. Y. Shi et al. Diffusion schrödinger bridge matching. In Advances in Neural Information Processing Systems , volume 36, pages 62183–62223, 2023. 10 On Approaches to Building Surrogate ODE Models for Diffusion Bridges A P REPRINT 

N. Gushchin et al. Adversarial schrödinger bridge matching. In Advances in Neural Information Processing Systems ,volume 37, pages 89612–89651, 2024. V. De Bortoli et al. Schrödinger bridge flow for unpaired data translation. In Advances in Neural Information Processing Systems , volume 37, pages 103384–103441, 2024. C. Bunne et al. The schrödinger bridge between gaussian measures has a closed form. In International Conference on Artificial Intelligence and Statistics , pages 5802–5833. PMLR, 2023. J. Huang et al. Diffusionpde: Generative pde-solving under partial observation. In Advances in Neural Information Processing Systems , volume 37, pages 130291–130323, 2024. S. Kholkin et al. Diffusion & adversarial schrödinger bridges via iterative proportional markovian fitting. arXiv preprint arXiv:2410.02601 , 2024. Y. Song et al. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456 , 2020. T. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512 ,2022. Gabriel Peyre and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning , 11 (5-6):355–607, 2019. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. pages 2256–2265, 2015. Yang Yang and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems , 32, 2019. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems , 33:6840–6851, 2020. Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences , 113(15):3932–3937, 2016. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747 .R. T. Q. Chen et al. Neural ordinary differential equations. In Advances in Neural Information Processing Systems ,volume 31, 2018. 11